---
title: "Gentoo Model Parameter Abundance Model"
author: "Michael J. Wethington"
date: "2024-06-04"
output: html_document
---


zi = latent nest abundance (mean-adjusted)
lz = logged abundance (re-expression of above):  lzi,t=log(zi,t). for the ith site in the tth year,
ri = intrinsic growth rate
lp = predicted population growth rate multiplier
la = actual population growth rate multiplier 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



**Generate Gentoo Growth Parameters**

```{r}

# Load required libraries
library(tidyverse)
library(coda)
library(mapppdr)
library(patchwork)
library(leaflet)
library(CCAMLRGIS)
library(rjags)
library(MCMCvis)
library(parallel)
library(stringi)
library(pander)
library(testthat)

# Define parameters
min_season <- 1970
max_season <- 2023
species <- "GEPE"

# Construct Presence-Absence Assumptions CSV for the JAGS model
penguin_obs <- mapppdr::penguin_obs

penguin_obs_processed <- penguin_obs %>%
  filter(species_id == species) %>%
  mutate(
    presence = ifelse(!is.na(count), 1, 0),
    known_w = 1) %>%
  select(site_id, season, presence, known_w, count, accuracy, type)

presence_absence_assumptions <- expand.grid(
  site_id = unique(penguin_obs$site_id),
  season = min_season:max_season
) %>%
  left_join(penguin_obs_processed, by = c("site_id", "season")) %>%
  mutate(
    presence = ifelse(is.na(presence), 0, presence),
    known_w = ifelse(is.na(known_w), 0, known_w),
    count = ifelse(is.na(count), 0, count)) %>%
  group_by(site_id, season) %>%
  arrange(desc(type), desc(accuracy), .by_group = TRUE) %>%
  slice(1) %>%
  ungroup() %>%
  select(site_id, season, presence, known_w, count, accuracy)

# Load the JAGS MCMC Output File
load("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/model_data_rinits_output.rda")

# Extract the Gentoo Abundance Estimates (lz)
model_samples <- as.matrix(model_data_rinits_output)

# Filter columns that are logged latent abundance (lz) parameters
lz_columns <- grep("^lz\\[", colnames(model_samples))
lz_samples <- model_samples[, lz_columns]

# Convert log-abundances to actual abundances
abundance_samples <- exp(lz_samples)

# Summarize the actual abundances
abundance_summary <- apply(abundance_samples, 2, function(x) {
  c(mean = mean(x), median = median(x), 
    lower_95 = quantile(x, 0.025), upper_95 = quantile(x, 0.975))
})

# Convert to a readable data frame (use t to transpose)
abundance_summary_df <- as.data.frame(t(abundance_summary))

# Extract site and season info from the indices
extract_indices <- function(colname) {
  indices <- gsub("[^0-9,]", "", colname)
  as.integer(unlist(strsplit(indices, ",")))
}

indices <- lapply(colnames(abundance_samples), extract_indices)
sites <- sapply(indices, `[`, 1)
seasons <- sapply(indices, `[`, 2)

abundance_summary_df$site <- sites
abundance_summary_df$season <- seasons

abundance_summary_df <- abundance_summary_df %>% 
  rename(mean_abundance = mean,
         median_abundance = median,
         lower_95_abundance = "lower_95.2.5%",
         upper_95_abundance = "upper_95.97.5%")

head(abundance_summary_df)

# Load and prepare SiteList
SiteList <- mapppdr::penguin_obs %>%
  filter(count > 0 & species_id == species & season >= min_season & season <= max_season) %>%
  mutate(season_relative = season - min_season + 1) %>%
  group_by(site_id) %>%
  summarise(initial_season = min(season_relative)) %>%
  ungroup() %>%
  left_join(mapppdr::sites, by = "site_id") %>%
  mutate(site = as.numeric(as.factor(site_id))) %>%
  select(site_id, site_name, ccamlr_id, site, initial_season, latitude, longitude)

(n_sites <- nrow(SiteList))

SiteList %>% 
  distinct(site, site_id, latitude, longitude)

# Join SiteList with abundance_summary_df
final_data <- left_join(SiteList, abundance_summary_df, by = "site")

# Ensure final_data is sorted by site and season
final_data <- final_data %>%
  arrange(site, season)

# Calculate the growth rate and append it to the data
final_data <- final_data %>%
  group_by(site) %>%
  mutate(growth_rate = mean_abundance / lag(mean_abundance)) %>%
  ungroup()

# Adjust the season column in the final data
final_data <- final_data %>%
  mutate(year = 1970 + season - 1)

# Display the adjusted final_data
print(head(final_data))

write.csv(final_data, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/modeled_gentoo_parameters.csv")
# Print the head of the final data with growth rates
print(head(final_data))

```

**calculate and plot geometric mean of growth rates** 
```{r}
library(ggplot2)
library(dplyr)

# Calculate geometric mean of growth rates
final_data <- final_data %>%
  group_by(site) %>%
  mutate(geometric_mean_growth_rate = exp(mean(log(growth_rate), na.rm = TRUE))) %>%
  ungroup()

# Filter data for the specific site
site_data <- final_data %>% filter(site_id == "AITC")

# Calculate the mean growth multiplier for the site
mean_growth <- site_data %>% 
  summarize(mean_growth_multiplier = exp(mean(log(growth_rate), na.rm = TRUE)),
            lower_ci = exp(mean(log(growth_rate), na.rm = TRUE) - 1.96 * sd(log(growth_rate), na.rm = TRUE)/sqrt(n())),
            upper_ci = exp(mean(log(growth_rate), na.rm = TRUE) + 1.96 * sd(log(growth_rate), na.rm = TRUE)/sqrt(n())))

# Check if the 'count' and 'type' columns exist and correct them if needed
if (!"count" %in% names(site_data)) {
  site_data$count <- site_data$mean_abundance # or any other logic that fits
}

if (!"type" %in% names(site_data)) {
  site_data$type <- "nests" # default type, adjust as necessary
}

# Add year column based on season
site_data <- site_data %>%
  mutate(year = 1970 + season - 1)

# Generate the plot
ggplot(site_data, aes(x = year, y = mean_abundance)) +
  geom_boxplot(aes(group = year), fill = "orange", alpha = 0.6) +
  geom_errorbar(aes(ymin = lower_95_abundance, ymax = upper_95_abundance), width = 0.2) +
  geom_point(data = site_data %>% filter(!is.na(count)), aes(y = count, color = type), size = 3, shape = 21, fill = "blue") +
  scale_color_manual(values = c("nests" = "red", "chicks" = "blue")) +
  labs(title = "AITC, Barrientos Island (Aitcho Islands), 48.1",
       subtitle = paste0("mean population growth multiplier = ", round(mean_growth$mean_growth_multiplier, 3), 
                         " (", round(mean_growth$lower_ci, 2), " - ", round(mean_growth$upper_ci, 2), ")"),
       x = "Year",
       y = "Abundance",
       color = "Type") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))


```


**Calcualte Sea Ice concentration and pass to dataframe**

```{r}



# This script analyzes daily sea ice concentration (SIC) and extent within defined home ranges by calculating the daily mean SIC, standard deviation of SIC, and total ice-covered area (sea ice extent) above a threshold for each day. It loads NSIDC sea ice data, applies a threshold to set low values to zero, and computes the specified metrics for each home range. The sea ice extent is calculated as the total area of cells with SIC above the threshold, effectively representing the "total ice-covered area" within the home range. The results are compiled and exported to a CSV file for further analysis.

# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(stringr)
library(lubridate)

# Function to compute daily mean SIC, SD SIC, and sea ice extent above a threshold
compute_daily_sic_statistics <- function(buffer_path, nsidc, threshold = 0.15, cell_area_sq_meters) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  
  # Extract dates before masking
  dates <- time(nsidc)
  
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  
  # Set all values < threshold to 0 for the entire raster stack
  buffer_mask <- app(buffer_mask, fun = function(x) { x[x < threshold] <- 0; return(x) })
  
  # Calculate mean and SD SIC excluding NAs for each layer
  mean_sic <- global(buffer_mask, fun = 'mean', na.rm = TRUE)[, 1]
  sd_sic <- global(buffer_mask, fun = 'sd', na.rm = TRUE)[, 1]
  
  # Calculate sea ice extent (total cells above threshold in square kilometers)
  valid_ice_cells <- global(buffer_mask >= threshold, fun = 'sum', na.rm = TRUE)[, 1]
  total_ice_area_sq_km <- (valid_ice_cells * cell_area_sq_meters) / 1e6  # Convert total ice area to square kilometers
  
  results <- data.frame(
    date = as.Date(dates, origin = "1970-01-01"),
    mean_sic = mean_sic,
    sd_sic = sd_sic,
    ice_extent_km2 = total_ice_area_sq_km
  )
  
  return(results)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds, nsidc_subset, results_dir) {
  # Initialize dataframe to store all results
  all_results_df <- data.frame()
  
  # Calculate the cell area in square meters (only once)
  cell_area_sq_meters <- prod(res(nsidc_subset))
  
  # Loop through each home range shapefile and compute results
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      metrics <- compute_daily_sic_statistics(buffer_path, nsidc_subset, threshold = threshold, cell_area_sq_meters = cell_area_sq_meters)
      
      metrics <- metrics %>%
        mutate(Threshold = threshold, HomeRangeSize = home_range_size)
      
      all_results_df <- bind_rows(all_results_df, metrics)
    }
  }
  
  # Export the compiled results to CSV
  write.csv(all_results_df, file.path(results_dir, "daily_sic_statistics.csv"), row.names = FALSE)
  
  return(all_results_df)
}

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1981 to 2023
start_date <- as.Date("1981-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Subset the NSIDC data for testing (e.g., first 100 layers)
# nsidc_subset <- subset(nsidc, 1:100)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
dir.create(results_dir, showWarnings = FALSE)

# Run the analysis on the subset
all_results_df <- analyze_sea_ice_effect(c(0.15), nsidc, results_dir)

# Display the results
head(all_results_df)




```

**Calcualte Persistence and Duration metrics and pass to dataframe**
```{r}

# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(stringr)

# Function to compute mean duration, standard deviation, mean persistence, and persistence standard deviation of sea ice concentration above a threshold
compute_duration_persistence_stats <- function(buffer_path, nsidc, threshold = .15, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  
  # Extract dates before masking
  dates <- time(nsidc)
  
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  
  all_years <- unique(year(dates))
  
  duration_persistence_stats <- data.frame(
    year = integer(),
    month = character(),
    mean_duration = numeric(),
    sd_duration = numeric(),
    mean_persistence = numeric(),
    sd_persistence = numeric()
  )
  
  calculate_metrics <- function(data, threshold) {
    durations <- app(data, function(x) {
      rle_result <- rle(x > threshold)
      durations <- rle_result$lengths[rle_result$values]
      return(mean(durations, na.rm = TRUE))
    })
    mean_duration <- mean(values(durations), na.rm = TRUE)
    sd_duration <- sd(values(durations), na.rm = TRUE)
    
    open_water_prop <- app(data, function(x) mean(x < threshold, na.rm = TRUE))
    mean_persistence <- mean(values(open_water_prop), na.rm = TRUE)
    sd_persistence <- sd(values(open_water_prop), na.rm = TRUE)
    
    return(list(mean_duration = mean_duration, sd_duration = sd_duration, mean_persistence = mean_persistence, sd_persistence = sd_persistence))
  }
  
  for (year in all_years) {
    for (month in winter_months) {
      monthly_indices <- which(year(dates) == year & month(dates) == month)
      if (length(monthly_indices) > 0) {
        monthly_data <- buffer_mask[[monthly_indices]]
        metrics <- calculate_metrics(monthly_data, threshold)
        
        duration_persistence_stats <- rbind(duration_persistence_stats, data.frame(
          year = year,
          month = month,
          mean_duration = metrics$mean_duration,
          sd_duration = metrics$sd_duration,
          mean_persistence = metrics$mean_persistence,
          sd_persistence = metrics$sd_persistence
        ))
      }
    }
    
    season_indices <- which(year(dates) == year & month(dates) %in% winter_months)
    if (length(season_indices) > 0) {
      season_data <- buffer_mask[[season_indices]]
      metrics <- calculate_metrics(season_data, threshold)
      
      duration_persistence_stats <- rbind(duration_persistence_stats, data.frame(
        year = year,
        month = "Season-wide",
        mean_duration = metrics$mean_duration,
        sd_duration = metrics$sd_duration,
        mean_persistence = metrics$mean_persistence,
        sd_persistence = metrics$sd_persistence
      ))
    }
  }
  
  return(duration_persistence_stats)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds, nsidc_subset, results_dir) {
  # Initialize dataframe to store all results
  all_duration_persistence_stats <- data.frame()
  
  # Define home range directory
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  # Loop through each home range shapefile and compute results
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      duration_persistence_stats <- compute_duration_persistence_stats(buffer_path, nsidc_subset, threshold = threshold)
      duration_persistence_stats$Threshold <- threshold
      duration_persistence_stats$HomeRangeSize <- home_range_size
      all_duration_persistence_stats <- bind_rows(all_duration_persistence_stats, duration_persistence_stats)
    }
  }
  
  # Export the compiled results to CSV
  write.csv(all_duration_persistence_stats, file.path(results_dir, "sea_ice_duration_persistence_stats_subset.csv"), row.names = FALSE)
  
  # Export the compiled results to RDS
  saveRDS(all_duration_persistence_stats, file.path(results_dir, "sea_ice_duration_persistence_stats.rds"))
}

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1981 to 2023
start_date <- as.Date("1981-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Subset the NSIDC data for testing (e.g., first 100 layers)
# nsidc_subset <- subset(nsidc, 1:100)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
dir.create(results_dir, showWarnings = FALSE)

# Run the analysis on the subset
analyze_sea_ice_effect(c(.15, .30, .50), nsidc, results_dir)

# Display the results
all_duration_persistence_stats <- read.csv(file.path(results_dir, "sea_ice_duration_persistence_stats_subset.csv"))
head(all_duration_persistence_stats)



```


**Run GLS Models on lagged year for Sea ice concentration**

```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(lubridate)
library(data.table)

# General function to compute annual ice metrics
compute_annual_ice_metrics <- function(ice_data, metrics) {
  ice_data <- ice_data %>%
    mutate(date = as.Date(date, format = "%m/%d/%Y"),
           year = year(date))
  
  annual_ice_metrics <- ice_data %>%
    group_by(year, Threshold, HomeRangeSize) %>%
    summarize(across(all_of(metrics), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}"),
              across(all_of(metrics), ~sd(.x, na.rm = TRUE), .names = "sd_{.col}"),
              .groups = 'drop')
  
  return(annual_ice_metrics)
}

# Function to get precomputed ice metrics
get_precomputed_ice_metrics <- function(years, ice_data, metric) {
  valid_years <- years[years >= 1980 & years <= 2023]
  if (length(valid_years) == 0) {
    return(NA)
  }
  
  ice_values <- ice_data %>%
    filter(year %in% valid_years) %>%
    pull(!!sym(metric))
  
  if (length(ice_values) > 0) {
    return(mean(ice_values, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Function 1: Fit GLS models for average metric from 1-5 years
fit_gls_models_avg <- function(penguin_data, ice_data, metric) {
  penguin_data <- penguin_data %>%
    rowwise() %>%
    mutate(
      overwinter_ice_1yr = get_precomputed_ice_metrics(year - 1, ice_data, metric),
      overwinter_ice_2yr = get_precomputed_ice_metrics(c(year - 1, year - 2), ice_data, metric),
      overwinter_ice_3yr = get_precomputed_ice_metrics(c(year - 1, year - 2, year - 3), ice_data, metric),
      overwinter_ice_4yr = get_precomputed_ice_metrics(c(year - 1, year - 2, year - 3, year - 4), ice_data, metric),
      overwinter_ice_5yr = get_precomputed_ice_metrics(c(year - 1, year - 2, year - 3, year - 4, year - 5), ice_data, metric)
    ) %>%
    ungroup() %>%
    filter(if_all(starts_with("overwinter_ice"), ~ !is.na(.)))
  
  results <- list()
  
  for (lag in 1:5) {
    print(paste("Fitting GLS model for average lag", lag, "year(s)"))
    formula <- as.formula(paste("growth_rate ~ overwinter_ice_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
    summary_model <- summary(model)
    
    results[[paste("Lag_Avg", lag)]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_value = summary_model$tTable[2, 4]
    )
  }
  
  return(results)
}

# Function to get individual precomputed ice metrics
get_individual_ice_metrics <- function(year, ice_data, metric) {
  ice_value <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_value) > 0) {
    return(ice_value)
  } else {
    return(NA)
  }
}

# Function 2: Fit GLS models for individual lag years
fit_gls_models_indiv <- function(penguin_data, ice_data, metric) {
  results <- list()
  
  for (lag in 1:5) {
    penguin_data <- penguin_data %>%
      rowwise() %>%
      mutate(overwinter_ice = get_individual_ice_metrics(year - lag, ice_data, metric)) %>%
      ungroup() %>%
      filter(!is.na(overwinter_ice))
    
    print(paste("Fitting GLS model for individual lag", lag, "year(s)"))
    formula <- as.formula("growth_rate ~ overwinter_ice")
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
    summary_model <- summary(model)
    
    results[[paste("Lag_Indiv", lag)]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_value = summary_model$tTable[2, 4]
    )
  }
  
  return(results)
}

# Main function to run GLS analysis for each condition and save results
run_gls_analysis <- function(penguin_abundance_data, ice_data, metrics, results_dir) {
  # Initialize results list
  all_gls_results_avg <- list()
  all_gls_results_indiv <- list()
  all_results_df_avg <- list()
  all_results_df_indiv <- list()
  significant_results_df_avg <- list()
  significant_results_df_indiv <- list()
  
  # Compute annual ice metrics
  annual_ice_metrics <- compute_annual_ice_metrics(ice_data, metrics)
  
  # List unique thresholds and home range sizes
  thresholds <- unique(ice_data$Threshold)
  home_range_sizes <- unique(ice_data$HomeRangeSize)
  
  for (metric in metrics) {
    for (threshold in thresholds) {
      for (home_range_size in home_range_sizes) {
        ice_subset <- annual_ice_metrics %>%
          filter(Threshold == threshold, HomeRangeSize == home_range_size)
        
        print(paste("Running GLS analysis for metric:", metric, "threshold:", threshold, "home range size:", home_range_size))
        
        # Perform GLS analysis for average metrics
        gls_results_avg <- fit_gls_models_avg(penguin_abundance_data, ice_subset, paste("mean_", metric, sep = ""))
        
        # Store results for average metrics
        all_gls_results_avg[[paste("Annual_Avg", metric, threshold, home_range_size, sep = "_")]] <- gls_results_avg
        
        results_list_avg <- lapply(names(gls_results_avg), function(lag) {
          result <- gls_results_avg[[lag]]
          data.frame(
            Metric = metric,
            HomeRangeSize = home_range_size,
            Threshold = threshold,
            Lag = lag,
            AIC = result$AIC,
            BIC = result$BIC,
            Coefficient_Intercept = result$coefficients[1, 1],
            StdError_Intercept = result$coefficients[1, 2],
            tValue_Intercept = result$coefficients[1, 3],
            pValue_Intercept = result$coefficients[1, 4],
            Coefficient = result$coefficients[2, 1],
            StdError = result$coefficients[2, 2],
            tValue = result$coefficients[2, 3],
            pValue = result$coefficients[2, 4],
            Bonferroni_Adjusted_pValue = p.adjust(result$coefficients[2, 4], method = "bonferroni", n = 5)
          )
        })
        
        all_results_df_avg <- append(all_results_df_avg, results_list_avg)
        significant_results_df_avg <- append(significant_results_df_avg, Filter(function(x) x$pValue < 0.05, results_list_avg))
        
        # Perform GLS analysis for individual lag years
        gls_results_indiv <- fit_gls_models_indiv(penguin_abundance_data, ice_subset, paste("mean_", metric, sep = ""))
        
        # Store results for individual lag years
        all_gls_results_indiv[[paste("Annual_Indiv", metric, threshold, home_range_size, sep = "_")]] <- gls_results_indiv
        
        results_list_indiv <- lapply(names(gls_results_indiv), function(lag) {
          result <- gls_results_indiv[[lag]]
          data.frame(
            Metric = metric,
            HomeRangeSize = home_range_size,
            Threshold = threshold,
            Lag = lag,
            AIC = result$AIC,
            BIC = result$BIC,
            Coefficient_Intercept = result$coefficients[1, 1],
            StdError_Intercept = result$coefficients[1, 2],
            tValue_Intercept = result$coefficients[1, 3],
            pValue_Intercept = result$coefficients[1, 4],
            Coefficient = result$coefficients[2, 1],
            StdError = result$coefficients[2, 2],
            tValue = result$coefficients[2, 3],
            pValue = result$coefficients[2, 4],
            Bonferroni_Adjusted_pValue = p.adjust(result$coefficients[2, 4], method = "bonferroni", n = 5)
          )
        })
        
        all_results_df_indiv <- append(all_results_df_indiv, results_list_indiv)
        significant_results_df_indiv <- append(significant_results_df_indiv, Filter(function(x) x$pValue < 0.05, results_list_indiv))
      }
    }
  }
  
  # Export the compiled results to CSV
  all_results_df_avg <- do.call(rbind, all_results_df_avg)
  significant_results_df_avg <- do.call(rbind, significant_results_df_avg)
  
  all_results_df_indiv <- do.call(rbind, all_results_df_indiv)
  significant_results_df_indiv <- do.call(rbind, significant_results_df_indiv)
  
  write.csv(all_results_df_avg, file.path(results_dir, "model_results_avg_ice_metrics.csv"), row.names = FALSE)
  write.csv(significant_results_df_avg, file.path(results_dir, "significant_model_results_avg_ice_metrics.csv"), row.names = FALSE)
  
  write.csv(all_results_df_indiv, file.path(results_dir, "model_results_indiv_ice_metrics.csv"), row.names = FALSE)
  write.csv(significant_results_df_indiv, file.path(results_dir, "significant_model_results_indiv_ice_metrics.csv"), row.names = FALSE)
  
  # Save results
  saveRDS(all_gls_results_avg, file.path(results_dir, "gls_analysis_results_avg.rds"))
  saveRDS(all_gls_results_indiv, file.path(results_dir, "gls_analysis_results_indiv.rds"))
}

### Load Data and Run Analysis

# Load Gentoo penguin data
penguin_abundance_data <- fread("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Load the SIC statistics
metric_calculation_csv <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
ice_data <- fread(file.path(metric_calculation_csv, "daily_sic_statistics.csv"))

# Define the metrics to analyze
metrics <- c("mean_sic", "sd_sic", "ice_extent_km2")

# Define the results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"
dir.create(results_dir, showWarnings = FALSE)

# Run the GLS analysis on the full dataset
results_output <- run_gls_analysis(penguin_abundance_data, ice_data, metrics, results_dir)


```

**Run GLS Models on lagged year for Persistence and Duration**
```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(lubridate)
library(data.table)

# Function to get precomputed ice metrics
get_precomputed_ice_metrics <- function(years, ice_data, metric) {
  valid_years <- years[years >= 1980 & years <= 2023]
  if (length(valid_years) == 0) {
    return(NA)
  }
  
  ice_values <- ice_data %>%
    filter(year %in% valid_years) %>%
    pull(!!sym(metric))
  
  if (length(ice_values) > 0) {
    return(mean(ice_values, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Function to get individual precomputed ice metrics
get_individual_ice_metrics <- function(year, ice_data, metric) {
  ice_values <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_values) > 0) {
    return(mean(ice_values, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Function 1: Fit GLS models for average metric from 1-5 years
fit_gls_models_avg <- function(penguin_data, ice_data, metric) {
  penguin_data <- penguin_data %>%
    rowwise() %>%
    mutate(
      overwinter_ice_1yr = get_precomputed_ice_metrics(year - 1, ice_data, metric),
      overwinter_ice_2yr = get_precomputed_ice_metrics(c(year - 1, year - 2), ice_data, metric),
      overwinter_ice_3yr = get_precomputed_ice_metrics(c(year - 1, year - 2, year - 3), ice_data, metric),
      overwinter_ice_4yr = get_precomputed_ice_metrics(c(year - 1, year - 2, year - 3, year - 4), ice_data, metric),
      overwinter_ice_5yr = get_precomputed_ice_metrics(c(year - 1, year - 2, year - 3, year - 4, year - 5), ice_data, metric)
    ) %>%
    ungroup() %>%
    filter(if_all(starts_with("overwinter_ice"), ~ !is.na(.)))
  
  results <- list()
  
  for (lag in 1:5) {
    print(paste("Fitting GLS model for average lag", lag, "year(s)"))
    formula <- as.formula(paste("growth_rate ~ overwinter_ice_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
    summary_model <- summary(model)
    
    results[[paste("Lag_Avg", lag)]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_value = summary_model$tTable[2, 4]
    )
  }
  
  return(results)
}

# Function 2: Fit GLS models for individual lag years
fit_gls_models_indiv <- function(penguin_data, ice_data, metric) {
  penguin_data <- penguin_data %>%
    rowwise() %>%
    mutate(
      overwinter_ice_indiv_1yr = get_individual_ice_metrics(year - 1, ice_data, metric),
      overwinter_ice_indiv_2yr = get_individual_ice_metrics(year - 2, ice_data, metric),
      overwinter_ice_indiv_3yr = get_individual_ice_metrics(year - 3, ice_data, metric),
      overwinter_ice_indiv_4yr = get_individual_ice_metrics(year - 4, ice_data, metric),
      overwinter_ice_indiv_5yr = get_individual_ice_metrics(year - 5, ice_data, metric)
    ) %>%
    ungroup() %>%
    filter(if_all(starts_with("overwinter_ice_indiv"), ~ !is.na(.)))
  
  print("Fitting GLS model for individual lag years")
  formula_indiv <- as.formula("growth_rate ~ overwinter_ice_indiv_1yr + overwinter_ice_indiv_2yr + overwinter_ice_indiv_3yr + overwinter_ice_indiv_4yr + overwinter_ice_indiv_5yr")
  model_indiv <- gls(formula_indiv, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
  summary_model_indiv <- summary(model_indiv)
  
  results <- list(
    AIC = AIC(model_indiv),
    BIC = BIC(model_indiv),
    coefficients = summary_model_indiv$tTable,
    p_value = summary_model_indiv$tTable[2:6, 4]
  )
  
  return(results)
}

# Main function to run GLS analysis for each condition and save results
run_gls_analysis <- function(penguin_abundance_data, ice_data, metrics, results_dir) {
  # Initialize results list
  all_gls_results_avg <- list()
  all_gls_results_indiv <- list()
  all_results_df_avg <- list()
  all_results_df_indiv <- list()
  significant_results_df_avg <- list()
  significant_results_df_indiv <- list()
  
  # List unique thresholds and home range sizes
  thresholds <- unique(ice_data$Threshold)
  home_range_sizes <- unique(ice_data$HomeRangeSize)
  
  for (metric in metrics) {
    for (threshold in thresholds) {
      for (home_range_size in home_range_sizes) {
        ice_subset <- ice_data %>%
          filter(Threshold == threshold, HomeRangeSize == home_range_size)
        
        print(paste("Running GLS analysis for metric:", metric, "threshold:", threshold, "home range size:", home_range_size))
        
        # Perform GLS analysis for average metrics
        gls_results_avg <- fit_gls_models_avg(penguin_abundance_data, ice_subset, metric)
        
        # Store results for average metrics
        all_gls_results_avg[[paste("Annual_Avg", metric, threshold, home_range_size, sep = "_")]] <- gls_results_avg
        
        results_list_avg <- lapply(names(gls_results_avg), function(lag) {
          result <- gls_results_avg[[lag]]
          data.frame(
            Metric = metric,
            HomeRangeSize = home_range_size,
            Threshold = threshold,
            Lag = lag,
            AIC = result$AIC,
            BIC = result$BIC,
            Coefficient_Intercept = result$coefficients[1, 1],
            StdError_Intercept = result$coefficients[1, 2],
            tValue_Intercept = result$coefficients[1, 3],
            pValue_Intercept = result$coefficients[1, 4],
            Coefficient = result$coefficients[2, 1],
            StdError = result$coefficients[2, 2],
            tValue = result$coefficients[2, 3],
            pValue = result$coefficients[2, 4],
            Bonferroni_Adjusted_pValue = p.adjust(result$coefficients[2, 4], method = "bonferroni", n = 5)
          )
        })
        
        all_results_df_avg <- append(all_results_df_avg, results_list_avg)
        significant_results_df_avg <- append(significant_results_df_avg, Filter(function(x) x$pValue < 0.05, results_list_avg))
        
        # Perform GLS analysis for individual lag years
        gls_results_indiv <- fit_gls_models_indiv(penguin_abundance_data, ice_subset, metric)
        
        # Store results for individual lag years
        all_gls_results_indiv[[paste("Annual_Indiv", metric, threshold, home_range_size, sep = "_")]] <- gls_results_indiv
        
        results_list_indiv <- data.frame(
          Metric = metric,
          HomeRangeSize = home_range_size,
          Threshold = threshold,
          Lag = "Individual",
          AIC = gls_results_indiv$AIC,
          BIC = gls_results_indiv$BIC,
          Coefficient_Intercept = gls_results_indiv$coefficients[1, 1],
          StdError_Intercept = gls_results_indiv$coefficients[1, 2],
          tValue_Intercept = gls_results_indiv$coefficients[1, 3],
          pValue_Intercept = gls_results_indiv$coefficients[1, 4],
          Coefficient_1yr = gls_results_indiv$coefficients[2, 1],
          StdError_1yr = gls_results_indiv$coefficients[2, 2],
          tValue_1yr = gls_results_indiv$coefficients[2, 3],
          pValue_1yr = gls_results_indiv$coefficients[2, 4],
          Coefficient_2yr = gls_results_indiv$coefficients[3, 1],
          StdError_2yr = gls_results_indiv$coefficients[3, 2],
          tValue_2yr = gls_results_indiv$coefficients[3, 3],
          pValue_2yr = gls_results_indiv$coefficients[3, 4],
          Coefficient_3yr = gls_results_indiv$coefficients[4, 1],
          StdError_3yr = gls_results_indiv$coefficients[4, 2],
          tValue_3yr = gls_results_indiv$coefficients[4, 3],
          pValue_3yr = gls_results_indiv$coefficients[4, 4],
          Coefficient_4yr = gls_results_indiv$coefficients[5, 1],
          StdError_4yr = gls_results_indiv$coefficients[5, 2],
          tValue_4yr = gls_results_indiv$coefficients[5, 3],
          pValue_4yr = gls_results_indiv$coefficients[5, 4],
          Coefficient_5yr = gls_results_indiv$coefficients[6, 1],
          StdError_5yr = gls_results_indiv$coefficients[6, 2],
          tValue_5yr = gls_results_indiv$coefficients[6, 3],
          pValue_5yr = gls_results_indiv$coefficients[6, 4],
          Bonferroni_Adjusted_pValue_1yr = p.adjust(gls_results_indiv$coefficients[2, 4], method = "bonferroni", n = 5),
          Bonferroni_Adjusted_pValue_2yr = p.adjust(gls_results_indiv$coefficients[3, 4], method = "bonferroni", n = 5),
          Bonferroni_Adjusted_pValue_3yr = p.adjust(gls_results_indiv$coefficients[4, 4], method = "bonferroni", n = 5),
          Bonferroni_Adjusted_pValue_4yr = p.adjust(gls_results_indiv$coefficients[5, 4], method = "bonferroni", n = 5),
          Bonferroni_Adjusted_pValue_5yr = p.adjust(gls_results_indiv$coefficients[6, 4], method = "bonferroni", n = 5)
        )
        
        all_results_df_indiv <- append(all_results_df_indiv, list(results_list_indiv))
        significant_results_df_indiv <- append(significant_results_df_indiv, Filter(function(x) any(x$pValue_1yr < 0.05, x$pValue_2yr < 0.05, x$pValue_3yr < 0.05, x$pValue_4yr < 0.05, x$pValue_5yr < 0.05), list(results_list_indiv)))
      }
    }
  }
  
  # Export the compiled results to CSV
  all_results_df_avg <- do.call(rbind, all_results_df_avg)
  significant_results_df_avg <- do.call(rbind, significant_results_df_avg)
  
  all_results_df_indiv <- do.call(rbind, all_results_df_indiv)
  significant_results_df_indiv <- do.call(rbind, significant_results_df_indiv)
  
  write.csv(all_results_df_avg, file.path(results_dir, "model_results_persistence-duration_avg_ice_metrics.csv"), row.names = FALSE)
  write.csv(significant_results_df_avg, file.path(results_dir, "significant_model_resultspersistence-duration_avg_ice_metrics.csv"), row.names = FALSE)
  
  write.csv(all_results_df_indiv, file.path(results_dir, "model_results_persistence-duration_indiv_ice_metrics.csv"), row.names = FALSE)
  write.csv(significant_results_df_indiv, file.path(results_dir, "significant_model_results_persistence-duration_indiv_ice_metrics.csv"), row.names = FALSE)
  
  # Save results
  saveRDS(all_gls_results_avg, file.path(results_dir, "gls_analysis_results_avg.rds"))
  saveRDS(all_gls_results_indiv, file.path(results_dir, "gls_analysis_results_indiv.rds"))
}

### Load Data and Run Analysis

# Load Gentoo penguin data
penguin_abundance_data <- fread("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Load the SIC statistics
metric_calculation_csv <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
ice_data <- fread(file.path(metric_calculation_csv, "sea_ice_duration_persistence_stats.csv"))

# Define the metrics to analyze
metrics <- c("mean_duration", "sd_duration", "mean_persistence", "sd_persistence")

# Define the results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"
dir.create(results_dir, showWarnings = FALSE)

# Run the GLS analysis on the full dataset
run_gls_analysis(penguin_abundance_data, ice_data, metrics, results_dir)


```



**Sea Ice Extent ** (updated - needs a larger home range) - consider adding average monthly min/max

```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(stringr)
library(lubridate)

# Function to compute mean extent of sea ice concentration above a threshold
compute_extent <- function(buffer_path, nsidc, threshold = 0.15, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  all_years <- unique(year(time(buffer_mask)))
  
  results <- data.frame(year = all_years, mean_extent = NA)
  
  cell_area_sq_meters <- prod(res(buffer_mask[[1]]))
  
  for (year in all_years) {
    winter_data <- subset(buffer_mask, which(year(time(buffer_mask)) == year & month(time(buffer_mask)) %in% winter_months))
    if (nlyr(winter_data) > 0) {
      # Set all values < threshold to 0
      winter_data <- app(winter_data, fun = function(x) { x[x < threshold] <- 0; return(x) })
      
      # Calculate mean SIC excluding NAs for each layer
      mean_extent <- global(winter_data, fun = 'mean', na.rm = TRUE)[, 1]
      
      # Handle sea ice extent: count cells >= threshold for each layer
      valid_ice_cells <- global(winter_data >= threshold, fun = 'sum', na.rm = TRUE)[, 1]
      total_ice_area_sq_km <- (valid_ice_cells * cell_area_sq_meters) / 1e6  # Convert total ice area to square kilometers
      
      results$mean_extent[results$year == year] <- mean(mean_extent, na.rm = TRUE)
    }
  }
  
  return(results)
}

# Function to fit GLS models and summarize results
fit_gls_models <- function(penguin_abundance_filtered, metrics) {
  results <- list()
  for (lag in 1:5) {
    penguin_abundance_filtered <- penguin_abundance_filtered %>%
      rowwise() %>%
      mutate(!!paste0("overwinter_extent_", lag, "yr") := ifelse(year - lag >= 1980, metrics$mean_extent[metrics$year == year - lag], NA)) %>%
      ungroup()
  }
  
  for (lag in 1:5) {
    penguin_abundance_filtered_lag <- penguin_abundance_filtered %>%
      filter(!is.na(!!sym(paste0("overwinter_extent_", lag, "yr"))))
    
    formula <- as.formula(paste("growth_rate ~ overwinter_extent_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_abundance_filtered_lag)
    summary_model <- summary(model)
    
    results[[paste("Lag", lag, sep = "_")]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_value_extent = summary_model$tTable[2, 4]
    )
  }
  
  return(results)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds) {
  # Load Gentoo penguin data
  penguin_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/gentoo_presence_absence_assumptions.csv")
  penguin_abundance_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")
  
  # Load study area shapefile
  study_area_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
  study_area <- st_read(study_area_path)
  
  # Load the NSIDC sea ice concentration data
  nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")
  
  # Filter the sea ice data from 1981 to 2023
  start_date <- as.Date("1981-01-01")
  end_date <- as.Date("2023-09-30")
  nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))
  
  # Adjust the season column in the penguin abundance data
  penguin_abundance_data <- penguin_abundance_data %>%
    mutate(year = 1970 + season - 1)
  
  # Filter penguin data to keep only consecutively censused sites and clean duplicates
  consecutively_censused <- penguin_data %>%
    arrange(site_id, season) %>%
    group_by(site_id) %>%
    mutate(next_season = lead(season),
           next_presence = lead(presence)) %>%
    filter(presence == 1 & next_presence == 1 & season != next_season) %>%
    ungroup() %>%
    select(site_id, season, presence, next_season)
  
  # Merge with penguin abundance data
  penguin_abundance_filtered <- penguin_abundance_data %>%
    left_join(consecutively_censused, by = c("site_id", "year" = "season")) %>%
    filter(!is.na(next_season))
  
  print("Penguin abundance data after filtering for consecutively censused sites:")
  print(head(penguin_abundance_filtered)) # Debug print
  
  # Define home range directory
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  # Create results directory
  results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results"
  dir.create(results_dir, showWarnings = FALSE)
  
  # Initialize dataframes to store all results and significant results
  all_results_df <- data.frame()
  significant_results_df <- data.frame()
  
  # Loop through each home range shapefile and compute results
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      metrics <- compute_extent(buffer_path, nsidc, threshold = threshold)
      results <- fit_gls_models(penguin_abundance_filtered, metrics)
      
      for (lag in 1:5) {
        result <- results[[paste0("Lag_", lag)]]
        result_df <- data.frame(
          Threshold_Extent = threshold,
          HomeRangeSize_Extent = home_range_size,
          Lag_Extent = lag,
          AIC_Extent = result$AIC,
          BIC_Extent = result$BIC,
          Coefficient_Intercept_Extent = result$coefficients[1, 1],
          StdError_Intercept_Extent = result$coefficients[1, 2],
          tValue_Intercept_Extent = result$coefficients[1, 3],
          pValue_Intercept_Extent = result$coefficients[1, 4],
          Coefficient_Extent = result$coefficients[2, 1],
          StdError_Extent = result$coefficients[2, 2],
          tValue_Extent = result$coefficients[2, 3],
          pValue_Extent = result$p_value_extent,
          Bonferroni_Adjusted_pValue_Extent = p.adjust(result$p_value_extent, method = "bonferroni", n = 5)
        )
        all_results_df <- rbind(all_results_df, result_df)
      }
    }
  }
  
  # Filter significant results using dplyr
  significant_results_df <- all_results_df %>%
    filter(pValue_Extent < 0.05)
  
  # Export the compiled results to CSV
  write.csv(all_results_df, file.path(results_dir, "model_results_sea_ice_extent.csv"), row.names = FALSE)
  
  # Export the significant results to a separate CSV
  write.csv(significant_results_df, file.path(results_dir, "significant_model_results_sea_ice_extent.csv"), row.names = FALSE)
}

# Run the analysis
analyze_sea_ice_effect(.15)



```


**Sea Ice Extent Variabilty** (updated - needs a larger home range) - consider adding average monthly min/max
```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(stringr)
library(lubridate)

# Function to compute extent variability of sea ice concentration above a threshold
compute_extent_variability <- function(buffer_path, nsidc, threshold = 0.15, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  all_years <- unique(year(time(buffer_mask)))
  
  results <- data.frame(year = all_years, extent_variability = NA)
  
  cell_area_sq_meters <- prod(res(buffer_mask[[1]]))
  
  for (year in all_years) {
    winter_data <- subset(buffer_mask, which(year(time(buffer_mask)) == year & month(time(buffer_mask)) %in% winter_months))
    if (nlyr(winter_data) > 0) {
      # Set all values < threshold to 0
      winter_data <- app(winter_data, fun = function(x) { x[x < threshold] <- 0; return(x) })
      
      # Handle sea ice extent: count cells >= threshold for each layer
      valid_ice_cells <- global(winter_data >= threshold, fun = 'sum', na.rm = TRUE)[, 1]
      monthly_extent <- tapp(winter_data, index = month(time(winter_data)), fun = function(x) sum(x >= threshold) * cell_area_sq_meters / 1e6, na.rm = TRUE)
      extent_variability <- sd(monthly_extent, na.rm = TRUE)
      
      results$extent_variability[results$year == year] <- extent_variability
    }
  }
  
  return(results)
}

# Function to fit GLS models and summarize results
fit_gls_models <- function(penguin_abundance_filtered, metrics) {
  results <- list()
  for (lag in 1:5) {
    penguin_abundance_filtered <- penguin_abundance_filtered %>%
      rowwise() %>%
      mutate(!!paste0("extent_variability_", lag, "yr") := ifelse(year - lag >= 1980, metrics$extent_variability[metrics$year == year - lag], NA)) %>%
      ungroup()
  }
  
  for (lag in 1:5) {
    penguin_abundance_filtered_lag <- penguin_abundance_filtered %>%
      filter(!is.na(!!sym(paste0("extent_variability_", lag, "yr"))))
    
    formula <- as.formula(paste("growth_rate ~ extent_variability_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_abundance_filtered_lag)
    summary_model <- summary(model)
    
    results[[paste("Lag", lag, sep = "_")]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_value_variability = summary_model$tTable[2, 4]
    )
  }
  
  return(results)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds) {
  # Load Gentoo penguin data
  penguin_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/gentoo_presence_absence_assumptions.csv")
  penguin_abundance_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")
  
  # Load study area shapefile
  study_area_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
  study_area <- st_read(study_area_path)
  
  # Load the NSIDC sea ice concentration data
  nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")
  
  # Filter the sea ice data from 1981 to 2023
  start_date <- as.Date("1981-01-01")
  end_date <- as.Date("2023-09-30")
  nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))
  
  # Adjust the season column in the penguin abundance data
  penguin_abundance_data <- penguin_abundance_data %>%
    mutate(year = 1970 + season - 1)
  
  # Filter penguin data to keep only consecutively censused sites and clean duplicates
  consecutively_censused <- penguin_data %>%
    arrange(site_id, season) %>%
    group_by(site_id) %>%
    mutate(next_season = lead(season),
           next_presence = lead(presence)) %>%
    filter(presence == 1 & next_presence == 1 & season != next_season) %>%
    ungroup() %>%
    select(site_id, season, presence, next_season)
  
  # Merge with penguin abundance data
  penguin_abundance_filtered <- penguin_abundance_data %>%
    left_join(consecutively_censused, by = c("site_id", "year" = "season")) %>%
    filter(!is.na(next_season))
  
  print("Penguin abundance data after filtering for consecutively censused sites:")
  print(head(penguin_abundance_filtered)) # Debug print
  
  # Define home range directory
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  # Create results directory
  results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results"
  dir.create(results_dir, showWarnings = FALSE)
  
  # Initialize dataframes to store all results and significant results
  all_results_df <- data.frame()
  significant_results_df <- data.frame()
  
  # Loop through each home range shapefile and compute results
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      metrics <- compute_extent_variability(buffer_path, nsidc, threshold = threshold)
      results <- fit_gls_models(penguin_abundance_filtered, metrics)
      
      for (lag in 1:5) {
        result <- results[[paste0("Lag_", lag)]]
        result_df <- data.frame(
          Threshold_Extent_Variability = threshold,
          HomeRangeSize_Extent_Variability = home_range_size,
          Lag_Extent_Variability = lag,
          AIC_Extent_Variability = result$AIC,
          BIC_Extent_Variability = result$BIC,
          Coefficient_Intercept_Extent_Variability = result$coefficients[1, 1],
          StdError_Intercept_Extent_Variability = result$coefficients[1, 2],
          tValue_Intercept_Extent_Variability = result$coefficients[1, 3],
          pValue_Intercept_Extent_Variability = result$coefficients[1, 4],
          Coefficient_Extent_Variability = result$coefficients[2, 1],
          StdError_Extent_Variability = result$coefficients[2, 2],
          tValue_Extent_Variability = result$coefficients[2, 3],
          pValue_Extent_Variability = result$p_value_variability,
          Bonferroni_Adjusted_pValue_Extent_Variability = p.adjust(result$p_value_variability, method = "bonferroni", n = 5)
        )
        all_results_df <- rbind(all_results_df, result_df)
      }
    }
  }
  
  # Filter significant results using dplyr
  significant_results_df <- all_results_df %>%
    filter(pValue_Extent_Variability < 0.05)
  
  # Export the compiled results to CSV
  write.csv(all_results_df, file.path(results_dir, "model_results_sea_ice_extent_variability.csv"), row.names = FALSE)
  
  # Export the significant results to a separate CSV
  write.csv(significant_results_df, file.path(results_dir, "significant_model_results_sea_ice_extent_variability.csv"), row.names = FALSE)
}

# Run the analysis
analyze_sea_ice_effect(.15)


```



**Sea Ice Concentration Analysis (Home Range Analysis)** (updated) - consider adding different thresholds 
```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(lubridate)
library(nlme)

# Function to compute mean SIC for all valid years within the specified buffer
compute_mean_sic <- function(buffer_path, nsidc, ice_threshold = .15, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  all_years <- unique(year(time(buffer_mask)))
  
  mean_sic_per_year <- data.frame(year = all_years, mean_sic = NA)
  
  for (year in all_years) {
    winter_data <- subset(buffer_mask, which(year(time(buffer_mask)) == year & month(time(buffer_mask)) %in% winter_months))
    if (nlyr(winter_data) > 0) {
      winter_data <- app(winter_data, function(x) ifelse(x < ice_threshold, 0, x))
      mean_sic <- global(winter_data, fun = mean, na.rm = TRUE)
      mean_sic_per_year$mean_sic[mean_sic_per_year$year == year] <- mean_sic[1, 1]
    }
  }
  
  return(mean_sic_per_year)
}

# Function to get precomputed mean SIC
get_precomputed_sic <- function(years, mean_sic_per_year) {
  valid_years <- years[years >= 1980 & years <= 2022]
  if (length(valid_years) == 0) {
    return(NA)
  }
  mean_sic_values <- mean_sic_per_year$mean_sic[mean_sic_per_year$year %in% valid_years]
  if (length(mean_sic_values) > 0) {
    return(mean(mean_sic_values, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Function to fit GLS models and summarize results
fit_gls_models <- function(penguin_abundance_filtered, mean_sic_per_year) {
  penguin_abundance_filtered <- penguin_abundance_filtered %>%
    rowwise() %>%
    mutate(overwinter_sic_1yr = ifelse(year - 1 >= 1980, get_precomputed_sic(year - 1, mean_sic_per_year), NA),
           overwinter_sic_2yr = ifelse(year - 2 >= 1980, get_precomputed_sic(c(year - 1, year - 2), mean_sic_per_year), NA),
           overwinter_sic_3yr = ifelse(year - 3 >= 1980, get_precomputed_sic(c(year - 1, year - 2, year - 3), mean_sic_per_year), NA),
           overwinter_sic_4yr = ifelse(year - 4 >= 1980, get_precomputed_sic(c(year - 1, year - 2, year - 3, year - 4), mean_sic_per_year), NA),
           overwinter_sic_5yr = ifelse(year - 5 >= 1980, get_precomputed_sic(c(year - 1, year - 2, year - 3, year - 4, year - 5), mean_sic_per_year), NA)) %>%
    ungroup() %>%
    filter(!is.na(overwinter_sic_1yr) & !is.na(overwinter_sic_2yr) & !is.na(overwinter_sic_3yr) & !is.na(overwinter_sic_4yr) & !is.na(overwinter_sic_5yr))
  
  results <- list()
  
  for (lag in 1:5) {
    formula <- as.formula(paste("growth_rate ~ overwinter_sic_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_abundance_filtered)
    summary_model <- summary(model)
    results[[paste("Lag", lag)]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_value = summary_model$tTable[2, 4]
    )
  }
  
  return(results)
}

# Load Gentoo penguin data
penguin_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/gentoo_presence_absence_assumptions.csv")
penguin_abundance_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

# Load study area shapefile
study_area_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
study_area <- st_read(study_area_path)

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1980 to 2022
start_date <- as.Date("1980-01-01")
end_date <- as.Date("2022-12-31")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Filter penguin data to keep only consecutively censused sites and clean duplicates
consecutively_censused <- penguin_data %>%
  arrange(site_id, season) %>%
  group_by(site_id) %>%
  mutate(next_season = lead(season),
         next_presence = lead(presence)) %>%
  filter(presence == 1 & next_presence == 1 & season != next_season) %>%
  ungroup() %>%
  select(site_id, season, presence, next_season)

# Merge with penguin abundance data
penguin_abundance_filtered <- penguin_abundance_data %>%
  left_join(consecutively_censused, by = c("site_id", "year" = "season")) %>%
  filter(!is.na(next_season))

print("Penguin abundance data after filtering for consecutively censused sites:")
print(head(penguin_abundance_filtered)) # Debug print

# Define home range directory and sizes
home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges/"
home_range_sizes <- c(50, 100, 150, 200, 250, 300, 350, 400, 450, 500)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results"
dir.create(results_dir, showWarnings = FALSE)

# Initialize dataframes to store all results and significant results
all_results_df <- data.frame()
significant_results_df <- data.frame()

# Loop through each home range size and compute results
for (size in home_range_sizes) {
  buffer_path <- file.path(home_range_dir, paste0("gepe_home_ranges_", size, "km.shp"))
  mean_sic_per_year <- compute_mean_sic(buffer_path, nsidc)
  results <- fit_gls_models(penguin_abundance_filtered, mean_sic_per_year)
  
  for (lag in names(results)) {
    result <- results[[lag]]
    result_df <- data.frame(
      HomeRangeSize_SIC = size,
      Lag_SIC = lag,
      AIC_SIC = result$AIC,
      BIC_SIC = result$BIC,
      Coefficient_Intercept_SIC = result$coefficients[1, 1],
      StdError_Intercept_SIC = result$coefficients[1, 2],
      tValue_Intercept_SIC = result$coefficients[1, 3],
      pValue_Intercept_SIC = result$coefficients[1, 4],
      Coefficient_SIC = result$coefficients[2, 1],
      StdError_SIC = result$coefficients[2, 2],
      tValue_SIC = result$coefficients[2, 3],
      pValue_SIC = result$coefficients[2, 4],
      Bonferroni_Adjusted_pValue_SIC = p.adjust(result$coefficients[2, 4], method = "bonferroni", n = 5)
    )
    all_results_df <- rbind(all_results_df, result_df)
    
    # Check if the result is significant
    if (result_df$pValue_SIC < 0.05) {
      significant_results_df <- rbind(significant_results_df, result_df)
    }
  }
}

# Export the compiled results to CSV
write.csv(all_results_df, file.path(results_dir, "model_results_sea_ice_concentration.csv"), row.names = FALSE)

# Export the significant results to a separate CSV
write.csv(significant_results_df, file.path(results_dir, "significant_model_results_sea_ice_concentration.csv"), row.names = FALSE)



```


**Sea Ice Concentration Variability** (updated) - consider adding different thresholds 

```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(lubridate)
library(nlme)

# Function to compute SD of sea ice persistence for all valid years within the specified buffer
compute_sd_persistence <- function(buffer_path, nsidc, ice_thresholds = c(.15, .30, .50), winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  all_years <- unique(year(time(buffer_mask)))
  
  calculate_persistence <- function(x, threshold) {
    sum(x >= threshold, na.rm = TRUE)
  }
  
  persistence_metrics_per_year <- data.frame(year = rep(all_years, each = length(ice_thresholds)), 
                                             threshold = rep(ice_thresholds, times = length(all_years)),
                                             persistence_sd = NA)
  
  for (year in all_years) {
    winter_data <- subset(buffer_mask, which(year(time(buffer_mask)) == year & month(time(buffer_mask)) %in% winter_months))
    if (nlyr(winter_data) > 0) {
      for (threshold in ice_thresholds) {
        persistence <- app(winter_data, function(x) calculate_persistence(x, threshold))
        persistence_sd <- global(persistence, fun = sd, na.rm = TRUE)
        persistence_metrics_per_year$persistence_sd[persistence_metrics_per_year$year == year & 
                                                      persistence_metrics_per_year$threshold == threshold] <- persistence_sd[1, 1]
      }
    }
  }
  
  return(persistence_metrics_per_year)
}

# Function to get precomputed SD of persistence
get_precomputed_persistence_sd <- function(years, threshold, persistence_metrics_per_year) {
  valid_years <- years[years >= 1980 & years <= 2022]
  if (length(valid_years) == 0) {
    return(NA)
  }
  persistence_sd_values <- persistence_metrics_per_year$persistence_sd[persistence_metrics_per_year$year %in% valid_years & 
                                                                         persistence_metrics_per_year$threshold == threshold]
  if (length(persistence_sd_values) > 0) {
    return(mean(persistence_sd_values, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Function to fit GLS models and summarize results
fit_gls_models_sd <- function(penguin_abundance_filtered, persistence_metrics_per_year, ice_threshold) {
  penguin_abundance_filtered <- penguin_abundance_filtered %>%
    rowwise() %>%
    mutate(
      overwinter_persistence_sd_1yr = ifelse(year - 1 >= 1980, get_precomputed_persistence_sd(year - 1, ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_sd_2yr = ifelse(year - 2 >= 1980, get_precomputed_persistence_sd(c(year - 1, year - 2), ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_sd_3yr = ifelse(year - 3 >= 1980, get_precomputed_persistence_sd(c(year - 1, year - 2, year - 3), ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_sd_4yr = ifelse(year - 4 >= 1980, get_precomputed_persistence_sd(c(year - 1, year - 2, year - 3, year - 4), ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_sd_5yr = ifelse(year - 5 >= 1980, get_precomputed_persistence_sd(c(year - 1, year - 2, year - 3, year - 4, year - 5), ice_threshold, persistence_metrics_per_year), NA)
    ) %>%
    ungroup() %>%
    filter(!is.na(overwinter_persistence_sd_1yr) & !is.na(overwinter_persistence_sd_2yr) & !is.na(overwinter_persistence_sd_3yr) & !is.na(overwinter_persistence_sd_4yr) & !is.na(overwinter_persistence_sd_5yr))
  
  results <- list()
  
  for (lag in 1:5) {
    formula <- as.formula(paste("growth_rate ~ overwinter_persistence_sd_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_abundance_filtered)
    summary_model <- summary(model)
    results[[paste("Lag", lag)]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_values = summary_model$tTable[, 4]
    )
  }
  
  return(results)
}

# Load Gentoo penguin data
penguin_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/gentoo_presence_absence_assumptions.csv")
penguin_abundance_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

# Load study area shapefile
study_area_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
study_area <- st_read(study_area_path)

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1980 to 2022
start_date <- as.Date("1980-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Filter penguin data to keep only consecutively censused sites and clean duplicates
consecutively_censused <- penguin_data %>%
  arrange(site_id, season) %>%
  group_by(site_id) %>%
  mutate(next_season = lead(season),
         next_presence = lead(presence)) %>%
  filter(presence == 1 & next_presence == 1 & season != next_season) %>%
  ungroup() %>%
  select(site_id, season, presence, next_season)

# Merge with penguin abundance data
penguin_abundance_filtered <- penguin_abundance_data %>%
  left_join(consecutively_censused, by = c("site_id", "year" = "season")) %>%
  filter(!is.na(next_season))

print("Penguin abundance data after filtering for consecutively censused sites:")
print(head(penguin_abundance_filtered)) # Debug print

# Define home range directory and sizes
home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
home_range_sizes <- c(25, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results"
dir.create(results_dir, showWarnings = FALSE)

# Initialize dataframes to store all results and significant results
all_results_df <- data.frame()
significant_results_df <- data.frame()

# Loop through each home range size and compute results
for (size in home_range_sizes) {
  buffer_path <- file.path(home_range_dir, paste0("gepe_home_ranges_", size, "km.shp"))
  persistence_metrics_per_year <- compute_sd_persistence(buffer_path, nsidc)
  
  for (threshold in c(.15, .30, .50)) {
    results <- fit_gls_models_sd(penguin_abundance_filtered, persistence_metrics_per_year, threshold)
    
    for (lag in names(results)) {
      result <- results[[lag]]
      result_df <- data.frame(
        HomeRangeSize = size,
        Threshold = threshold,
        Lag = lag,
        AIC = result$AIC,
        BIC = result$BIC,
        Coefficient_Intercept = result$coefficients[1, 1],
        StdError_Intercept = result$coefficients[1, 2],
        tValue_Intercept = result$coefficients[1, 3],
        pValue_Intercept = result$p_values[1],
        Coefficient_Persistence_SD = result$coefficients[2, 1],
        StdError_Persistence_SD = result$coefficients[2, 2],
        tValue_Persistence_SD = result$coefficients[2, 3],
        pValue_Persistence_SD = result$p_values[2],
        Bonferroni_Adjusted_pValue_Persistence_SD = p.adjust(result$p_values[2], method = "bonferroni", n = 5)
      )
      all_results_df <- rbind(all_results_df, result_df)
      
      if (result_df$pValue_Persistence_SD < 0.05) {
        significant_results_df <- rbind(significant_results_df, result_df)
      }
    }
  }
}

write.csv(all_results_df, file.path(results_dir, "model_results_sea_ice_persistence_sd.csv"), row.names = FALSE)
write.csv(significant_results_df, file.path(results_dir, "significant_model_results_sea_ice_persistence_sd.csv"), row.names = FALSE)



```



**Gentoo Sea Ice Persistence (Proportion of Open Water Days) Analysis** (updated - needs checking) Need to implement the monthly calculations 

    For each year in the study period, calculate the proportion of open water days (days with sea ice concentration below a threshold of 15%) during winter months


```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(lubridate)
library(nlme)

# Function to compute mean persistence for all valid years within the specified buffer
compute_mean_persistence <- function(buffer_path, nsidc, ice_thresholds = c(.15, .30, .50), winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  all_years <- unique(year(time(buffer_mask)))
  
  calculate_persistence <- function(x, threshold) {
    sum(x >= threshold, na.rm = TRUE)
  }
  
  persistence_metrics_per_year <- data.frame(year = rep(all_years, each = length(ice_thresholds)), 
                                             threshold = rep(ice_thresholds, times = length(all_years)),
                                             mean_persistence = NA)
  
  for (year in all_years) {
    winter_data <- subset(buffer_mask, which(year(time(buffer_mask)) == year & month(time(buffer_mask)) %in% winter_months))
    if (nlyr(winter_data) > 0) {
      for (threshold in ice_thresholds) {
        persistence <- app(winter_data, function(x) calculate_persistence(x, threshold))
        mean_persistence <- global(persistence, fun = mean, na.rm = TRUE)
        persistence_metrics_per_year$mean_persistence[persistence_metrics_per_year$year == year & 
                                                        persistence_metrics_per_year$threshold == threshold] <- mean_persistence[1, 1]
      }
    }
  }
  
  return(persistence_metrics_per_year)
}

# Function to get precomputed mean persistence
get_precomputed_persistence <- function(years, threshold, persistence_metrics_per_year) {
  valid_years <- years[years >= 1980 & years <= 2022]
  if (length(valid_years) == 0) {
    return(NA)
  }
  mean_persistence_values <- persistence_metrics_per_year$mean_persistence[persistence_metrics_per_year$year %in% valid_years & 
                                                                             persistence_metrics_per_year$threshold == threshold]
  if (length(mean_persistence_values) > 0) {
    return(mean(mean_persistence_values, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Function to fit GLS models and summarize results
fit_gls_models <- function(penguin_abundance_filtered, persistence_metrics_per_year, ice_threshold) {
  penguin_abundance_filtered <- penguin_abundance_filtered %>%
    rowwise() %>%
    mutate(
      overwinter_persistence_1yr = ifelse(year - 1 >= 1980, get_precomputed_persistence(year - 1, ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_2yr = ifelse(year - 2 >= 1980, get_precomputed_persistence(c(year - 1, year - 2), ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_3yr = ifelse(year - 3 >= 1980, get_precomputed_persistence(c(year - 1, year - 2, year - 3), ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_4yr = ifelse(year - 4 >= 1980, get_precomputed_persistence(c(year - 1, year - 2, year - 3, year - 4), ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_5yr = ifelse(year - 5 >= 1980, get_precomputed_persistence(c(year - 1, year - 2, year - 3, year - 4, year - 5), ice_threshold, persistence_metrics_per_year), NA)
    ) %>%
    ungroup() %>%
    filter(!is.na(overwinter_persistence_1yr) & !is.na(overwinter_persistence_2yr) & !is.na(overwinter_persistence_3yr) & !is.na(overwinter_persistence_4yr) & !is.na(overwinter_persistence_5yr))
  
  results <- list()
  
  for (lag in 1:5) {
    formula <- as.formula(paste("growth_rate ~ overwinter_persistence_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_abundance_filtered)
    summary_model <- summary(model)
    results[[paste("Lag", lag)]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_values = summary_model$tTable[, 4]
    )
  }
  
  return(results)
}

# Load Gentoo penguin data
penguin_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/gentoo_presence_absence_assumptions.csv")
penguin_abundance_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

# Load study area shapefile
study_area_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
study_area <- st_read(study_area_path)

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1980 to 2022
start_date <- as.Date("1980-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Filter penguin data to keep only consecutively censused sites and clean duplicates
consecutively_censused <- penguin_data %>%
  arrange(site_id, season) %>%
  group_by(site_id) %>%
  mutate(next_season = lead(season),
         next_presence = lead(presence)) %>%
  filter(presence == 1 & next_presence == 1 & season != next_season) %>%
  ungroup() %>%
  select(site_id, season, presence, next_season)

# Merge with penguin abundance data
penguin_abundance_filtered <- penguin_abundance_data %>%
  left_join(consecutively_censused, by = c("site_id", "year" = "season")) %>%
  filter(!is.na(next_season))

print("Penguin abundance data after filtering for consecutively censused sites:")
print(head(penguin_abundance_filtered)) # Debug print

# Define home range directory and sizes
home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
home_range_sizes <- c(25, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results"
dir.create(results_dir, showWarnings = FALSE)

# Initialize dataframes to store all results and significant results
all_results_df <- data.frame()
significant_results_df <- data.frame()

# Loop through each home range size and compute results
for (size in home_range_sizes) {
  buffer_path <- file.path(home_range_dir, paste0("gepe_home_ranges_", size, "km.shp"))
  persistence_metrics_per_year <- compute_mean_persistence(buffer_path, nsidc)
  
  for (threshold in c(.15, .30, .50)) {
    results <- fit_gls_models(penguin_abundance_filtered, persistence_metrics_per_year, threshold)
    
    for (lag in names(results)) {
      result <- results[[lag]]
      result_df <- data.frame(
        HomeRangeSize = size,
        Threshold = threshold,
        Lag = lag,
        AIC = result$AIC,
        BIC = result$BIC,
        Coefficient_Intercept = result$coefficients[1, 1],
        StdError_Intercept = result$coefficients[1, 2],
        tValue_Intercept = result$coefficients[1, 3],
        pValue_Intercept = result$p_values[1],
        Coefficient_Persistence = result$coefficients[2, 1],
        StdError_Persistence = result$coefficients[2, 2],
        tValue_Persistence = result$coefficients[2, 3],
        pValue_Persistence = result$p_values[2],
        Bonferroni_Adjusted_pValue_Persistence = p.adjust(result$p_values[2], method = "bonferroni", n = 5)
      )
      all_results_df <- rbind(all_results_df, result_df)
      
      if (result_df$pValue_Persistence < 0.05) {
        significant_results_df <- rbind(significant_results_df, result_df)
      }
    }
  }
}

write.csv(all_results_df, file.path(results_dir, "model_results_sea_ice_persistence.csv"), row.names = FALSE)
write.csv(significant_results_df, file.path(results_dir, "significant_model_results_sea_ice_persistence.csv"), row.names = FALSE)


```


***Persistence Varaibility** (updated - needs checking) Need to implement the monthly calculations 
```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(lubridate)
library(nlme)

# Function to compute SD of sea ice persistence for all valid years within the specified buffer
compute_sd_persistence <- function(buffer_path, nsidc, ice_thresholds = c(.15, .30, .50), winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  all_years <- unique(year(time(buffer_mask)))
  
  calculate_persistence <- function(x, threshold) {
    sum(x >= threshold, na.rm = TRUE)
  }
  
  persistence_metrics_per_year <- data.frame(year = rep(all_years, each = length(ice_thresholds)), 
                                             threshold = rep(ice_thresholds, times = length(all_years)),
                                             persistence_sd = NA)
  
  for (year in all_years) {
    winter_data <- subset(buffer_mask, which(year(time(buffer_mask)) == year & month(time(buffer_mask)) %in% winter_months))
    if (nlyr(winter_data) > 0) {
      for (threshold in ice_thresholds) {
        persistence <- app(winter_data, function(x) calculate_persistence(x, threshold))
        persistence_sd <- global(persistence, fun = sd, na.rm = TRUE)
        persistence_metrics_per_year$persistence_sd[persistence_metrics_per_year$year == year & 
                                                      persistence_metrics_per_year$threshold == threshold] <- persistence_sd[1, 1]
      }
    }
  }
  
  return(persistence_metrics_per_year)
}

# Function to get precomputed SD of persistence
get_precomputed_persistence_sd <- function(years, threshold, persistence_metrics_per_year) {
  valid_years <- years[years >= 1980 & years <= 2022]
  if (length(valid_years) == 0) {
    return(NA)
  }
  persistence_sd_values <- persistence_metrics_per_year$persistence_sd[persistence_metrics_per_year$year %in% valid_years & 
                                                                         persistence_metrics_per_year$threshold == threshold]
  if (length(persistence_sd_values) > 0) {
    return(mean(persistence_sd_values, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Function to fit GLS models and summarize results
fit_gls_models_sd <- function(penguin_abundance_filtered, persistence_metrics_per_year, ice_threshold) {
  penguin_abundance_filtered <- penguin_abundance_filtered %>%
    rowwise() %>%
    mutate(
      overwinter_persistence_sd_1yr = ifelse(year - 1 >= 1980, get_precomputed_persistence_sd(year - 1, ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_sd_2yr = ifelse(year - 2 >= 1980, get_precomputed_persistence_sd(c(year - 1, year - 2), ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_sd_3yr = ifelse(year - 3 >= 1980, get_precomputed_persistence_sd(c(year - 1, year - 2, year - 3), ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_sd_4yr = ifelse(year - 4 >= 1980, get_precomputed_persistence_sd(c(year - 1, year - 2, year - 3, year - 4), ice_threshold, persistence_metrics_per_year), NA),
      overwinter_persistence_sd_5yr = ifelse(year - 5 >= 1980, get_precomputed_persistence_sd(c(year - 1, year - 2, year - 3, year - 4, year - 5), ice_threshold, persistence_metrics_per_year), NA)
    ) %>%
    ungroup() %>%
    filter(!is.na(overwinter_persistence_sd_1yr) & !is.na(overwinter_persistence_sd_2yr) & !is.na(overwinter_persistence_sd_3yr) & !is.na(overwinter_persistence_sd_4yr) & !is.na(overwinter_persistence_sd_5yr))
  
  results <- list()
  
  for (lag in 1:5) {
    formula <- as.formula(paste("growth_rate ~ overwinter_persistence_sd_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_abundance_filtered)
    summary_model <- summary(model)
    results[[paste("Lag", lag)]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_values = summary_model$tTable[, 4]
    )
  }
  
  return(results)
}

# Load Gentoo penguin data
penguin_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/gentoo_presence_absence_assumptions.csv")
penguin_abundance_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

# Load study area shapefile
study_area_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
study_area <- st_read(study_area_path)

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1980 to 2022
start_date <- as.Date("1980-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Filter penguin data to keep only consecutively censused sites and clean duplicates
consecutively_censused <- penguin_data %>%
  arrange(site_id, season) %>%
  group_by(site_id) %>%
  mutate(next_season = lead(season),
         next_presence = lead(presence)) %>%
  filter(presence == 1 & next_presence == 1 & season != next_season) %>%
  ungroup() %>%
  select(site_id, season, presence, next_season)

# Merge with penguin abundance data
penguin_abundance_filtered <- penguin_abundance_data %>%
  left_join(consecutively_censused, by = c("site_id", "year" = "season")) %>%
  filter(!is.na(next_season))

print("Penguin abundance data after filtering for consecutively censused sites:")
print(head(penguin_abundance_filtered)) # Debug print

# Define home range directory and sizes
home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
home_range_sizes <- c(25, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results"
dir.create(results_dir, showWarnings = FALSE)

# Initialize dataframes to store all results and significant results
all_results_df <- data.frame()
significant_results_df <- data.frame()

# Loop through each home range size and compute results
for (size in home_range_sizes) {
  buffer_path <- file.path(home_range_dir, paste0("gepe_home_ranges_", size, "km.shp"))
  persistence_metrics_per_year <- compute_sd_persistence(buffer_path, nsidc)
  
  for (threshold in c(.15, .30, .50)) {
    results <- fit_gls_models_sd(penguin_abundance_filtered, persistence_metrics_per_year, threshold)
    
    for (lag in names(results)) {
      result <- results[[lag]]
      result_df <- data.frame(
        HomeRangeSize = size,
        Threshold = threshold,
        Lag = lag,
        AIC = result$AIC,
        BIC = result$BIC,
        Coefficient_Intercept = result$coefficients[1, 1],
        StdError_Intercept = result$coefficients[1, 2],
        tValue_Intercept = result$coefficients[1, 3],
        pValue_Intercept = result$p_values[1],
        Coefficient_Persistence_SD = result$coefficients[2, 1],
        StdError_Persistence_SD = result$coefficients[2, 2],
        tValue_Persistence_SD = result$coefficients[2, 3],
        pValue_Persistence_SD = result$p_values[2],
        Bonferroni_Adjusted_pValue_Persistence_SD = p.adjust(result$p_values[2], method = "bonferroni", n = 5)
      )
      all_results_df <- rbind(all_results_df, result_df)
      
      if (result_df$pValue_Persistence_SD < 0.05) {
        significant_results_df <- rbind(significant_results_df, result_df)
      }
    }
  }
}

write.csv(all_results_df, file.path(results_dir, "model_results_sea_ice_persistence_sd.csv"), row.names = FALSE)
write.csv(significant_results_df, file.path(results_dir, "significant_model_results_sea_ice_persistence_sd.csv"), row.names = FALSE)


```




**Sea Ice Duration** (updated) - Need to implement the monthly calculations 

```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(stringr)

# Function to compute mean duration of sea ice concentration above a threshold
compute_mean_duration <- function(buffer_path, nsidc, threshold = .15, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  all_years <- unique(year(time(buffer_mask)))
  
  mean_duration_results <- data.frame(year = all_years, mean_duration = NA)
  
  for (year in all_years) {
    winter_data <- subset(buffer_mask, which(year(time(buffer_mask)) == year & month(time(buffer_mask)) %in% winter_months))
    if (nlyr(winter_data) > 0) {
      durations <- app(winter_data, function(x) {
        rle_result <- rle(x > threshold)
        durations <- rle_result$lengths[rle_result$values]
        return(mean(durations, na.rm = TRUE))
      })
      mean_duration <- mean(values(durations), na.rm = TRUE)
      
      # Debugging print statements
      print(paste("Year:", year, "Mean Duration:", mean_duration))
      
      mean_duration_results$mean_duration[mean_duration_results$year == year] <- mean_duration
    }
  }
  
  return(mean_duration_results)
}

# Function to fit GLS models and summarize results
fit_gls_models <- function(penguin_abundance_filtered, duration_metrics) {
  results <- list()
  for (lag in 1:5) {
    penguin_abundance_filtered <- penguin_abundance_filtered %>%
      rowwise() %>%
      mutate(!!paste0("overwinter_duration_", lag, "yr") := ifelse(year - lag >= 1980, duration_metrics$mean_duration[duration_metrics$year == year - lag], NA)) %>%
      ungroup()
  }
  
  for (lag in 1:5) {
    penguin_abundance_filtered_lag <- penguin_abundance_filtered %>%
      filter(!is.na(!!sym(paste0("overwinter_duration_", lag, "yr"))))
    
    formula <- as.formula(paste("growth_rate ~ overwinter_duration_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_abundance_filtered_lag)
    summary_model <- summary(model)
    
    results[[paste("Lag", lag, sep = "_")]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_value = summary_model$tTable[2, 4]
    )
  }
  
  return(results)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds) {
  # Load Gentoo penguin data
  penguin_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/gentoo_presence_absence_assumptions.csv")
  penguin_abundance_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")
  
  # Load study area shapefile
  study_area_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
  study_area <- st_read(study_area_path)
  
  # Load the NSIDC sea ice concentration data
  nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")
  
  # Filter the sea ice data from 2005 to 2023
  start_date <- as.Date("1981-01-01")
  end_date <- as.Date("2023-09-30")
  nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))
  
  # Adjust the season column in the penguin abundance data
  penguin_abundance_data <- penguin_abundance_data %>%
    mutate(year = 1970 + season - 1)
  
  # Filter penguin data to keep only consecutively censused sites and clean duplicates
  consecutively_censused <- penguin_data %>%
    arrange(site_id, season) %>%
    group_by(site_id) %>%
    mutate(next_season = lead(season),
           next_presence = lead(presence)) %>%
    filter(presence == 1 & next_presence == 1 & season != next_season) %>%
    ungroup() %>%
    select(site_id, season, presence, next_season)
  
  # Merge with penguin abundance data
  penguin_abundance_filtered <- penguin_abundance_data %>%
    left_join(consecutively_censused, by = c("site_id", "year" = "season")) %>%
    filter(!is.na(next_season))
  
  print("Penguin abundance data after filtering for consecutively censused sites:")
  print(head(penguin_abundance_filtered)) # Debug print
  
  # Define home range directory
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  # Create results directory
  results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results"
  dir.create(results_dir, showWarnings = FALSE)
  
  # Initialize dataframes to store all results and significant results
  all_results_df <- data.frame()
  significant_results_df <- data.frame()
  
  # Loop through each home range shapefile and compute results
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      duration_metrics <- compute_mean_duration(buffer_path, nsidc, threshold = threshold)
      results <- fit_gls_models(penguin_abundance_filtered, duration_metrics)
      
      for (lag in 1:5) {
        result <- results[[paste0("Lag_", lag)]]
        result_df <- data.frame(
          Threshold_Duration = threshold,
          HomeRangeSize_Duration = home_range_size,
          Lag_Duration = lag,
          AIC_Duration = result$AIC,
          BIC_Duration = result$BIC,
          Coefficient_Intercept_Duration = result$coefficients[1, 1],
          StdError_Intercept_Duration = result$coefficients[1, 2],
          tValue_Intercept_Duration = result$coefficients[1, 3],
          pValue_Intercept_Duration = result$coefficients[1, 4],
          Coefficient_Duration = result$coefficients[2, 1],
          StdError_Duration = result$coefficients[2, 2],
          tValue_Duration = result$coefficients[2, 3],
          pValue_Duration = result$coefficients[2, 4],
          Bonferroni_Adjusted_pValue_Duration = p.adjust(result$coefficients[2, 4], method = "bonferroni", n = 5)
        )
        all_results_df <- rbind(all_results_df, result_df)
        
        # Check if the result is significant
        if (result_df$pValue_Duration < 0.05) {
          significant_results_df <- rbind(significant_results_df, result_df)
        }
      }
    }
  }
  
  # Export the compiled results to CSV
  write.csv(all_results_df, file.path(results_dir, "model_results_sea_ice_duration.csv"), row.names = FALSE)
  
  # Export the significant results to a separate CSV
  write.csv(significant_results_df, file.path(results_dir, "significant_model_results_sea_ice_duration.csv"), row.names = FALSE)
}

# Example usage
thresholds <- c(.15, .30, .50)  # You can change the thresholds here
results <- analyze_sea_ice_effect(thresholds)





```

**Sea Ice Duration Variability** - Need to implement the monthly calculations 

In the provided script, variability is calculated by first identifying durations of consecutive days where sea ice concentration exceeds a given threshold. This is achieved using the rle (run-length encoding) function to find runs of days meeting the threshold. For each year, the mean duration of these runs is computed. The standard deviation (SD) of these durations is then calculated to quantify the variability, providing a measure of how consistent the durations of sea ice persistence are across different years. This SD is used as a metric of variability in subsequent analyses.

```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(stringr)
library(lubridate)

# Function to compute mean duration and variability of sea ice concentration above a threshold
compute_duration_and_variability <- function(buffer_path, nsidc, threshold, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  all_years <- unique(year(time(buffer_mask)))
  
  results <- data.frame(year = all_years, mean_duration = NA, variability = NA)
  
  for (year in all_years) {
    winter_data <- subset(buffer_mask, which(year(time(buffer_mask)) == year & month(time(buffer_mask)) %in% winter_months))
    if (nlyr(winter_data) > 0) {
      durations <- app(winter_data, function(x) {
        rle_result <- rle(x > threshold)
        durations <- rle_result$lengths[rle_result$values]
        return(mean(durations, na.rm = TRUE))
      })
      mean_duration <- mean(values(durations), na.rm = TRUE)
      variability <- sd(values(durations), na.rm = TRUE)
      
      # Debugging print statements
      print(paste("Year:", year, "Mean Duration:", mean_duration, "Variability:", variability))
      
      results$mean_duration[results$year == year] <- mean_duration
      results$variability[results$year == year] <- variability
    }
  }
  
  return(results)
}

# Function to fit GLS models and summarize results
fit_gls_models <- function(penguin_abundance_filtered, metrics) {
  results <- list()
  for (lag in 1:5) {
    penguin_abundance_filtered <- penguin_abundance_filtered %>%
      rowwise() %>%
      mutate(!!paste0("overwinter_duration_", lag, "yr") := ifelse(year - lag >= 1980, metrics$mean_duration[metrics$year == year - lag], NA),
             !!paste0("variability_", lag, "yr") := ifelse(year - lag >= 1980, metrics$variability[metrics$year == year - lag], NA)) %>%
      ungroup()
  }
  
  for (lag in 1:5) {
    penguin_abundance_filtered_lag <- penguin_abundance_filtered %>%
      filter(!is.na(!!sym(paste0("overwinter_duration_", lag, "yr"))) & !is.na(!!sym(paste0("variability_", lag, "yr"))))
    
    formula <- as.formula(paste("growth_rate ~ overwinter_duration_", lag, "yr + variability_", lag, "yr", sep = ""))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_abundance_filtered_lag)
    summary_model <- summary(model)
    
    results[[paste("Lag", lag, sep = "_")]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_value_duration = summary_model$tTable[2, 4],
      p_value_variability = summary_model$tTable[3, 4]
    )
  }
  
  return(results)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds) {
  # Load Gentoo penguin data
  penguin_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/gentoo_presence_absence_assumptions.csv")
  penguin_abundance_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")
  
  # Load study area shapefile
  study_area_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
  study_area <- st_read(study_area_path)
  
  # Load the NSIDC sea ice concentration data
  nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")
  
  
  # Filter the sea ice data from 1981 to 2023
  start_date <- as.Date("1981-01-01")
  end_date <- as.Date("2023-09-30")
  nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))
  
  # Adjust the season column in the penguin abundance data
  penguin_abundance_data <- penguin_abundance_data %>%
    mutate(year = 1970 + season - 1)
  
  # Filter penguin data to keep only consecutively censused sites and clean duplicates
  consecutively_censused <- penguin_data %>%
    arrange(site_id, season) %>%
    group_by(site_id) %>%
    mutate(next_season = lead(season),
           next_presence = lead(presence)) %>%
    filter(presence == 1 & next_presence == 1 & season != next_season) %>%
    ungroup() %>%
    select(site_id, season, presence, next_season)
  
  # Merge with penguin abundance data
  penguin_abundance_filtered <- penguin_abundance_data %>%
    left_join(consecutively_censused, by = c("site_id", "year" = "season")) %>%
    filter(!is.na(next_season))
  
  print("Penguin abundance data after filtering for consecutively censused sites:")
  print(head(penguin_abundance_filtered)) # Debug print
  
  # Define home range directory
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  # Create results directory
  results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results"
  dir.create(results_dir, showWarnings = FALSE)
  
  # Initialize dataframes to store all results and significant results
  all_results_df <- data.frame()
  significant_results_df <- data.frame()
  
  # Loop through each home range shapefile and compute results
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      metrics <- compute_duration_and_variability(buffer_path, nsidc, threshold = threshold)
      results <- fit_gls_models(penguin_abundance_filtered, metrics)
      
      for (lag in 1:5) {
        result <- results[[paste0("Lag_", lag)]]
        result_df <- data.frame(
          Threshold_Duration = threshold,
          HomeRangeSize_Duration = home_range_size,
          Lag_Duration = lag,
          AIC_Duration = result$AIC,
          BIC_Duration = result$BIC,
          Coefficient_Intercept_Duration = result$coefficients[1, 1],
          StdError_Intercept_Duration = result$coefficients[1, 2],
          tValue_Intercept_Duration = result$coefficients[1, 3],
          pValue_Intercept_Duration = result$coefficients[1, 4],
          Coefficient_Duration = result$coefficients[2, 1],
          StdError_Duration = result$coefficients[2, 2],
          tValue_Duration = result$coefficients[2, 3],
          pValue_Duration = result$p_value_duration,
          Bonferroni_Adjusted_pValue_Duration = p.adjust(result$p_value_duration, method = "bonferroni", n = 5),
          Coefficient_Duration-Variability = result$coefficients[3, 1],
          StdError_Duration-Variability = result$coefficients[3, 2],
          tValue_Duration-Variability= result$coefficients[3, 3],
          pValue_Duration-Variability = result$p_value_variability,
          Bonferroni_Adjusted_pValue_Duration-Variability = p.adjust(result$p_value_variability, method = "bonferroni", n = 5)
        )
        all_results_df <- rbind(all_results_df, result_df)
      }
    }
  }
  
  # Filter significant results using dplyr
  significant_results_df <- all_results_df %>%
    filter(pValue_Duration < 0.05 | pValue_Variability < 0.05)
  
  # Export the compiled results to CSV
  write.csv(all_results_df, file.path(results_dir, "model_results_sea_ice_duration_variability.csv"), row.names = FALSE)
  
  # Export the significant results to a separate CSV
  write.csv(significant_results_df, file.path(results_dir, "significant_model_results_sea_ice_duration_variability.csv"), row.names = FALSE)
}

# Example usage
thresholds <- c(.15, .30, .50)  # You can change the thresholds here
results <- analyze_sea_ice_effect(thresholds)
print("All Results:")
print(results$all_results)
print("Significant Results:")
print(results$significant_results)
```



```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(stringr)
library(lubridate)

# Function to compute mean duration and variability of sea ice concentration above a threshold
compute_duration_and_variability <- function(buffer_path, nsidc, threshold, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  all_years <- unique(year(time(buffer_mask)))
  
  results <- expand.grid(year = all_years, month = winter_months, stringsAsFactors = FALSE)
  results$mean_duration <- NA
  results$variability <- NA
  
  calculate_durations <- function(x, threshold) {
    rle_result <- rle(x > threshold)
    durations <- rle_result$lengths[rle_result$values]
    return(mean(durations, na.rm = TRUE))
  }
  
  for (year in all_years) {
    for (month in winter_months) {
      month_data <- subset(buffer_mask, which(year(time(buffer_mask)) == year & month(time(buffer_mask)) == month))
      if (nlyr(month_data) > 0) {
        durations <- app(month_data, function(x) calculate_durations(x, threshold))
        mean_duration <- mean(values(durations), na.rm = TRUE)
        variability <- sd(values(durations), na.rm = TRUE)
        
        results$mean_duration[results$year == year & results$month == month] <- mean_duration
        results$variability[results$year == year & results$month == month] <- variability
      }
    }
  }
  
  annual_results <- results %>%
    group_by(year) %>%
    summarise(
      mean_duration = mean(mean_duration, na.rm = TRUE),
      variability = sd(mean_duration, na.rm = TRUE)
    )
  
  list(monthly = results, annual = annual_results)
}

# Function to fit GLS models and summarize results
fit_gls_models <- function(penguin_abundance_filtered, metrics, type = "monthly") {
  results <- list()
  for (lag in 1:5) {
    if (type == "monthly") {
      penguin_abundance_filtered <- penguin_abundance_filtered %>%
        rowwise() %>%
        mutate(
          overwinter_duration = ifelse(year - lag >= 1980, metrics$mean_duration[metrics$year == year - lag & metrics$month == month], NA),
          variability = ifelse(year - lag >= 1980, metrics$variability[metrics$year == year - lag & metrics$month == month], NA)
        ) %>%
        ungroup()
    } else {
      penguin_abundance_filtered <- penguin_abundance_filtered %>%
        rowwise() %>%
        mutate(
          overwinter_duration = ifelse(year - lag >= 1980, metrics$mean_duration[metrics$year == year - lag], NA),
          variability = ifelse(year - lag >= 1980, metrics$variability[metrics$year == year - lag], NA)
        ) %>%
        ungroup()
    }
  }
  
  for (lag in 1:5) {
    penguin_abundance_filtered_lag <- penguin_abundance_filtered %>%
      filter(!is.na(overwinter_duration) & !is.na(variability))
    
    formula <- as.formula(paste("growth_rate ~ overwinter_duration + variability"))
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_abundance_filtered_lag)
    summary_model <- summary(model)
    
    results[[paste("Lag", lag, sep = "_")]] <- list(
      AIC = AIC(model),
      BIC = BIC(model),
      coefficients = summary_model$tTable,
      p_value_duration = summary_model$tTable[2, 4],
      p_value_variability = summary_model$tTable[3, 4]
    )
  }
  
  return(results)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds) {
  # Load Gentoo penguin data
  penguin_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/gentoo_presence_absence_assumptions.csv")
  penguin_abundance_data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")
  
  # Load study area shapefile
  study_area_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
  study_area <- st_read(study_area_path)
  
  # Load the NSIDC sea ice concentration data
  nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")
  
  # Filter the sea ice data from 1981 to 2023
  start_date <- as.Date("1981-01-01")
  end_date <- as.Date("2023-09-30")
  nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))
  
  # Adjust the season column in the penguin abundance data
  penguin_abundance_data <- penguin_abundance_data %>%
    mutate(year = 1970 + season - 1)
  
  # Filter penguin data to keep only consecutively censused sites and clean duplicates
  consecutively_censused <- penguin_data %>%
    arrange(site_id, season) %>%
    group_by(site_id) %>%
    mutate(next_season = lead(season),
           next_presence = lead(presence)) %>%
    filter(presence == 1 & next_presence == 1 & season != next_season) %>%
    ungroup() %>%
    select(site_id, season, presence, next_season)
  
  # Merge with penguin abundance data
  penguin_abundance_filtered <- penguin_abundance_data %>%
    left_join(consecutively_censused, by = c("site_id", "year" = "season")) %>%
    filter(!is.na(next_season))
  
  print("Penguin abundance data after filtering for consecutively censused sites:")
  print(head(penguin_abundance_filtered)) # Debug print
  
  # Define home range directory
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  # Create results directory
  results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results"
  dir.create(results_dir, showWarnings = FALSE)
  
  # Initialize dataframes to store all results and significant results
  all_results_df <- data.frame()
  significant_results_df <- data.frame()
  
  # Loop through each home range shapefile and compute results
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      metrics <- compute_duration_and_variability(buffer_path, nsidc, threshold = threshold)
      
      # Fit GLS models for monthly data
      monthly_results <- fit_gls_models(penguin_abundance_filtered, metrics$monthly, type = "monthly")
      
      for (lag in 1:5) {
        result <- monthly_results[[paste0("Lag_", lag)]]
        result_df <- data.frame(
          Threshold = threshold,
          HomeRangeSize = home_range_size,
          Lag = lag,
          Type = "Monthly",
          AIC = result$AIC,
          BIC = result$BIC,
          Coefficient_Intercept = result$coefficients[1, 1],
          StdError_Intercept = result$coefficients[1, 2],
          tValue_Intercept = result$coefficients[1, 3],
          pValue_Intercept = result$coefficients[1, 4],
          Coefficient_Duration = result$coefficients[2, 1],
          StdError_Duration = result$coefficients[2, 2],
          tValue_Duration = result$coefficients[2, 3],
          pValue_Duration = result$p_value_duration,
          Bonferroni_Adjusted_pValue_Duration = p.adjust(result$p_value_duration, method = "bonferroni", n = 5),
          Coefficient_Duration_Variability = result$coefficients[3, 1],
          StdError_Duration_Variability = result$coefficients[3, 2],
          tValue_Duration_Variability = result$coefficients[3, 3],
          pValue_Duration_Variability = result$p_value_variability,
          Bonferroni_Adjusted_pValue_Duration_Variability = p.adjust(result$p_value_variability, method = "bonferroni", n = 5)
        )
        all_results_df <- rbind(all_results_df, result_df)
      }
      
      # Fit GLS models for annual data
      annual_results <- fit_gls_models(penguin_abundance_filtered, metrics$annual, type = "annual")
      
      for (lag in 1:5) {
        result <- annual_results[[paste0("Lag_", lag)]]
        result_df <- data.frame(
          Threshold = threshold,
          HomeRangeSize = home_range_size,
          Lag = lag,
          Type = "Annual",
          AIC = result$AIC,
          BIC = result$BIC,
          Coefficient_Intercept = result$coefficients[1, 1],
          StdError_Intercept = result$coefficients[1, 2],
          tValue_Intercept = result$coefficients[1, 3],
          pValue_Intercept = result$coefficients[1, 4],
          Coefficient_Duration = result$coefficients[2, 1],
          StdError_Duration = result$coefficients[2, 2],
          tValue_Duration = result$coefficients[2, 3],
          pValue_Duration = result$p_value_duration,
          Bonferroni_Adjusted_pValue_Duration = p.adjust(result$p_value_duration, method = "bonferroni", n = 5),
          Coefficient_Duration_Variability = result$coefficients[3, 1],
          StdError_Duration_Variability = result$coefficients[3, 2],
          tValue_Duration_Variability = result$coefficients[3, 3],
          pValue_Duration_Variability = result$p_value_variability,
          Bonferroni_Adjusted_pValue_Duration_Variability = p.adjust(result$p_value_variability, method = "bonferroni", n = 5)
        )
        all_results_df <- rbind(all_results_df, result_df)
      }
    }
  }
  
  # Filter significant results using dplyr
  significant_results_df <- all_results_df %>%
    filter(pValue_Duration < 0.05 | pValue_Duration_Variability < 0.05)
  
  # Export the compiled results to CSV
  write.csv(all_results_df, file.path(results_dir, "model_results_sea_ice_duration_variability.csv"), row.names = FALSE)
  
  # Export the significant results to a separate CSV
  write.csv(significant_results_df, file.path(results_dir, "significant_model_results_sea_ice_duration_variability.csv"), row.names = FALSE)
}

# Example usage
thresholds <- c(.15, .30, .50)  # You can change the thresholds here
analyze_sea_ice_effect(thresholds)


```





**Compile Significant Results**
```{r}

# Load necessary libraries
library(dplyr)
library(readr)

# Define the directory containing the CSV files
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/significant"

# Get a list of all CSV files in the directory
csv_files <- list.files(path = results_dir, pattern = "*.csv", full.names = TRUE)

# Initialize an empty list to store the dataframes
dfs <- list()

# Loop through each CSV file and read the data
for (file in csv_files) {
  # Read the current CSV file using readr
  df <- read_csv(file, col_types = cols(.default = "c"))  # Read all columns as character
  
  # Append the dataframe to the list
  dfs <- append(dfs, list(df))
}

# Combine all dataframes into one, filling missing columns with NA
combined_results_df <- bind_rows(dfs)

# Define the output file path
output_file <- file.path(results_dir, "combined_significant_results.csv")

# Export the combined results to a new CSV file
write_csv(combined_results_df, output_file)

# Print a message indicating the completion of the process
cat("Combined results have been saved to:", output_file, "\n")


```