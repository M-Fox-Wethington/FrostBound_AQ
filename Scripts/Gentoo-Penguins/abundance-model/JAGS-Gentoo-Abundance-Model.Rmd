---
title: "Gentoo Model Parameter Abundance Model"
author: "Michael J. Wethington"
date: "2024-06-04"
output: html_document
---


zi = latent nest abundance (mean-adjusted)
lz = logged abundance (re-expression of above):  lzi,t=log(zi,t). for the ith site in the tth year,
ri = intrinsic growth rate
lp = predicted population growth rate multiplier
la = actual population growth rate multiplier 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



**Generate Gentoo Growth Parameters**

```{r}

# Load required libraries
library(tidyverse)
library(coda)
library(mapppdr)
library(patchwork)
library(leaflet)
library(CCAMLRGIS)
library(rjags)
library(MCMCvis)
library(parallel)
library(stringi)
library(pander)
library(testthat)

# Define parameters
min_season <- 1970
max_season <- 2023
species <- "GEPE"

# Construct Presence-Absence Assumptions CSV for the JAGS model
penguin_obs <- mapppdr::penguin_obs

penguin_obs_processed <- penguin_obs %>%
  filter(species_id == species) %>%
  mutate(
    presence = ifelse(!is.na(count), 1, 0),
    known_w = 1) %>%
  select(site_id, season, presence, known_w, count, accuracy, type)

presence_absence_assumptions <- expand.grid(
  site_id = unique(penguin_obs$site_id),
  season = min_season:max_season
) %>%
  left_join(penguin_obs_processed, by = c("site_id", "season")) %>%
  mutate(
    presence = ifelse(is.na(presence), 0, presence),
    known_w = ifelse(is.na(known_w), 0, known_w),
    count = ifelse(is.na(count), 0, count)) %>%
  group_by(site_id, season) %>%
  arrange(desc(type), desc(accuracy), .by_group = TRUE) %>%
  slice(1) %>%
  ungroup() %>%
  select(site_id, season, presence, known_w, count, accuracy)

# Load the JAGS MCMC Output File
load("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/model_data_rinits_output.rda")

# Extract the Gentoo Abundance Estimates (lz)
model_samples <- as.matrix(model_data_rinits_output)

# Filter columns that are logged latent abundance (lz) parameters
lz_columns <- grep("^lz\\[", colnames(model_samples))
lz_samples <- model_samples[, lz_columns]

# Convert log-abundances to actual abundances
abundance_samples <- exp(lz_samples)

# Summarize the actual abundances
abundance_summary <- apply(abundance_samples, 2, function(x) {
  c(mean = mean(x), median = median(x), 
    lower_95 = quantile(x, 0.025), upper_95 = quantile(x, 0.975))
})

# Convert to a readable data frame (use t to transpose)
abundance_summary_df <- as.data.frame(t(abundance_summary))

# Extract site and season info from the indices
extract_indices <- function(colname) {
  indices <- gsub("[^0-9,]", "", colname)
  as.integer(unlist(strsplit(indices, ",")))
}

indices <- lapply(colnames(abundance_samples), extract_indices)
sites <- sapply(indices, `[`, 1)
seasons <- sapply(indices, `[`, 2)

abundance_summary_df$site <- sites
abundance_summary_df$season <- seasons

abundance_summary_df <- abundance_summary_df %>% 
  rename(mean_abundance = mean,
         median_abundance = median,
         lower_95_abundance = "lower_95.2.5%",
         upper_95_abundance = "upper_95.97.5%")

head(abundance_summary_df)

# Load and prepare SiteList
SiteList <- mapppdr::penguin_obs %>%
  filter(count > 0 & species_id == species & season >= min_season & season <= max_season) %>%
  mutate(season_relative = season - min_season + 1) %>%
  group_by(site_id) %>%
  summarise(initial_season = min(season_relative)) %>%
  ungroup() %>%
  left_join(mapppdr::sites, by = "site_id") %>%
  mutate(site = as.numeric(as.factor(site_id))) %>%
  select(site_id, site_name, ccamlr_id, site, initial_season, latitude, longitude)

(n_sites <- nrow(SiteList))

SiteList %>% 
  distinct(site, site_id, latitude, longitude)

# Join SiteList with abundance_summary_df
final_data <- left_join(SiteList, abundance_summary_df, by = "site")

# Ensure final_data is sorted by site and season
final_data <- final_data %>%
  arrange(site, season)

# Calculate the growth rate and append it to the data
final_data <- final_data %>%
  group_by(site) %>%
  mutate(growth_rate = mean_abundance / lag(mean_abundance)) %>%
  ungroup()

# Adjust the season column in the final data
final_data <- final_data %>%
  mutate(year = 1970 + season - 1)

# Display the adjusted final_data
print(head(final_data))

write.csv(final_data, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/modeled_gentoo_parameters.csv")
# Print the head of the final data with growth rates
print(head(final_data))

```

**calculate and plot geometric mean of growth rates** 
```{r}
library(ggplot2)
library(dplyr)

# Calculate geometric mean of growth rates
final_data <- final_data %>%
  group_by(site) %>%
  mutate(geometric_mean_growth_rate = exp(mean(log(growth_rate), na.rm = TRUE))) %>%
  ungroup()

# Filter data for the specific site
site_data <- final_data %>% filter(site_id == "AITC")

# Calculate the mean growth multiplier for the site
mean_growth <- site_data %>% 
  summarize(mean_growth_multiplier = exp(mean(log(growth_rate), na.rm = TRUE)),
            lower_ci = exp(mean(log(growth_rate), na.rm = TRUE) - 1.96 * sd(log(growth_rate), na.rm = TRUE)/sqrt(n())),
            upper_ci = exp(mean(log(growth_rate), na.rm = TRUE) + 1.96 * sd(log(growth_rate), na.rm = TRUE)/sqrt(n())))

# Check if the 'count' and 'type' columns exist and correct them if needed
if (!"count" %in% names(site_data)) {
  site_data$count <- site_data$mean_abundance # or any other logic that fits
}

if (!"type" %in% names(site_data)) {
  site_data$type <- "nests" # default type, adjust as necessary
}

# Add year column based on season
site_data <- site_data %>%
  mutate(year = 1970 + season - 1)

# Generate the plot
ggplot(site_data, aes(x = year, y = mean_abundance)) +
  geom_boxplot(aes(group = year), fill = "orange", alpha = 0.6) +
  geom_errorbar(aes(ymin = lower_95_abundance, ymax = upper_95_abundance), width = 0.2) +
  geom_point(data = site_data %>% filter(!is.na(count)), aes(y = count, color = type), size = 3, shape = 21, fill = "blue") +
  scale_color_manual(values = c("nests" = "red", "chicks" = "blue")) +
  labs(title = "AITC, Barrientos Island (Aitcho Islands), 48.1",
       subtitle = paste0("mean population growth multiplier = ", round(mean_growth$mean_growth_multiplier, 3), 
                         " (", round(mean_growth$lower_ci, 2), " - ", round(mean_growth$upper_ci, 2), ")"),
       x = "Year",
       y = "Abundance",
       color = "Type") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))


```


**Calculate Sea Ice concentration and pass to dataframe**

```{r}



# This script analyzes daily sea ice concentration (SIC) and extent within defined home ranges by calculating the daily mean SIC, standard deviation of SIC, and total ice-covered area (sea ice extent) above a threshold for each day. It loads NSIDC sea ice data, applies a threshold to set low values to zero, and computes the specified metrics for each home range. The sea ice extent is calculated as the total area of cells with SIC above the threshold, effectively representing the "total ice-covered area" within the home range. The results are compiled and exported to a CSV file for further analysis.

# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(stringr)
library(lubridate)

# Function to compute daily mean SIC, SD SIC, and sea ice extent above a threshold
compute_daily_sic_statistics <- function(buffer_path, nsidc, threshold = 0.15, cell_area_sq_meters) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  
  # Extract dates before masking
  dates <- time(nsidc)
  
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  
  # Set all values < threshold to 0 for the entire raster stack
  buffer_mask <- app(buffer_mask, fun = function(x) { x[x < threshold] <- 0; return(x) })
  
  # Calculate mean and SD SIC excluding NAs for each layer
  mean_sic <- global(buffer_mask, fun = 'mean', na.rm = TRUE)[, 1]
  sd_sic <- global(buffer_mask, fun = 'sd', na.rm = TRUE)[, 1]
  
  # Calculate sea ice extent (total cells above threshold in square kilometers)
  valid_ice_cells <- global(buffer_mask >= threshold, fun = 'sum', na.rm = TRUE)[, 1]
  total_ice_area_sq_km <- (valid_ice_cells * cell_area_sq_meters) / 1e6  # Convert total ice area to square kilometers
  
  results <- data.frame(
    date = as.Date(dates, origin = "1970-01-01"),
    mean_sic = mean_sic,
    sd_sic = sd_sic,
    ice_extent_km2 = total_ice_area_sq_km
  )
  
  return(results)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds, nsidc_subset, results_dir) {
  # Initialize dataframe to store all results
  all_results_df <- data.frame()
  
  # Calculate the cell area in square meters (only once)
  cell_area_sq_meters <- prod(res(nsidc_subset))
  
  # Loop through each home range shapefile and compute results
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      metrics <- compute_daily_sic_statistics(buffer_path, nsidc_subset, threshold = threshold, cell_area_sq_meters = cell_area_sq_meters)
      
      metrics <- metrics %>%
        mutate(Threshold = threshold, HomeRangeSize = home_range_size)
      
      all_results_df <- bind_rows(all_results_df, metrics)
    }
  }
  
  # Export the compiled results to CSV
  write.csv(all_results_df, file.path(results_dir, "daily_sic_statistics.csv"), row.names = FALSE)
  
  return(all_results_df)
}

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1981 to 2023
start_date <- as.Date("1981-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Subset the NSIDC data for testing (e.g., first 100 layers)
# nsidc_subset <- subset(nsidc, 1:100)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
dir.create(results_dir, showWarnings = FALSE)

# Run the analysis on the subset
all_results_df <- analyze_sea_ice_effect(c(0.15), nsidc, results_dir)

# Display the results
head(all_results_df)




```

**Calculate Persistence and Duration metrics and pass to dataframe**
```{r}

# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(stringr)

# Function to compute mean duration, standard deviation, mean persistence, and persistence standard deviation of sea ice concentration above a threshold
compute_duration_persistence_stats <- function(buffer_path, nsidc, threshold = .15, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  
  # Extract dates before masking
  dates <- time(nsidc)
  
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  
  all_years <- unique(year(dates))
  
  duration_persistence_stats <- data.frame(
    year = integer(),
    month = character(),
    mean_duration = numeric(),
    sd_duration = numeric(),
    mean_persistence = numeric(),
    sd_persistence = numeric()
  )
  
  calculate_metrics <- function(data, threshold) {
    durations <- app(data, function(x) {
      rle_result <- rle(x > threshold)
      durations <- rle_result$lengths[rle_result$values]
      return(mean(durations, na.rm = TRUE))
    })
    mean_duration <- mean(values(durations), na.rm = TRUE)
    sd_duration <- sd(values(durations), na.rm = TRUE)
    
    open_water_prop <- app(data, function(x) mean(x < threshold, na.rm = TRUE))
    mean_persistence <- mean(values(open_water_prop), na.rm = TRUE)
    sd_persistence <- sd(values(open_water_prop), na.rm = TRUE)
    
    return(list(mean_duration = mean_duration, sd_duration = sd_duration, mean_persistence = mean_persistence, sd_persistence = sd_persistence))
  }
  
  for (year in all_years) {
    for (month in winter_months) {
      monthly_indices <- which(year(dates) == year & month(dates) == month)
      if (length(monthly_indices) > 0) {
        monthly_data <- buffer_mask[[monthly_indices]]
        metrics <- calculate_metrics(monthly_data, threshold)
        
        duration_persistence_stats <- rbind(duration_persistence_stats, data.frame(
          year = year,
          month = month,
          mean_duration = metrics$mean_duration,
          sd_duration = metrics$sd_duration,
          mean_persistence = metrics$mean_persistence,
          sd_persistence = metrics$sd_persistence
        ))
      }
    }
    
    season_indices <- which(year(dates) == year & month(dates) %in% winter_months)
    if (length(season_indices) > 0) {
      season_data <- buffer_mask[[season_indices]]
      metrics <- calculate_metrics(season_data, threshold)
      
      duration_persistence_stats <- rbind(duration_persistence_stats, data.frame(
        year = year,
        month = "Season-wide",
        mean_duration = metrics$mean_duration,
        sd_duration = metrics$sd_duration,
        mean_persistence = metrics$mean_persistence,
        sd_persistence = metrics$sd_persistence
      ))
    }
  }
  
  return(duration_persistence_stats)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds, nsidc_subset, results_dir) {
  # Initialize dataframe to store all results
  all_duration_persistence_stats <- data.frame()
  
  # Define home range directory
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  # Loop through each home range shapefile and compute results
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      duration_persistence_stats <- compute_duration_persistence_stats(buffer_path, nsidc_subset, threshold = threshold)
      duration_persistence_stats$Threshold <- threshold
      duration_persistence_stats$HomeRangeSize <- home_range_size
      all_duration_persistence_stats <- bind_rows(all_duration_persistence_stats, duration_persistence_stats)
    }
  }
  
  # Export the compiled results to CSV
  write.csv(all_duration_persistence_stats, file.path(results_dir, "sea_ice_duration_persistence_stats_subset.csv"), row.names = FALSE)
  
  # Export the compiled results to RDS
  saveRDS(all_duration_persistence_stats, file.path(results_dir, "sea_ice_duration_persistence_stats.rds"))
}

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1981 to 2023
start_date <- as.Date("1981-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Subset the NSIDC data for testing (e.g., first 100 layers)
# nsidc_subset <- subset(nsidc, 1:100)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
dir.create(results_dir, showWarnings = FALSE)

# Run the analysis on the subset
analyze_sea_ice_effect(c(.15, .30, .50), nsidc, results_dir)

# Display the results
all_duration_persistence_stats <- read.csv(file.path(results_dir, "sea_ice_duration_persistence_stats_subset.csv"))
head(all_duration_persistence_stats)



```


**Run GLS Models on lagged year for Sea ice concentration**

```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(lubridate)
library(data.table)

# Function to calculate monthly average standard deviation for Extent
calculate_monthly_sd <- function(ice_data) {
  ice_data <- ice_data %>%
    mutate(date = as.Date(date, format = "%Y-%m-%d"),
           year_month = format(date, "%Y-%m")) %>%
    group_by(year_month, Threshold, HomeRangeSize) %>%
    summarize(Extent_SD = sd(ice_extent_km2, na.rm = TRUE), .groups = 'drop') %>%
    mutate(date = as.Date(paste0(year_month, "-01"))) %>%
    select(-year_month)
  return(ice_data)
}

# Function to merge monthly Extent_SD with daily data
merge_monthly_sd <- function(daily_data, monthly_sd_data) {
  daily_data <- daily_data %>%
    mutate(date = as.Date(date, format = "%Y-%m-%d"),
           year_month = format(date, "%Y-%m")) %>%
    left_join(monthly_sd_data, by = c("date", "Threshold", "HomeRangeSize")) %>%
    select(-year_month)
  return(daily_data)
}

# General function to compute annual ice metrics
compute_annual_ice_metrics <- function(ice_data, metrics) {
  ice_data <- ice_data %>%
    mutate(date = as.Date(date, format = "%Y-%m-%d"),
           year = year(date))
  
  annual_ice_metrics <- ice_data %>%
    group_by(year, Threshold, HomeRangeSize) %>%
    summarize(across(all_of(metrics), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}"),
              across(all_of(metrics), ~sd(.x, na.rm = TRUE), .names = "sd_{.col}"),
              .groups = 'drop')
  
  return(annual_ice_metrics)
}

# Function to get individual precomputed ice metrics
get_individual_ice_metrics <- function(year, ice_data, metric) {
  ice_value <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_value) > 0) {
    return(ice_value)
  } else {
    return(NA)
  }
}

# Function to fit GLS models for individual lag years
fit_gls_models_indiv <- function(penguin_data, ice_data, metric, threshold, home_range_size) {
  results <- list()
  
  for (lag in 1:5) {
    penguin_data <- penguin_data %>%
      rowwise() %>%
      mutate(overwinter_ice = get_individual_ice_metrics(year - lag, ice_data, metric)) %>%
      ungroup() %>%
      filter(!is.na(overwinter_ice) & !is.na(growth_rate) & growth_rate <= 3)
    
    if (nrow(penguin_data) < 3) next
    
    print(paste("Fitting GLS model for individual lag", lag, "year(s)"))
    formula <- as.formula("growth_rate ~ overwinter_ice")
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
    summary_model <- summary(model)
    bonferroni_p_value <- p.adjust(summary_model$tTable[2, 4], method = "bonferroni", n = 5)
    
    if (bonferroni_p_value < 0.05) {
      results[[paste("Lag_Indiv", lag)]] <- list(
        AIC = AIC(model),
        BIC = BIC(model),
        coefficients = summary_model$tTable,
        p_value = summary_model$tTable[2, 4],
        bonferroni_p_value = bonferroni_p_value,
        model = model,
        penguin_data = penguin_data
      )
    }
  }
  
  return(results)
}

# Main function to run GLS analysis for each condition and save results
run_gls_analysis <- function(penguin_abundance_data, ice_data, metrics, results_dir) {
  # Initialize results list
  all_gls_results_indiv <- list()
  all_results_df_indiv <- list()
  significant_results_df_indiv <- list()
  
  # Compute monthly average standard deviation
  monthly_sd_data <- calculate_monthly_sd(ice_data)
  
  # Merge monthly SD data with daily data
  ice_data <- merge_monthly_sd(ice_data, monthly_sd_data)
  
  # Compute annual ice metrics
  annual_ice_metrics <- compute_annual_ice_metrics(ice_data, metrics)
  
  # List unique thresholds and home range sizes
  thresholds <- unique(annual_ice_metrics$Threshold)
  home_range_sizes <- unique(annual_ice_metrics$HomeRangeSize)
  
  for (metric in metrics) {
    for (threshold in thresholds) {
      for (home_range_size in home_range_sizes) {
        ice_subset <- annual_ice_metrics %>%
          filter(Threshold == threshold, HomeRangeSize == home_range_size)
        
        print(paste("Running GLS analysis for metric:", metric, "threshold:", threshold, "home range size:", home_range_size))
        
        # Perform GLS analysis for individual lag years
        gls_results_indiv <- fit_gls_models_indiv(penguin_abundance_data, ice_subset, paste("mean_", metric, sep = ""), threshold, home_range_size)
        
        # Store results for individual lag years
        all_gls_results_indiv[[paste("Annual_Indiv", metric, threshold, home_range_size, sep = "_")]] <- gls_results_indiv
        
        results_list_indiv <- lapply(names(gls_results_indiv), function(lag) {
          result <- gls_results_indiv[[lag]]
          data.frame(
            Metric = metric,
            HomeRangeSize = home_range_size,
            Threshold = threshold,
            Lag = lag,
            AIC = result$AIC,
            BIC = result$BIC,
            Coefficient_Intercept = result$coefficients[1, 1],
            StdError_Intercept = result$coefficients[1, 2],
            tValue_Intercept = result$coefficients[1, 3],
            pValue_Intercept = result$coefficients[1, 4],
            Coefficient = result$coefficients[2, 1],
            StdError = result$coefficients[2, 2],
            tValue = result$coefficients[2, 3],
            pValue = result$coefficients[2, 4],
            Bonferroni_Adjusted_pValue = result$bonferroni_p_value
          )
        })
        
        all_results_df_indiv <- append(all_results_df_indiv, results_list_indiv)
        significant_results_df_indiv <- append(significant_results_df_indiv, Filter(function(x) x$Bonferroni_Adjusted_pValue < 0.05, results_list_indiv))
      }
    }
  }
  
  # Export the compiled results to CSV
  all_results_df_indiv <- do.call(rbind, all_results_df_indiv)
  significant_results_df_indiv <- do.call(rbind, significant_results_df_indiv)
  
  write.csv(all_results_df_indiv, file.path(results_dir, "model_results_indiv_ice_metrics_extent_extentsd.csv"), row.names = FALSE)
  write.csv(significant_results_df_indiv, file.path(results_dir, "significant_model_results_indiv_ice_metrics_extent_extentsd.csv"), row.names = FALSE)
  
  # Save only significant results to RDS
  significant_gls_results_indiv <- list()
  for (metric in names(all_gls_results_indiv)) {
    significant_gls_results_indiv[[metric]] <- Filter(Negate(is.null), all_gls_results_indiv[[metric]])
  }
  saveRDS(significant_gls_results_indiv, file.path(results_dir, "gls_analysis_results_indiv_concentration_extent_extentsd.rds"))
}

### Load Data and Run Analysis

# Load Gentoo penguin data
penguin_abundance_data <- fread("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

str(penguin_abundance_data)

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Load the SIC statistics
metric_calculation_csv <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
ice_data <- fread(file.path(metric_calculation_csv, "daily_sic_statistics.csv"))
str(ice_data)

# Define the metrics to analyze
metrics <- c("mean_sic", "sd_sic", "ice_extent_km2", "Extent_SD")

# Define the results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"
dir.create(results_dir, showWarnings = FALSE)

# Run the GLS analysis on the full dataset
results_output <- run_gls_analysis(penguin_abundance_data, ice_data, metrics, results_dir)

```

**Run GLS Models on lagged year for Persistence and Duration**
```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(lubridate)
library(data.table)

# Function to get individual precomputed ice metrics
get_individual_ice_metrics <- function(year, ice_data, metric) {
  ice_values <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_values) > 0) {
    return(mean(ice_values, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Function: Fit GLS models for individual lag years
fit_gls_models_indiv <- function(penguin_data, ice_data, metric) {
  results <- list()
  
  for (lag in 1:5) {
    penguin_data <- penguin_data %>%
      rowwise() %>%
      mutate(overwinter_ice = get_individual_ice_metrics(year - lag, ice_data, metric)) %>%
      ungroup() %>%
      filter(!is.na(overwinter_ice))
    
    if (nrow(penguin_data) < 3) next
    
    print(paste("Fitting GLS model for individual lag", lag, "year(s)"))
    formula <- as.formula("growth_rate ~ overwinter_ice")
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
    summary_model <- summary(model)
    bonferroni_p_value <- p.adjust(summary_model$tTable[2, 4], method = "bonferroni", n = 5)
    
    if (bonferroni_p_value < 0.05) {
      results[[paste("Lag_Indiv", lag)]] <- list(
        AIC = AIC(model),
        BIC = BIC(model),
        coefficients = summary_model$tTable,
        p_value = summary_model$tTable[2, 4],
        bonferroni_p_value = bonferroni_p_value,
        model = model,
        penguin_data = penguin_data
      )
    }
  }
  
  return(results)
}

# Main function to run GLS analysis for each condition and save results
run_gls_analysis <- function(penguin_abundance_data, ice_data, metrics, results_dir) {
  # Initialize results list
  all_gls_results_indiv <- list()
  all_results_df_indiv <- list()
  significant_results_df_indiv <- list()
  
  # List unique thresholds and home range sizes
  thresholds <- unique(ice_data$Threshold)
  home_range_sizes <- unique(ice_data$HomeRangeSize)
  
  for (metric in metrics) {
    for (threshold in thresholds) {
      for (home_range_size in home_range_sizes) {
        ice_subset <- ice_data %>%
          filter(Threshold == threshold, HomeRangeSize == home_range_size)
        
        print(paste("Running GLS analysis for metric:", metric, "threshold:", threshold, "home range size:", home_range_size))
        
        # Perform GLS analysis for individual lag years
        gls_results_indiv <- fit_gls_models_indiv(penguin_abundance_data, ice_subset, metric)
        
        # Store results for individual lag years
        all_gls_results_indiv[[paste("Annual_Indiv", metric, threshold, home_range_size, sep = "_")]] <- gls_results_indiv
        
        results_list_indiv <- lapply(names(gls_results_indiv), function(lag) {
          result <- gls_results_indiv[[lag]]
          data.frame(
            Metric = metric,
            HomeRangeSize = home_range_size,
            Threshold = threshold,
            Lag = lag,
            AIC = result$AIC,
            BIC = result$BIC,
            Coefficient_Intercept = result$coefficients[1, 1],
            StdError_Intercept = result$coefficients[1, 2],
            tValue_Intercept = result$coefficients[1, 3],
            pValue_Intercept = result$coefficients[1, 4],
            Coefficient = result$coefficients[2, 1],
            StdError = result$coefficients[2, 2],
            tValue = result$coefficients[2, 3],
            pValue = result$coefficients[2, 4],
            Bonferroni_Adjusted_pValue = result$bonferroni_p_value
          )
        })
        
        all_results_df_indiv <- append(all_results_df_indiv, results_list_indiv)
        significant_results_df_indiv <- append(significant_results_df_indiv, Filter(function(x) x$Bonferroni_Adjusted_pValue < 0.05, results_list_indiv))
      }
    }
  }
  
  # Export the compiled results to CSV
  for (metric in metrics) {
    metric_results_df_indiv <- do.call(rbind, lapply(all_results_df_indiv, function(x) if (x$Metric == metric) x else NULL))
    metric_significant_results_df_indiv <- do.call(rbind, lapply(significant_results_df_indiv, function(x) if (x$Metric == metric) x else NULL))
    
    write.csv(metric_results_df_indiv, file.path(results_dir, paste0("model_results_indiv_", metric, ".csv")), row.names = FALSE)
    write.csv(metric_significant_results_df_indiv, file.path(results_dir, paste0("significant_model_results_indiv_", metric, ".csv")), row.names = FALSE)
  }
  
  # Save only significant results to RDS
  significant_gls_results_indiv <- list()
  for (metric in names(all_gls_results_indiv)) {
    significant_gls_results_indiv[[metric]] <- Filter(Negate(is.null), all_gls_results_indiv[[metric]])
  }
  saveRDS(significant_gls_results_indiv, file.path(results_dir, "gls_analysis_results_indiv.rds"))
}

# Example Usage for a Small Subset

# Load Gentoo penguin data
penguin_abundance_data <- fread("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Load the SIC statistics
metric_calculation_csv <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
ice_data <- fread(file.path(metric_calculation_csv, "sea_ice_duration_persistence_stats.csv"))

# Define the metrics to analyze
metrics <- c("mean_duration", "sd_duration", "mean_persistence", "sd_persistence")

# Define the results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"
dir.create(results_dir, showWarnings = FALSE)

run_gls_analysis(penguin_abundance_data, ice_data, metrics, results_dir)




# Load necessary libraries
library(dplyr)
library(readr)

# Define the directory containing the significant CSV files
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"

# Define the metrics to analyze
metrics <- c("mean_duration", "sd_duration", "mean_persistence", "sd_persistence")

# Initialize an empty list to store the data frames
all_significant_results <- list()

# Loop through each metric and read the corresponding CSV file
for (metric in metrics) {
  file_path <- file.path(results_dir, paste0("significant_model_results_indiv_", metric, ".csv"))
  
  # Check if the file exists
  if (file.exists(file_path)) {
    # Read the CSV file and add the metric as a new column
    data <- read_csv(file_path) %>%
      mutate(Metric_Type = metric)
    all_significant_results[[metric]] <- data
  } else {
    cat("File does not exist:", file_path, "\n")
  }
}

# Combine all data frames into a single data frame
merged_significant_results <- bind_rows(all_significant_results)

# Save the merged data frame to a new CSV file
merged_file_path <- file.path(results_dir, "merged_significant_model_results_duration_persistence.csv")
write_csv(merged_significant_results, merged_file_path)

cat("Merged CSV file saved to:", merged_file_path, "\n")



```



**Compile and organize Significant Results**
```{r}
# Load necessary libraries
library(data.table)
library(dplyr)

# Define the directory paths
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"
significant_results_dir <- file.path(results_dir, "bonferroni_significant_results")

# Create the new subdirectory if it doesn't exist
dir.create(significant_results_dir, showWarnings = FALSE)

# Function to clean HomeRangeSize column
clean_home_range_size <- function(df) {
  df %>%
    mutate(HomeRangeSize = as.numeric(gsub("km", "", HomeRangeSize)))
}

# Function to create subsets of data based on Bonferroni significance criteria and Metric
subset_significant_results <- function(file_path) {
  # Load the data
  data <- fread(file_path)
  
  # Clean the HomeRangeSize column
  data <- clean_home_range_size(data)
  
  # Get unique metrics
  metrics <- unique(data$Metric)
  
  # Initialize a list to store subsets
  metric_data_list <- list()
  
  # Loop through each metric and create subset
  for (metric in metrics) {
    metric_data <- data %>%
      filter(Metric == metric, Bonferroni_Adjusted_pValue < 0.05)
    
    if (nrow(metric_data) > 0) {
      metric_data_list[[metric]] <- metric_data
    }
  }
  
  return(metric_data_list)
}

# Get a list of CSV files in the directory
csv_files <- list.files(path = results_dir, pattern = "*.csv", full.names = TRUE)

# Apply the function to each CSV file and bind the results by Metric
all_significant_results <- lapply(csv_files, subset_significant_results)
all_significant_results <- do.call(rbind, unlist(all_significant_results, recursive = FALSE))

# Split the data by Metric and save to separate CSV files
split_data <- split(all_significant_results, all_significant_results$Metric)

# Save each subset to a separate CSV file
lapply(names(split_data), function(metric) {
  output_file_name <- paste0(metric, "_bonferroni_significant.csv")
  output_file_path <- file.path(significant_results_dir, output_file_name)
  write.csv(split_data[[metric]], output_file_path, row.names = FALSE)
})


```


**Plot SIC and Concentration Results for Single model**
```{r}
# Load necessary libraries
library(nlme)
library(data.table)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(showtext)
library(patchwork)

# Define file paths
model_results_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results/model_results_indiv_ice_metrics.csv"

daily_sic_statistics_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv/daily_sic_statistics.csv"

modeled_gentoo_parameters_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv"

# Load the data files
model_results_df <- fread(model_results_path)
daily_sic_statistics_df <- fread(daily_sic_statistics_path)
modeled_gentoo_parameters_df <- fread(modeled_gentoo_parameters_path)

# Compute annual ice metrics
compute_annual_ice_metrics <- function(ice_data, metrics) {
  ice_data <- ice_data %>%
    mutate(year = year(date))
  
  annual_ice_metrics <- ice_data %>%
    group_by(year, Threshold, HomeRangeSize) %>%
    summarize(across(all_of(metrics), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}"),
              across(all_of(metrics), ~sd(.x, na.rm = TRUE), .names = "sd_{.col}"),
              .groups = 'drop')
  
  return(annual_ice_metrics)
}

# Define the metrics to analyze
metrics <- c("mean_sic", "ice_extent_km2", "sd_sic")

# Compute the annual ice metrics
annual_ice_metrics <- compute_annual_ice_metrics(daily_sic_statistics_df, metrics)

# Function to get precomputed ice metrics
get_precomputed_ice_metrics <- function(year, ice_data, metric) {
  ice_value <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_value) > 0) {
    return(mean(ice_value, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Calculate the average sea ice concentration, extent, sd_sic, and sd_extent from the prior year
penguin_data_with_ice <- modeled_gentoo_parameters_df %>%
  rowwise() %>%
  mutate(overwinter_ice_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_mean_sic"),
         overwinter_extent_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_ice_extent_km2"),
         overwinter_sd_sic_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_sd_sic"),
         overwinter_sd_extent_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "sd_ice_extent_km2")) %>%
  ungroup()

# Filter out growth rates above 3
filtered_penguin_data <- penguin_data_with_ice %>%
  filter(growth_rate <= 3)

# Function to fit GLS model for average metric from 1 year with filtered data
fit_gls_model_avg <- function(penguin_data, metric) {
  penguin_data <- penguin_data %>%
    filter(!is.na(!!sym(metric)), !is.na(growth_rate))
  
  print(paste("Fitting GLS model for average lag 1 year for", metric))
  formula <- as.formula(paste("growth_rate ~", metric))
  model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
  
  return(model)
}

# Fit the models for the new metrics
gls_models <- list()
metrics_list <- c("overwinter_ice_1yr", "overwinter_extent_1yr", "overwinter_sd_sic_1yr", "overwinter_sd_extent_1yr")

for (metric in metrics_list) {
  gls_models[[metric]] <- fit_gls_model_avg(filtered_penguin_data, metric)
}

# Get confidence intervals for the model coefficients
conf_intervals <- lapply(gls_models, intervals)

# Extract the confidence intervals for the intercept and slope
conf_intervals_list <- lapply(conf_intervals, function(ci) {
  list(
    intercept_ci = ci$coef[1, ],
    slope_ci = ci$coef[2, ]
  )
})

# Plot settings
font <- "Gudea"
font_add_google(family = font, font, db_cache = TRUE)
showtext_auto()

theme_set(theme_minimal(base_family = font, base_size = 12))
bg <- "#F4F5F1"
txt_col <- "black"

# Function to create prediction data and plot
create_plot <- function(penguin_data, gls_model, intercept_ci, slope_ci, metric, x_label, title) {
  new_data <- data.frame(overwinter_metric = seq(min(penguin_data[[metric]], na.rm = TRUE),
                                                 max(penguin_data[[metric]], na.rm = TRUE),
                                                 length.out = 100))
  
  colnames(new_data) <- metric  # Rename the column back to its original name
  
  predicted_values <- predict(gls_model, new_data)
  
  new_data$fit <- predicted_values
  new_data$upper <- intercept_ci[3] + slope_ci[3] * new_data[[metric]]
  new_data$lower <- intercept_ci[1] + slope_ci[1] * new_data[[metric]]
  
  plot <- ggplot() +
    geom_point(data = penguin_data, aes_string(x = metric, y = "growth_rate"), color = "black", size = 1) +
    geom_line(data = new_data, aes_string(x = metric, y = "fit"), color = "blue") +
    geom_ribbon(data = new_data, aes_string(x = metric, ymin = "lower", ymax = "upper"), alpha = 0.2) +
    labs(title = title,
         x = x_label,
         y = "Growth Rate") +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      plot.caption = element_text(hjust = 0.5, size = 10),
      plot.margin = margin(20, 20, 20, 20)
    ) +
    ylim(NA, 1.5)
  
  return(plot)
}

# Create the plots with updated x-axis labels
plot_mean_sic <- create_plot(filtered_penguin_data, gls_models[["overwinter_ice_1yr"]],
                             conf_intervals_list[["overwinter_ice_1yr"]]$intercept_ci,
                             conf_intervals_list[["overwinter_ice_1yr"]]$slope_ci,
                             "overwinter_ice_1yr", "Winter SIC Mean", "Growth Rate vs. Mean SIC")

plot_sd_sic <- create_plot(filtered_penguin_data, gls_models[["overwinter_sd_sic_1yr"]],
                           conf_intervals_list[["overwinter_sd_sic_1yr"]]$intercept_ci,
                           conf_intervals_list[["overwinter_sd_sic_1yr"]]$slope_ci,
                           "overwinter_sd_sic_1yr", "Winter SIC SD", "Growth Rate vs. SD SIC")

plot_extent <- create_plot(filtered_penguin_data, gls_models[["overwinter_extent_1yr"]],
                           conf_intervals_list[["overwinter_extent_1yr"]]$intercept_ci,
                           conf_intervals_list[["overwinter_extent_1yr"]]$slope_ci,
                           "overwinter_extent_1yr", "Winter Extent", "Growth Rate vs. Ice Extent")

plot_sd_extent <- create_plot(filtered_penguin_data, gls_models[["overwinter_sd_extent_1yr"]],
                              conf_intervals_list[["overwinter_sd_extent_1yr"]]$intercept_ci,
                              conf_intervals_list[["overwinter_sd_extent_1yr"]]$slope_ci,
                              "overwinter_sd_extent_1yr", "Winter Extent SD", "Growth Rate vs. SD Ice Extent")

# Save the plot objects
save(plot_mean_sic, plot_sd_sic, plot_extent, plot_sd_extent, file = "penguin_growth_sic_extent_rate_plots.RData")

# Load the plot objects (if needed in the future)
# load("penguin_growth_sic_extent_rate_plots.RData")

# Combine the plots i
# n a 2x2 grid
final_plot <- (plot_mean_sic + plot_sd_sic) / (plot_extent + plot_sd_extent) +
  plot_layout(heights = c(2, 2)) +
  plot_annotation(
    theme = theme(
      plot.background = element_rect(fill = bg, color = NA),
      plot.margin = margin(20, 20, 20, 20)
    )
  )

# Display the final plot
print(final_plot)

# Save the figure
ggsave("growth_rate_effects_sic_extent.png", plot = final_plot, bg = bg, height = 10, width = 12, dpi = 300)


```



**Plot Forest Plots - Concentration/Extent**


```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(readr)

# Define the directory containing the significant CSV files
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results/concentration_extent"

# Define the metrics to analyze
metrics <- c("Sea Ice Concentration", "Concentration SD", "Sea Ice Extent", "Sea Ice Extent SD")

# Load the data
data <- read_csv(file.path(results_dir, "significant_model_results_indiv_ice_metrics_extent_extentsd.csv"))

# Reorder the factor levels for HomeRangeSize and Metric
data$HomeRangeSize <- factor(data$HomeRangeSize, levels = c("25km", "50km", "100km", "150km", "200km", "250km", "300km", "350km", "400km", "450km", "500km"))
data$Metric <- recode(data$Metric, 
                      'mean_sic' = 'Sea Ice Concentration', 
                      'sd_sic' = 'Concentration SD', 
                      'ice_extent_km2' = 'Sea Ice Extent',
                      'Extent_SD' = 'Sea Ice Extent SD')

# Recode the Lag column
data$Lag <- recode(data$Lag, 
                   'Lag_Indiv 1' = '1 Year Lag', 
                   'Lag_Indiv 2' = '2 Year Lag', 
                   'Lag_Indiv 3' = '3 Year Lag', 
                   'Lag_Indiv 4' = '4 Year Lag', 
                   'Lag_Indiv 5' = '5 Year Lag')

# Function to create and save forest plots
create_forest_plot <- function(data, metric, results_dir) {
  if (nrow(data) == 0) {
    cat("No data available for", metric, "\n")
    return(NULL)
  }
  
  plot <- ggplot(data, aes(x = HomeRangeSize, y = Coefficient, color = Metric)) +
    geom_point() +
    geom_errorbar(aes(ymin = Coefficient - StdError, ymax = Coefficient + StdError), width = 0.2) +
    facet_wrap(~ Lag, scales = 'free_x', nrow = 2) +
    labs(title = paste("Forest Plot of Coefficients for", metric),
         x = "Home Range Size",
         y = "Coefficient") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Print the plot
  print(plot)
  
  # Save the plot as PNG
  ggsave(file.path(results_dir, paste0("forest_plot_", metric, ".png")), plot, width = 10, height = 8)
  
  # Save the plot as PDF
  pdf(file.path(results_dir, paste0("forest_plot_", metric, ".pdf")), width = 10, height = 8)
  print(plot)
  dev.off()
}

# Generate forest plots for each metric
for (metric in metrics) {
  print(metric)
  metric_data <- data %>% filter(Metric == metric)
  create_forest_plot(metric_data, metric, results_dir)
}

# Generate the combined forest plot
combined_plot <- ggplot(data, aes(x = HomeRangeSize, y = Coefficient, color = Metric)) +
  geom_point() +
  geom_errorbar(aes(ymin = Coefficient - StdError, ymax = Coefficient + StdError), width = 0.2) +
  facet_wrap(~ Lag, scales = 'free_x', nrow = 2) +
  labs(title = "Forest Plot of Coefficients by Metric, Home Range Size, and Lag",
       x = "Home Range Size",
       y = "Coefficient") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the combined plot
print(combined_plot)

# Save the combined plot as PNG
ggsave(file.path(results_dir, "forest_plot_combined_metrics.png"), combined_plot, width = 12, height = 10)

# Save the combined plot as PDF
pdf(file.path(results_dir, "forest_plot_combined_metrics.pdf"), width = 12, height = 10)
print(combined_plot)
dev.off()


```


**Plot Persistence and Duration Forest Plots**

```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(ggplot2)

# Define the directory containing the significant CSV files
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"

# Define the metrics to analyze
metrics <- c("mean_duration", "sd_duration", "mean_persistence", "sd_persistence")

# Initialize an empty list to store the data frames
all_significant_results <- list()

# Loop through each metric and read the corresponding CSV file
for (metric in metrics) {
  file_path <- file.path(results_dir, paste0("significant_model_results_indiv_", metric, ".csv"))
  
  # Check if the file exists
  if (file.exists(file_path)) {
    # Read the CSV file and add the metric as a new column
    data <- read_csv(file_path) %>%
      mutate(Metric_Type = metric)
    all_significant_results[[metric]] <- data
  } else {
    cat("File does not exist:", file_path, "\n")
  }
}

# Combine all data frames into a single data frame
merged_significant_results <- bind_rows(all_significant_results)

# Save the merged data frame to a new CSV file
merged_file_path <- file.path(results_dir, "merged_significant_model_results_duration_persistence.csv")
write_csv(merged_significant_results, merged_file_path)

cat("Merged CSV file saved to:", merged_file_path, "\n")

# Load the merged data for plotting
data <- read_csv(merged_file_path)

# Reorder the factor levels for HomeRangeSize and Metric
data$HomeRangeSize <- factor(data$HomeRangeSize, levels = c("25km", "50km", "100km", "150km", "200km", "250km", "300km", "350km", "400km", "450km", "500km"))
data$Metric <- recode(data$Metric, 
                      'mean_duration' = 'Duration', 
                      'sd_duration' = 'Duration SD', 
                      'mean_persistence' = 'Open Water Frequency',
                      'sd_persistence' = 'Open Water Frequency SD')

# Recode the Lag column
data$Lag <- recode(data$Lag, 
                   'Lag_Indiv 1' = '1 Year Lag', 
                   'Lag_Indiv 2' = '2 Year Lag', 
                   'Lag_Indiv 3' = '3 Year Lag', 
                   'Lag_Indiv 4' = '4 Year Lag',
                   'Lag_Indiv 5' = '5 Year Lag')

# Function to create and save forest plots
create_forest_plot <- function(data, metric, threshold, results_dir) {
  if (nrow(data) == 0) {
    cat("No data available for", metric, "at", threshold * 100, "% threshold\n")
    return(NULL)
  }
  
  plot <- ggplot(data, aes(x = HomeRangeSize, y = Coefficient, color = Metric)) +
    geom_point() +
    geom_errorbar(aes(ymin = Coefficient - StdError, ymax = Coefficient + StdError), width = 0.2) +
    facet_wrap(~ Lag, scales = 'free_x', nrow = 2) +
    labs(title = paste("Forest Plot of Coefficients for", metric, "at", threshold * 100, "% Threshold"),
         x = "Home Range Size",
         y = "Coefficient") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Print the plot
  print(plot)
  
  # Save the plot as PNG
  ggsave(file.path(results_dir, paste0("forest_plot_", metric, "_", threshold * 100, "pct.png")), plot, width = 10, height = 8)
  
  # Save the plot as PDF
  pdf(file.path(results_dir, paste0("forest_plot_", metric, "_", threshold * 100, "pct.pdf")), width = 10, height = 8)
  print(plot)
  dev.off()
}

# Generate forest plots for each metric and threshold
thresholds <- unique(data$Threshold)
for (metric in metrics) {
  for (threshold in thresholds) {
    metric_data <- data %>% filter(Metric_Type == metric, Threshold == threshold)
    create_forest_plot(metric_data, metric, threshold, results_dir)
  }
}

# Generate the combined forest plot
combined_plot <- ggplot(data, aes(x = HomeRangeSize, y = Coefficient, color = Metric)) +
  geom_point() +
  geom_errorbar(aes(ymin = Coefficient - StdError, ymax = Coefficient + StdError), width = 0.2) +
  facet_wrap(~ Lag + Threshold, scales = 'free_x', nrow = 2) +
  labs(title = "Forest Plot of Coefficients by Metric, Home Range Size, Lag, and Threshold",
       x = "Home Range Size",
       y = "Coefficient") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the combined plot
print(combined_plot)

# Save the combined plot as PNG
ggsave(file.path(results_dir, "Figure_forest_plot_combined_metrics_thresholds_Durations-Persistence.png"), combined_plot, width = 12, height = 10)

# Save the combined plot as PDF
pdf(file.path(results_dir, "Figure_forest_plot_combined_metrics_thresholds_Durations-Persistence.pdf"), width = 12, height = 10)
print(combined_plot)
dev.off()


```

**Plot Persistence and Duration for single model**

```{r}
# Load necessary libraries
library(nlme)
library(data.table)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(showtext)
library(patchwork)

# Define file paths
sea_ice_duration_persistence_stats_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv/sea_ice_duration_persistence_stats.csv"
modeled_gentoo_parameters_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv"

# Load the data files
sea_ice_duration_persistence_df <- fread(sea_ice_duration_persistence_stats_path)
modeled_gentoo_parameters_df <- fread(modeled_gentoo_parameters_path)

# Compute annual ice metrics
compute_annual_ice_metrics <- function(ice_data, metrics) {
  ice_data <- ice_data %>%
    filter(month == "Season-wide") %>%
    mutate(year = as.integer(year))
  
  annual_ice_metrics <- ice_data %>%
    group_by(year, Threshold, HomeRangeSize) %>%
    summarize(across(all_of(metrics), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}"),
              .groups = 'drop')
  
  return(annual_ice_metrics)
}

# Define the metrics to analyze
metrics <- c("mean_duration", "sd_duration", "mean_persistence", "sd_persistence")

# Compute the annual ice metrics
annual_ice_metrics <- compute_annual_ice_metrics(sea_ice_duration_persistence_df, metrics)

# Function to get precomputed ice metrics
get_precomputed_ice_metrics <- function(year, ice_data, metric) {
  ice_value <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_value) > 0) {
    return(mean(ice_value, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Calculate the average ice metrics from the prior year
penguin_data_with_ice <- modeled_gentoo_parameters_df %>%
  rowwise() %>%
  mutate(
    overwinter_mean_duration_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_mean_duration"),
    overwinter_sd_duration_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_sd_duration"),
    overwinter_mean_persistence_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_mean_persistence"),
    overwinter_sd_persistence_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_sd_persistence")
  ) %>%
  ungroup()

# Filter out growth rates above 3
filtered_penguin_data <- penguin_data_with_ice %>%
  filter(growth_rate <= 3)

# Function to fit GLS model for average metric from 1 year with filtered data
fit_gls_model_avg <- function(penguin_data, metric) {
  penguin_data <- penguin_data %>%
    filter(!is.na(!!sym(metric)), !is.na(growth_rate))
  
  print(paste("Fitting GLS model for average lag 1 year for", metric))
  formula <- as.formula(paste("growth_rate ~", metric))
  model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
  
  return(model)
}

# Fit the models for the new metrics
gls_models <- list()
metrics_list <- c("overwinter_mean_duration_1yr", "overwinter_sd_duration_1yr", "overwinter_mean_persistence_1yr", "overwinter_sd_persistence_1yr")

for (metric in metrics_list) {
  gls_models[[metric]] <- fit_gls_model_avg(filtered_penguin_data, metric)
}

# Get confidence intervals for the model coefficients
conf_intervals <- lapply(gls_models, intervals)

# Extract the confidence intervals for the intercept and slope
conf_intervals_list <- lapply(conf_intervals, function(ci) {
  list(
    intercept_ci = ci$coef[1, ],
    slope_ci = ci$coef[2, ]
  )
})

# Plot settings
font <- "Gudea"
font_add_google(family = font, font, db_cache = TRUE)
showtext_auto()

theme_set(theme_minimal(base_family = font, base_size = 12))
bg <- "#F4F5F1"
txt_col <- "black"

# Function to create prediction data and plot
create_plot <- function(penguin_data, gls_model, intercept_ci, slope_ci, metric, title) {
  new_data <- data.frame(overwinter_metric = seq(min(penguin_data[[metric]], na.rm = TRUE),
                                                 max(penguin_data[[metric]], na.rm = TRUE),
                                                 length.out = 100))
  
  colnames(new_data) <- metric  # Rename the column back to its original name
  
  predicted_values <- predict(gls_model, new_data)
  
  new_data$fit <- predicted_values
  new_data$upper <- intercept_ci[3] + slope_ci[3] * new_data[[metric]]
  new_data$lower <- intercept_ci[1] + slope_ci[1] * new_data[[metric]]
  
  plot <- ggplot() +
    geom_point(data = penguin_data, aes_string(x = metric, y = "growth_rate"), color = "black", size = 1) +
    geom_line(data = new_data, aes_string(x = metric, y = "fit"), color = "blue") +
    geom_ribbon(data = new_data, aes_string(x = metric, ymin = "lower", ymax = "upper"), alpha = 0.2) +
    labs(title = title,
         x = metric,
         y = "Growth Rate") +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      plot.caption = element_text(hjust = 0.5, size = 10),
      plot.margin = margin(20, 20, 20, 20)
    ) +
    ylim(NA, 1.5)
  
  return(plot)
}

# Create the plots
plot_mean_duration <- create_plot(filtered_penguin_data, gls_models[["overwinter_mean_duration_1yr"]],
                                  conf_intervals_list[["overwinter_mean_duration_1yr"]]$intercept_ci,
                                  conf_intervals_list[["overwinter_mean_duration_1yr"]]$slope_ci,
                                  "overwinter_mean_duration_1yr", "Growth Rate vs. Mean Duration")

plot_sd_duration <- create_plot(filtered_penguin_data, gls_models[["overwinter_sd_duration_1yr"]],
                                conf_intervals_list[["overwinter_sd_duration_1yr"]]$intercept_ci,
                                conf_intervals_list[["overwinter_sd_duration_1yr"]]$slope_ci,
                                "overwinter_sd_duration_1yr", "Growth Rate vs. SD Duration")

plot_mean_persistence <- create_plot(filtered_penguin_data, gls_models[["overwinter_mean_persistence_1yr"]],
                                     conf_intervals_list[["overwinter_mean_persistence_1yr"]]$intercept_ci,
                                     conf_intervals_list[["overwinter_mean_persistence_1yr"]]$slope_ci,
                                     "overwinter_mean_persistence_1yr", "Growth Rate vs. Mean Persistence")

plot_sd_persistence <- create_plot(filtered_penguin_data, gls_models[["overwinter_sd_persistence_1yr"]],
                                   conf_intervals_list[["overwinter_sd_persistence_1yr"]]$intercept_ci,
                                   conf_intervals_list[["overwinter_sd_persistence_1yr"]]$slope_ci,
                                   "overwinter_sd_persistence_1yr", "Growth Rate vs. SD Persistence")

# Save the plot objects
save(plot_mean_duration, plot_sd_duration, plot_mean_persistence, plot_sd_persistence, file = "penguin_growth_persistence-duration_rate_plots.RData")

# Load the plot objects (if needed in the future)
# load("penguin_growth_rate_plots.RData")

# Combine the plots in a 2x2 grid
final_plot <- (plot_mean_duration + plot_sd_duration) / (plot_mean_persistence + plot_sd_persistence) +
  plot_layout(heights = c(2, 2)) +
  plot_annotation(
    theme = theme(
      plot.background = element_rect(fill = bg, color = NA),
      plot.margin = margin(20, 20, 20, 20)
    )
  )

# Display the final plot
print(final_plot)

# Save the figure
ggsave("growth_rate_effects.png", plot = final_plot, bg = bg, height = 10, width = 12, dpi = 300)



```




**Compile Significant Results**
```{r}

# Load necessary libraries
library(dplyr)
library(readr)

# Define the directory containing the CSV files
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/significant"

# Get a list of all CSV files in the directory
csv_files <- list.files(path = results_dir, pattern = "*.csv", full.names = TRUE)

# Initialize an empty list to store the dataframes
dfs <- list()

# Loop through each CSV file and read the data
for (file in csv_files) {
  # Read the current CSV file using readr
  df <- read_csv(file, col_types = cols(.default = "c"))  # Read all columns as character
  
  # Append the dataframe to the list
  dfs <- append(dfs, list(df))
}

# Combine all dataframes into one, filling missing columns with NA
combined_results_df <- bind_rows(dfs)

# Define the output file path
output_file <- file.path(results_dir, "combined_significant_results.csv")

# Export the combined results to a new CSV file
write_csv(combined_results_df, output_file)

# Print a message indicating the completion of the process
cat("Combined results have been saved to:", output_file, "\n")


```






 str(penguin_abundance_data)
Classes ‘data.table’ and 'data.frame':	7182 obs. of  15 variables:
 $ V1                : int  1 2 3 4 5 6 7 8 9 10 ...
 $ site_id           : chr  "AITC" "AITC" "AITC" "AITC" ...
 $ site_name         : chr  "Barrientos Island (Aitcho Islands)" "Barrientos Island (Aitcho Islands)" "Barrientos Island (Aitcho Islands)" "Barrientos Island (Aitcho Islands)" ...
 $ ccamlr_id         : num  48.1 48.1 48.1 48.1 48.1 48.1 48.1 48.1 48.1 48.1 ...
 $ site              : int  1 1 1 1 1 1 1 1 1 1 ...
 $ initial_season    : int  30 30 30 30 30 30 30 30 30 30 ...
 $ latitude          : num  -62.4 -62.4 -62.4 -62.4 -62.4 ...
 $ longitude         : num  -59.8 -59.8 -59.8 -59.8 -59.8 ...
 $ mean_abundance    : num  126 143 154 166 180 ...
 $ median_abundance  : num  22.2 25.8 29.2 33.4 37.2 ...
 $ lower_95_abundance: num  0.475 0.606 0.701 0.899 1.046 ...
 $ upper_95_abundance: num  891 962 1066 1098 1136 ...
 $ season            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ growth_rate       : num  NA 1.13 1.07 1.08 1.08 ...
 $ year              : int  1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 ...
 - attr(*, ".internal.selfref")=<externalptr> 

 penguin_abundance_data
        V1 site_id                          site_name ccamlr_id site initial_season latitude longitude mean_abundance median_abundance
   1:    1    AITC Barrientos Island (Aitcho Islands)      48.1    1             30 -62.4071  -59.7517       126.3283         22.23919
   2:    2    AITC Barrientos Island (Aitcho Islands)      48.1    1             30 -62.4071  -59.7517       143.1843         25.79221
   3:    3    AITC Barrientos Island (Aitcho Islands)      48.1    1             30 -62.4071  -59.7517       153.8049         29.16895
   4:    4    AITC Barrientos Island (Aitcho Islands)      48.1    1             30 -62.4071  -59.7517       166.2261         33.39426
   5:    5    AITC Barrientos Island (Aitcho Islands)      48.1    1             30 -62.4071  -59.7517       179.9120         37.22348
  ---                                                                                                                                 
7178: 7178    YANK                      Yankee Harbor      48.1  133             17 -62.5257  -59.7687      6777.7819       6222.52283
7179: 7179    YANK                      Yankee Harbor      48.1  133             17 -62.5257  -59.7687      6333.8677       5812.61450
7180: 7180    YANK                      Yankee Harbor      48.1  133             17 -62.5257  -59.7687      4878.7842       4639.47868
7181: 7181    YANK                      Yankee Harbor      48.1  133             17 -62.5257  -59.7687      5812.5561       5810.36291
7182: 7182    YANK                      Yankee Harbor      48.1  133             17 -62.5257  -59.7687      6367.1223       5980.06667
      lower_95_abundance upper_95_abundance season growth_rate year
   1:          0.4746036           891.2210      1          NA 1970
   2:          0.6055929           962.2192      2   1.1334300 1971
   3:          0.7006604          1065.6057      3   1.0741740 1972
   4:          0.8986447          1098.3622      4   1.0807600 1973
   5:          1.0463629          1135.9525      5   1.0823328 1974
  ---                                                              
7178:       2700.6075429         14117.7663     50   1.1135183 2019
7179:       2675.8846639         12970.1751     51   0.9345045 2020
7180:       2477.4954917          8559.3868     52   0.7702693 2021
7181:       5525.2365224          6100.5236     53   1.1913944 2022
7182:       2932.2791716         12073.9408     54   1.0954083 2023
> 


> str(ice_data)
Classes ‘data.table’ and 'data.frame':	53240 obs. of  6 variables:
 $ date          : IDate, format: "1981-06-02" "1981-06-04" "1981-06-06" "1981-06-08" ...
 $ mean_sic      : num  0.358 0.414 0.48 0.442 0.485 ...
 $ sd_sic        : num  0.407 0.385 0.401 0.405 0.41 ...
 $ ice_extent_km2: num  76875 96406 100469 92500 105938 ...
 $ Threshold     : num  0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 0.15 ...
 $ HomeRangeSize : chr  "100km" "100km" "100km" "100km" ...
 - attr(*, ".internal.selfref")=<externalptr> 


> ice_data
             date  mean_sic    sd_sic ice_extent_km2 Threshold HomeRangeSize
    1: 1981-06-02 0.3582459 0.4068131       76875.00      0.15         100km
    2: 1981-06-04 0.4138860 0.3853788       96406.25      0.15         100km
    3: 1981-06-06 0.4800201 0.4010149      100468.75      0.15         100km
    4: 1981-06-08 0.4416560 0.4049839       92500.00      0.15         100km
    5: 1981-06-10 0.4850076 0.4096986      105937.50      0.15         100km
   ---                                                                      
53236: 2023-09-26 0.2215939 0.3025081       26875.00      0.15          50km
53237: 2023-09-27 0.1268865 0.2715207       15000.00      0.15          50km
53238: 2023-09-28 0.2127582 0.3102216       25312.50      0.15          50km
53239: 2023-09-29 0.1705984 0.3001898       16875.00      0.15          50km
53240: 2023-09-30 0.1757676 0.2820827       20625.00      0.15          50km



2. I want to be able to plot the linear models (line + confidence intervals), overlaid on the points used for each model.

3. I want to make sure I can load these back up without having to run the entire script becuase it takes a long time
