---
title: "Gentoo Model Parameter Abundance Model"
author: "Michael J. Wethington"
date: "2024-06-04"
output: html_document
---


zi = latent nest abundance (mean-adjusted)
lz = logged abundance (re-expression of above):  lzi,t=log(zi,t). for the ith site in the tth year,
ri = intrinsic growth rate
lp = predicted population growth rate multiplier
la = actual population growth rate multiplier 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


**Generate Presence Absence Assumptions**
```{r}
library(tidyverse)


required_packages <- c("rlang", "fastmap", "digest", "fs", "cachem", "vctrs", "stringi", "glue", "cli", "utf8", "fansi", "dplyr")

for(pkg in required_packages){
  if(!require(pkg, character.only = TRUE)){
    install.packages(pkg)
  }
}

min_season <- 1970
max_season <- 2023
species <- "GEPE"

# assign the total number of seasons as n_seasons
(n_seasons <- (max_season - min_season) + 1)


SiteList <- mapppdr::penguin_obs %>%
  # keep all sites that have at least 1 count between min and max season
  dplyr::filter(count > 0 & species_id == species & season >= min_season & season <= max_season) %>%
  # create relative season index 
  mutate(season_relative = season - min_season + 1) %>%
  # determine first season a count is observed for each site
  group_by(site_id) %>%
  summarise(initial_season = min(season_relative)) %>%
  ungroup() %>%
  # join to get other site specific covariates for visualization purposes
  left_join(mapppdr::sites, by = "site_id") %>%
  # create site index for model and visualization
  mutate(site = as.numeric(as.factor(site_id))) %>%
  dplyr::select(site_id, site_name, ccamlr_id, site, initial_season, latitude, longitude)

(n_sites <- nrow(SiteList))

# create site x season template which is used throughout analysis
w_template <- SiteList %>%
  dplyr::select(site_id, site) %>%
  # expand each site by the number of seasons
  uncount(n_seasons) %>%
  # create relative season index for each site
  mutate(season_relative = rep(1:n_seasons, n_sites)) %>%
  # create season var from relative season index
  mutate(season = season_relative + min_season - 1) %>%
  arrange(season_relative, site)

w_df <- rbind(
  # keep all presence/absence data and assign observation type as 2 (observed)
  mapppdr::penguin_obs %>%
    dplyr::filter(species_id == species & season >= min_season & season <= max_season) %>%
    dplyr::select(site_id, season, presence) %>%
    mutate(known_w = 1),
  # append presence/absence assumption data which is not part of mapppd
  # and assign observation type of 1 (assumed)
  data.frame(read_csv(file = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/mapppd/gentoo_presence_absence_assumptions.csv")) %>%
    mutate(known_w = 0)) %>%
  # determine for each site x season if breeding is observed or assumed
  group_by(site_id, season) %>%
  summarise(w = base::max(presence), known_w = base::max(known_w)) %>%
  ungroup() %>%
  # join with w_template to fill in missing site x seasons with no presence/absence data
  right_join(w_template, by = c("site_id", "season")) %>%
  # assign observation type as 0 (imputed)
  mutate(known_w = replace(known_w, is.na(known_w), 0)) %>%
  arrange(site_id, season) %>%
  # impute missing presence/absence data using the following assumptions
  # ASSSUMPTION: fill in NA between (1,1) with 1
  # ASSSUMPTION: fill in NA between (0,1) with 0
  # ASSSUMPTION: fill in NA between (1,0) with 1
  # ASSSUMPTION: fill in NA between (.,1) and (1,.) with 1
  # ASSSUMPTION: fill in NA between (.,0) and (0,.) with 0
  dplyr::group_by(site_id) %>%
  tidyr::fill(w, .direction = "downup") %>%
  dplyr::ungroup() %>%
  # create second site_id var for plotting sites alphabetically in ggplot
  mutate(site_id_rev = factor(site_id, levels = rev(sort(unique(site_id))))) %>%
  dplyr::select(site_id, site_id_rev, season, site, season_relative, w, known_w)

# convert w to matrix to be used in model
w <- w_df %>%
  dplyr::select(site, season_relative, w) %>%
  # create matrix where rows are sites and columns are seasons
  pivot_wider(names_from = season_relative, values_from = w, names_sort = TRUE) %>%
  dplyr::select(-site) %>%
  as.matrix()


abundance <- mapppdr::penguin_obs %>%
  # keep all counts between min and max season
  dplyr::filter(count > 0 & species_id == species & season >= min_season & season <= max_season) %>%
  # join to get site index and initial season
  right_join(SiteList, by = "site_id") %>%
  # create relative season index 
  mutate(season_relative = season - min_season + 1) %>%
  # ASSUMPTION: increase accuracy category of all adult counts by + 3 with a max error of 5
  rowwise() %>%
  mutate(accuracy = replace(accuracy, type == "adults", base::min((accuracy[type == "adults"] + 3), 5))) %>%
  ungroup() %>%  
  mutate(type = replace(type, type == "adults", "nests")) %>%
  # ASSUMPTION: keep maximum nest and chick count reported each season for a site
  group_by(site_id, season, season_relative, type) %>%
  arrange(desc(count), accuracy) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  # ASSUMPTION: convert accuracy to the following errors/precisions
  mutate(sigma = case_when(
    accuracy == 1 ~ 0.02490061, 
    accuracy == 2 ~ 0.04955838,
    accuracy == 3 ~ 0.1201131, 
    accuracy == 4 ~ 0.2212992, 
    accuracy == 5 ~ 0.4472728)) %>%
  mutate(precision = case_when(
    accuracy == 1 ~ 1/0.02490061^2, 
    accuracy == 2 ~ 1/0.04955838^2,
    accuracy == 3 ~ 1/0.1201131^2, 
    accuracy == 4 ~ 1/0.2212992^2, 
    accuracy == 5 ~ 1/0.4472728^2)) %>%  
  dplyr::select(site_id, site, season, season_relative, initial_season, type, 
                count, presence, accuracy, sigma, precision) %>%
  arrange(site, season_relative, type, -count, accuracy, sigma, precision)  

abundance_initial <- abundance %>%
  # keep first observed count for each site's time series
  dplyr::filter(initial_season == season_relative) %>%
  # ASSUMPTION: if no nest count is available in the initial season and a chick count is then
  # assume chick count is 1:1 nest count
  group_by(site_id, season, site, season_relative) %>%
  arrange(desc(type)) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  dplyr::select(site_id, season, site, season_relative, count, sigma, precision)

abundance_nests <- abundance %>%
  # keep all nest counts after the initial season
  dplyr::filter(initial_season != season_relative & type == "nests") %>%
  dplyr::select(site_id, season, site, season_relative, count, sigma, precision)

abundance_chicks <- rbind(
  # keep all chick counts after the initial season
  abundance %>%
    dplyr::filter(initial_season != season_relative & type == "chicks") %>%
    dplyr::select(site_id, season, site, season_relative, count, sigma, precision),
  # append chick counts from initial season that were not converted to nest counts
  # meaning there was both a chick and nest count in the initial season
  abundance %>%
    dplyr::filter(initial_season == season_relative) %>%
    group_by(site_id, season, site, season_relative) %>%
    arrange(desc(type)) %>%
    slice(2) %>%
    ungroup() %>%
    dplyr::select(site_id, season, site, season_relative, count, sigma, precision))

# moment match alpha shape and rate parameters for breeding productivity
mu <- .5
sigma <- .25
a <- (mu^2 - mu^3 - mu * sigma^3) / sigma^2
b <- (mu - 2* mu^2 + mu^3 - sigma^2 + mu * sigma^3) / sigma^2

# create the data list for the JAGS model
model_data <- list(
  nests = nrow(abundance_nests),
  y_n = log(abundance_nests$count), 
  precision_n = abundance_nests$precision,
  site_n = abundance_nests$site,
  season_n = abundance_nests$season_relative,
  chicks = nrow(abundance_chicks),
  y_c = log(abundance_chicks$count), 
  precision_c = abundance_chicks$precision,
  site_c = abundance_chicks$site,
  season_c = abundance_chicks$season_relative,
  y_i = log(abundance_initial$count),
  precision_i = abundance_initial$precision,
  n_sites = n_sites,
  n_seasons = n_seasons,
  s = as.vector(SiteList$initial_season),
  w = w,
  a = a,
  b = b)


```

**JAGS Model**
```{r}

# Install and load necessary libraries
# Optional installation lines are commented out. Uncomment to install if needed.
# if (!require("rjags")) install.packages("rjags", dependencies = TRUE)
# if (!require("coda")) install.packages("coda", dependencies = TRUE)
library(rjags)   # JAGS interface for Bayesian modeling
library(coda)    # Tools for MCMC output analysis
library(dplyr)
library(tidyr)
library(readr)
library(parallel) # Enables parallel processing

# Define the file path for the JAGS model script
model_file_path <- "D:/Manuscripts_localData/FrostBound_AQ/Results/gentoo-abundance-model/jags_model.jags"

# Write the JAGS model code to a text file
sink(model_file_path)
cat("
model {

# Define prior for process variance (sigma) and precision (tau)
sigma ~ dunif(0, 1)         # Uniform prior for sigma, allowing variation between 0 and 1
tau <- pow(sigma, -2)       # Precision is inverse squared sigma

# Breeding success prior for chick abundance
for (i in 1:chicks) {
  alpha[i] ~ dbeta(a, b)    # Beta distribution for breeding success, based on input parameters a and b
}

# Initial abundance for each site
for (i in 1:n_sites) {
  lz[i, s[i]] ~ dnorm(0, .001)  # Normally distributed initial abundance with high variance
}

# Define a prior for the intercept (beta) to allow variability in population growth
beta ~ dunif(-.5, .5)

# Site-specific effects (eta) for each site
for (i in 1:n_sites) {
  eta[i] ~ dnorm(0, tau_site)
}
sigma_site ~ dunif(0, 1)       # Uniform prior for site-level variation
tau_site <- pow(sigma_site, -2)

# Seasonal effects (epsilon) for each season
for (t in 1:n_seasons) {
  epsilon[t] ~ dnorm(0, tau_season)
}
sigma_season ~ dunif(0, 1)     # Uniform prior for season-level variation
tau_season <- pow(sigma_season, -2)

# Observation model for nest data
for (i in 1:nests) {
  y_n[i] ~ dnorm(mu_y_n[i], precision_n[i])   # Observed nest counts with specified precision
  mu_y_n[i] <- lz[site_n[i], season_n[i]] - 1/(2 * precision_n[i])  # Mean adjusted for precision
  y_n_new[i] ~ dnorm(mu_y_n[i], precision_n[i])   # Simulated nest counts for predictive checks
  y_n_sq[i] <- pow((y_n[i] - mu_y_n[i]), 2)       # Squared residuals for observed nest counts
  y_n_sq_new[i] <- pow((y_n_new[i] - mu_y_n[i]), 2)  # Squared residuals for simulated nest counts
}

# Observation model for chick data
for (i in 1:chicks) {
  N[i] <- 2 * round(exp(lz[site_c[i], season_c[i]]))   # Expected population size for chicks
  z_c[i] ~ dbin(alpha[i], N[i])                        # Binomially distributed chick abundance
  lz_c[i] <- log(z_c[i])                               # Log-transformed abundance
  y_c[i] ~ dnorm(mu_y_c[i], precision_c[i])            # Observed chick counts with specified precision
  mu_y_c[i] <- lz_c[i] - 1/(2 * precision_c[i])        # Mean for chick data, adjusted for precision
  y_c_new[i] ~ dnorm(mu_y_c[i], precision_c[i])        # Simulated chick counts for predictive checks
  y_c_sq[i] <- pow((y_c[i] - mu_y_c[i]), 2)            # Squared residuals for observed chick counts
  y_c_sq_new[i] <- pow((y_c_new[i] - mu_y_c[i]), 2)    # Squared residuals for simulated chick counts
}

# Process model for population growth
for (i in 1:n_sites) {
  for (t in 1:n_seasons) {
    zr[i, t] <- beta + eta[i] + epsilon[t]   # Growth rate influenced by site and seasonal effects
    lza[i, t] <- lz[i, t] * w[i, t]          # Adjusted abundance accounting for site-season presence
  }
}  

# Initial year abundance data model
for (i in 1:n_sites) {
  y_i[i] ~ dnorm(mu_y_i[i], precision_i[i])   # Observed abundance with precision for initial season
  mu_y_i[i] <- lz[i, s[i]] - 1/(2 * precision_i[i])  # Mean adjusted for precision
  y_i_new[i] ~ dnorm(mu_y_i[i], precision_i[i])      # Simulated initial counts for predictive checks
  y_i_sq[i] <- pow((y_i[i] - mu_y_i[i]), 2)          # Squared residuals for observed initial counts
  y_i_sq_new[i] <- pow((y_i_new[i] - mu_y_i[i]), 2)  # Squared residuals for simulated initial counts
}

# Population abundance dynamics for subsequent seasons
for (i in 1:n_sites) {
  for (t in (s[i] + 1):n_seasons) {
    lz[i, t] ~ dnorm(mu_lz[i, t], tau)              # Abundance in each season based on prior year
    mu_lz[i, t] <- lz[i, t - 1] + zr[i, t] - 1/(2 * tau)  # Process model with adjusted precision
  }
}

# Population abundance dynamics for pre-initial seasons
for (i in 1:n_sites) {
  for (t in 1:(s[i] - 1)) {
    lz[i, s[i] - t] ~ dnorm(mu_lz[i, s[i] - t], tau)       # Prior abundance modeled on next season
    mu_lz[i, s[i] - t] <- lz[i, s[i] - t + 1] - zr[i, s[i] - t + 1] - 1/(2 * tau)
  }
}
 
# Posterior predictive checks
y_n_sqs <- sum(y_n_sq[])      # Sum of squared residuals for nests (observed)
y_n_sqs_new <- sum(y_n_sq_new[])  # Sum of squared residuals for nests (simulated)
y_i_sqs <- sum(y_i_sq[])      # Sum of squared residuals for initial season (observed)
y_i_sqs_new <- sum(y_i_sq_new[])  # Sum of squared residuals for initial season (simulated)
y_c_sqs <- sum(y_c_sq[])      # Sum of squared residuals for chicks (observed)
y_c_sqs_new <- sum(y_c_sq_new[])  # Sum of squared residuals for chicks (simulated)

# Derived quantities for growth rate calculation
for (i in 1:n_sites) {
  for (t in 2:n_seasons) {
    l_a[i, t - 1] <- exp(lz[i, t] - lz[i, t - 1])   # Annual growth based on log-abundance change
    l_p[i, t - 1] <- exp(zr[i, t])                  # Growth rate (zr) transformation
    lw_a[i, t - 1] <- ifelse(sum(w[i, (t-1):t]) == 2, l_a[i, t - 1], 1)   # Weighted growth rate
    lw_p[i, t - 1] <- ifelse(sum(w[i, (t-1):t]) == 2, l_p[i, t - 1], 1)   # Weighted persistence rate
  }
}

# Geometric mean calculations for site-specific growth and persistence
for (i in 1:n_sites) {
  x[i, 1:n_seasons] <- ifelse(sum(w[i, 1:n_seasons]) > 1, w[i, 1:n_seasons], rep(1, n_seasons)) 
  gl_a[i] <- ifelse(sum(w[i, 1:n_seasons]) > 1, pow(prod(lw_a[i, ]), (1/(sum(x[i, 1:n_seasons]) - 1))), 0)
  gl_p[i] <- ifelse(sum(w[i, 1:n_seasons]) > 1, pow(prod(lw_p[i, ]), (1/(sum(x[i, 1:n_seasons]) - 1))), 0)
}

}", fill = TRUE)
sink() # Complete writing of the model to file

# Prepare the input data list for JAGS model
model_data <- list(
  nests = nrow(abundance_nests),    # Total number of nest observations
  y_n = log(abundance_nests$count), # Log-transformed nest counts
  precision_n = abundance_nests$precision, # Precision for each nest observation
  site_n = abundance_nests$site,    # Site indices for nest observations
  season_n = abundance_nests$season_relative, # Season indices for nest observations
  chicks = nrow(abundance_chicks),  # Total number of chick observations
  y_c = log(abundance_chicks$count), # Log-transformed chick counts
  precision_c = abundance_chicks$precision, # Precision for each chick observation
  site_c = abundance_chicks$site,    # Site indices for chick observations
  season_c = abundance_chicks$season_relative, # Season indices for chick observations
  y_i = log(abundance_initial$count), # Log-transformed initial season counts
  precision_i = abundance_initial$precision, # Precision for initial counts
  n_sites = n_sites,               # Number of unique sites
  n_seasons = n_seasons,           # Total number of seasons
  s = as.vector(SiteList$initial_season), # Initial seasons for each site
  w = w,                           # Presence-absence matrix for each site-season
  a = a,                           # Shape parameter for breeding productivity
  b = b                            # Rate parameter for breeding productivity
)



random_inits <- function(model_data) {
  # Generate a random seed for reproducibility
  seed = runif(1, 1, 100000)
  
  # Initialize model parameters with random values within specified ranges
  beta <- runif(1, -.025, .025)        # Random intercept for growth rate
  sigma_site <- runif(1, .025, .05)    # Standard deviation for site-level effects
  sigma_season <- runif(1, .05, .1)    # Standard deviation for season-level effects
  sigma <- runif(1, .05, .1)           # Standard deviation for process variance

  # Extract key values from model_data for use in initializations
  chicks <- model_data$chicks          # Total number of chick observations
  n_sites <- model_data$n_sites        # Number of unique sites
  n_seasons <- model_data$n_seasons    # Number of seasons in the study period
  s <- model_data$s                    # Initial season indices for each site
  y_c <- model_data$y_c                # Log-transformed chick counts
  y_i <- model_data$y_i                # Log-transformed initial season counts
  site_c <- model_data$site_c          # Site indices for chick observations
  season_c <- model_data$season_c      # Season indices for chick observations
  a <- model_data$a                    # Beta distribution shape parameter for chick survival
  b <- model_data$b                    # Beta distribution rate parameter for chick survival

  # Initialize random effects for sites and seasons
  eta <- rnorm(n_sites, 0, sigma_site)       # Site-specific random effects
  epsilon <- rnorm(n_seasons, 0, sigma_season) # Season-specific random effects
  
  # Initialize alpha for chick survival using the beta distribution
  alpha <- rbeta(chicks, a, b)               # Chick survival rates for each observation

  # Initialize abundance and growth rate matrices
  lz <- zr <- array(NA, dim = c(n_sites, n_seasons))  # Matrices for abundance (lz) and growth rate (zr)

  # Loop through each site to initialize abundance (lz) and growth rate (zr) across seasons
  for (i in 1:n_sites) {
    # Set initial abundance (lz) for the starting season based on mean initial chick count
    lz[i, s[i]] <- mean(y_i[i], na.rm = TRUE)
    
    # Forward initialization: calculate abundance for each season after the initial season
    for (t in (s[i] + 1):n_seasons) {
      zr[i, t] <- beta + eta[i] + epsilon[t]                # Growth rate as a function of intercept and random effects
      lz[i, t] <- rnorm(1, lz[i, (t - 1)] + zr[i, t] - sigma^2 / 2, sigma) # Abundance based on previous season and growth rate
    }
    
    # Backward initialization: calculate abundance for seasons before the initial season
    for (t in 1:(s[i] - 1)) {
      zr[i, (s[i] - t + 1)] <- beta + eta[i] + epsilon[(s[i] - t + 1)]  # Growth rate using random effects
      lz[i, (s[i] - t)] <- rnorm(1, lz[i, (s[i] - t + 1)] - zr[i, (s[i] - t + 1)] - sigma^2 / 2, sigma)
      # Calculate abundance by extrapolating backward from the initial season
    }
  }
  
  # Initialize chick abundance based on calculated abundance and survival rates
  z_c <- N <- NA  # Empty vectors for chick abundance and population size

  # Loop through each chick observation to calculate chick abundance (z_c)
  for (i in 1:chicks) {
    # Ensure abundance is non-negative for each chick observation's site and season
    if (lz[site_c[i], season_c[i]] < 0) lz[site_c[i], season_c[i]] <- 0
    
    # Calculate expected population size as twice the rounded abundance
    N[i] <- 2 * round(exp(lz[site_c[i], season_c[i]]))
    
    # Calculate chick abundance using a binomial distribution, with a minimum of 1 to avoid zeroes
    z_c[i] <- base::max(rbinom(1, prob = alpha[i], N[i]), 1)
  }
  
  # Return a list of initialized values for the JAGS model
  return(list(
    sigma = sigma,                # Process variance
    sigma_site = sigma_site,      # Site-level variance
    sigma_season = sigma_season,  # Season-level variance
    beta = beta,                  # Growth rate intercept
    eta = eta,                    # Site-specific random effects
    epsilon = epsilon,            # Season-specific random effects
    alpha = alpha,                # Chick survival rates
    lz = lz,                      # Site-season abundance matrix
    z_c = z_c,                    # Chick abundance values
    .RNG.name = "base::Mersenne-Twister", # RNG for reproducibility in JAGS
    .RNG.seed = seed              # Random seed for reproducibility
  ))
}

save(random_inits, file = "D:/Manuscripts_localData/FrostBound_AQ/Results/gentoo-abundance-model/random_inits.rda")
expect_error(random_inits(model_data), NA)


n.chains <- 6
n.adapt <- 3000
n.update <- 300000
n.iter <- 200000
thin <- 200
cl <- makeCluster(n.chains)

cvars <- c("model_data", "n.adapt", "n.update", "n.iter", "thin", "params", "random_inits")
params <- c("beta", "sigma", "sigma_site", "sigma_season", "alpha", "epsilon", "eta", "z_c", "lz", 
            "gl_a", "l_a", "y_i_new", "y_n_new", "y_c_new", "y_n_sqs", "y_n_sqs_new", "y_i_sqs_new",
            "y_i_sqs", "y_c_sqs", "y_c_sqs_new", "lz_c", "lza")

parallel::clusterExport(cl, cvars)

out <- clusterEvalQ(cl, {
  library(rjags)
  inits <- random_inits(model_data)
  jm = jags.model("D:/Manuscripts_localData/FrostBound_AQ/Results/gentoo-abundance-model/jags_model.jags", data = model_data, n.chains = 1, n.adapt = n.adapt, 
                  inits = inits)
  update(jm, n.iter = n.update)
  zm = coda.samples(jm, variable.names = params, n.iter = n.iter, thin = thin)
  return(as.mcmc(zm))
})
stopCluster(cl)
model_data_rinits_output = mcmc.list(out)  
save(model_data_rinits_output, file = "D:/Manuscripts_localData/FrostBound_AQ/Results/gentoo-abundance-model/model_data_rinits_output.rda")


model_data_rinits_output <- read

MCMCsummary(model_data_rinits_output, params = c("beta", "sigma", "sigma_site", "sigma_season"), 
            HPD = TRUE, hpd_prob = .95, round = 3)

#Posterior Predictive Checks
params <- c("y_i_sqs", "y_i_sqs_new", "y_n_sqs", "y_n_sqs_new", "y_c_sqs", "y_c_sqs_new")
MCMCsummary(model_data_rinits_output, params = params, n.eff = FALSE, round = 3)
```




**Generate Gentoo Growth Parameters**

```{r}

# Load required libraries
library(tidyverse)
library(coda)
library(mapppdr)
library(patchwork)
library(leaflet)
library(CCAMLRGIS)
library(rjags)
library(MCMCvis)
library(parallel)
library(stringi)
library(pander)
library(testthat)

# Define parameters
min_season <- 1970
max_season <- 2023
species <- "GEPE"

# Construct Presence-Absence Assumptions CSV for the JAGS model
penguin_obs <- mapppdr::penguin_obs

penguin_obs_processed <- penguin_obs %>%
  filter(species_id == species) %>%
  mutate(
    presence = ifelse(!is.na(count), 1, 0),
    known_w = 1) %>%
  select(site_id, season, presence, known_w, count, accuracy, type)

presence_absence_assumptions <- expand.grid(
  site_id = unique(penguin_obs$site_id),
  season = min_season:max_season
) %>%
  left_join(penguin_obs_processed, by = c("site_id", "season")) %>%
  mutate(
    presence = ifelse(is.na(presence), 0, presence),
    known_w = ifelse(is.na(known_w), 0, known_w),
    count = ifelse(is.na(count), 0, count)) %>%
  group_by(site_id, season) %>%
  arrange(desc(type), desc(accuracy), .by_group = TRUE) %>%
  slice(1) %>%
  ungroup() %>%
  select(site_id, season, presence, known_w, count, accuracy)

# Load the JAGS MCMC Output File
load("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/model_data_rinits_output.rda")

# Extract the Gentoo Abundance Estimates (lz)
model_samples <- as.matrix(model_data_rinits_output)

# Filter columns that are logged latent abundance (lz) parameters
lz_columns <- grep("^lz\\[", colnames(model_samples))
lz_samples <- model_samples[, lz_columns]

# Convert log-abundances to actual abundances
abundance_samples <- exp(lz_samples)

# Summarize the actual abundances
abundance_summary <- apply(abundance_samples, 2, function(x) {
  c(mean = mean(x), median = median(x), 
    lower_95 = quantile(x, 0.025), upper_95 = quantile(x, 0.975))
})

# Convert to a readable data frame (use t to transpose)
abundance_summary_df <- as.data.frame(t(abundance_summary))

# Extract site and season info from the indices
extract_indices <- function(colname) {
  indices <- gsub("[^0-9,]", "", colname)
  as.integer(unlist(strsplit(indices, ",")))
}

indices <- lapply(colnames(abundance_samples), extract_indices)
sites <- sapply(indices, `[`, 1)
seasons <- sapply(indices, `[`, 2)

abundance_summary_df$site <- sites
abundance_summary_df$season <- seasons

abundance_summary_df <- abundance_summary_df %>% 
  rename(mean_abundance = mean,
         median_abundance = median,
         lower_95_abundance = "lower_95.2.5%",
         upper_95_abundance = "upper_95.97.5%")

head(abundance_summary_df)

# Load and prepare SiteList
SiteList <- mapppdr::penguin_obs %>%
  filter(count > 0 & species_id == species & season >= min_season & season <= max_season) %>%
  mutate(season_relative = season - min_season + 1) %>%
  group_by(site_id) %>%
  summarise(initial_season = min(season_relative)) %>%
  ungroup() %>%
  left_join(mapppdr::sites, by = "site_id") %>%
  mutate(site = as.numeric(as.factor(site_id))) %>%
  select(site_id, site_name, ccamlr_id, site, initial_season, latitude, longitude)

(n_sites <- nrow(SiteList))

SiteList %>% 
  distinct(site, site_id, latitude, longitude)

# Join SiteList with abundance_summary_df
final_data <- left_join(SiteList, abundance_summary_df, by = "site")

# Ensure final_data is sorted by site and season
final_data <- final_data %>%
  arrange(site, season)

# Calculate the growth rate and append it to the data
final_data <- final_data %>%
  group_by(site) %>%
  mutate(growth_rate = mean_abundance / lag(mean_abundance)) %>%
  ungroup()

# Adjust the season column in the final data
final_data <- final_data %>%
  mutate(year = 1970 + season - 1)

# Display the adjusted final_data
print(head(final_data))

write.csv(final_data, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/modeled_gentoo_parameters.csv")
# Print the head of the final data with growth rates
print(head(final_data))

```


**calculate and plot geometric mean of growth rates** 
```{r}
library(ggplot2)
library(dplyr)

# Calculate geometric mean of growth rates
final_data <- final_data %>%
  group_by(site) %>%
  mutate(geometric_mean_growth_rate = exp(mean(log(growth_rate), na.rm = TRUE))) %>%
  ungroup()

# Filter data for the specific site
site_data <- final_data %>% filter(site_id == "AITC")

# Calculate the mean growth multiplier for the site
mean_growth <- site_data %>% 
  summarize(mean_growth_multiplier = exp(mean(log(growth_rate), na.rm = TRUE)),
            lower_ci = exp(mean(log(growth_rate), na.rm = TRUE) - 1.96 * sd(log(growth_rate), na.rm = TRUE)/sqrt(n())),
            upper_ci = exp(mean(log(growth_rate), na.rm = TRUE) + 1.96 * sd(log(growth_rate), na.rm = TRUE)/sqrt(n())))

# Check if the 'count' and 'type' columns exist and correct them if needed
if (!"count" %in% names(site_data)) {
  site_data$count <- site_data$mean_abundance # or any other logic that fits
}

if (!"type" %in% names(site_data)) {
  site_data$type <- "nests" # default type, adjust as necessary
}

# Add year column based on season
site_data <- site_data %>%
  mutate(year = 1970 + season - 1)

# Generate the plot
ggplot(site_data, aes(x = year, y = mean_abundance)) +
  geom_boxplot(aes(group = year), fill = "orange", alpha = 0.6) +
  geom_errorbar(aes(ymin = lower_95_abundance, ymax = upper_95_abundance), width = 0.2) +
  geom_point(data = site_data %>% filter(!is.na(count)), aes(y = count, color = type), size = 3, shape = 21, fill = "blue") +
  scale_color_manual(values = c("nests" = "red", "chicks" = "blue")) +
  labs(title = "AITC, Barrientos Island (Aitcho Islands), 48.1",
       subtitle = paste0("mean population growth multiplier = ", round(mean_growth$mean_growth_multiplier, 3), 
                         " (", round(mean_growth$lower_ci, 2), " - ", round(mean_growth$upper_ci, 2), ")"),
       x = "Year",
       y = "Abundance",
       color = "Type") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))


```


**Calculate Sea Ice concentration and pass to dataframe**

```{r}



# This script analyzes daily sea ice concentration (SIC) and extent within defined home ranges by calculating the daily mean SIC, standard deviation of SIC, and total ice-covered area (sea ice extent) above a threshold for each day. It loads NSIDC sea ice data, applies a threshold to set low values to zero, and computes the specified metrics for each home range. The sea ice extent is calculated as the total area of cells with SIC above the threshold, effectively representing the "total ice-covered area" within the home range. The results are compiled and exported to a CSV file for further analysis.

# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(stringr)
library(lubridate)

# Function to compute daily mean SIC, SD SIC, and sea ice extent above a threshold
compute_daily_sic_statistics <- function(buffer_path, nsidc, threshold = 0.15, cell_area_sq_meters) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  
  # Extract dates before masking
  dates <- time(nsidc)
  
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  
  # Set all values < threshold to 0 for the entire raster stack
  buffer_mask <- app(buffer_mask, fun = function(x) { x[x < threshold] <- 0; return(x) })
  
  # Calculate mean and SD SIC excluding NAs for each layer
  mean_sic <- global(buffer_mask, fun = 'mean', na.rm = TRUE)[, 1]
  sd_sic <- global(buffer_mask, fun = 'sd', na.rm = TRUE)[, 1]
  
  # Calculate sea ice extent (total cells above threshold in square kilometers)
  valid_ice_cells <- global(buffer_mask >= threshold, fun = 'sum', na.rm = TRUE)[, 1]
  total_ice_area_sq_km <- (valid_ice_cells * cell_area_sq_meters) / 1e6  # Convert total ice area to square kilometers
  
  results <- data.frame(
    date = as.Date(dates, origin = "1970-01-01"),
    mean_sic = mean_sic,
    sd_sic = sd_sic,
    ice_extent_km2 = total_ice_area_sq_km
  )
  
  return(results)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds, nsidc_subset, results_dir) {
  # Initialize dataframe to store all results
  all_results_df <- data.frame()
  
  # Calculate the cell area in square meters (only once)
  cell_area_sq_meters <- prod(res(nsidc_subset))
  
  # Loop through each home range shapefile and compute results
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      metrics <- compute_daily_sic_statistics(buffer_path, nsidc_subset, threshold = threshold, cell_area_sq_meters = cell_area_sq_meters)
      
      metrics <- metrics %>%
        mutate(Threshold = threshold, HomeRangeSize = home_range_size)
      
      all_results_df <- bind_rows(all_results_df, metrics)
    }
  }
  
  # Export the compiled results to CSV
  write.csv(all_results_df, file.path(results_dir, "daily_sic_statistics.csv"), row.names = FALSE)
  
  return(all_results_df)
}

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1981 to 2023
start_date <- as.Date("1981-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Subset the NSIDC data for testing (e.g., first 100 layers)
# nsidc_subset <- subset(nsidc, 1:100)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
dir.create(results_dir, showWarnings = FALSE)

# Run the analysis on the subset
all_results_df <- analyze_sea_ice_effect(c(0.15), nsidc, results_dir)

# Display the results
head(all_results_df)




```

**Calculate Persistence and Duration metrics and pass to dataframe**
```{r}

# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(stringr)

# Function to compute mean duration, standard deviation, mean persistence, and persistence standard deviation of sea ice concentration above a threshold
compute_duration_persistence_stats <- function(buffer_path, nsidc, threshold = .15, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  
  # Extract dates before masking
  dates <- time(nsidc)
  
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  
  all_years <- unique(year(dates))
  
  duration_persistence_stats <- data.frame(
    year = integer(),
    month = character(),
    mean_duration = numeric(),
    sd_duration = numeric(),
    mean_persistence = numeric(),
    sd_persistence = numeric()
  )
  
  calculate_metrics <- function(data, threshold) {
    durations <- app(data, function(x) {
      rle_result <- rle(x > threshold)
      durations <- rle_result$lengths[rle_result$values]
      return(mean(durations, na.rm = TRUE))
    })
    mean_duration <- mean(values(durations), na.rm = TRUE)
    sd_duration <- sd(values(durations), na.rm = TRUE)
    
    open_water_prop <- app(data, function(x) mean(x < threshold, na.rm = TRUE))
    mean_persistence <- mean(values(open_water_prop), na.rm = TRUE)
    sd_persistence <- sd(values(open_water_prop), na.rm = TRUE)
    
    return(list(mean_duration = mean_duration, sd_duration = sd_duration, mean_persistence = mean_persistence, sd_persistence = sd_persistence))
  }
  
  for (year in all_years) {
    for (month in winter_months) {
      monthly_indices <- which(year(dates) == year & month(dates) == month)
      if (length(monthly_indices) > 0) {
        monthly_data <- buffer_mask[[monthly_indices]]
        metrics <- calculate_metrics(monthly_data, threshold)
        
        duration_persistence_stats <- rbind(duration_persistence_stats, data.frame(
          year = year,
          month = month,
          mean_duration = metrics$mean_duration,
          sd_duration = metrics$sd_duration,
          mean_persistence = metrics$mean_persistence,
          sd_persistence = metrics$sd_persistence
        ))
      }
    }
    
    season_indices <- which(year(dates) == year & month(dates) %in% winter_months)
    if (length(season_indices) > 0) {
      season_data <- buffer_mask[[season_indices]]
      metrics <- calculate_metrics(season_data, threshold)
      
      duration_persistence_stats <- rbind(duration_persistence_stats, data.frame(
        year = year,
        month = "Season-wide",
        mean_duration = metrics$mean_duration,
        sd_duration = metrics$sd_duration,
        mean_persistence = metrics$mean_persistence,
        sd_persistence = metrics$sd_persistence
      ))
    }
  }
  
  return(duration_persistence_stats)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds, nsidc_subset, results_dir) {
  # Initialize dataframe to store all results
  all_duration_persistence_stats <- data.frame()
  
  # Define home range directory
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  # Loop through each home range shapefile and compute results
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      duration_persistence_stats <- compute_duration_persistence_stats(buffer_path, nsidc_subset, threshold = threshold)
      duration_persistence_stats$Threshold <- threshold
      duration_persistence_stats$HomeRangeSize <- home_range_size
      all_duration_persistence_stats <- bind_rows(all_duration_persistence_stats, duration_persistence_stats)
    }
  }
  
  # Export the compiled results to CSV
  write.csv(all_duration_persistence_stats, file.path(results_dir, "sea_ice_duration_persistence_stats_subset.csv"), row.names = FALSE)
  
  # Export the compiled results to RDS
  saveRDS(all_duration_persistence_stats, file.path(results_dir, "sea_ice_duration_persistence_stats.rds"))
}

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1981 to 2023
start_date <- as.Date("1981-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Subset the NSIDC data for testing (e.g., first 100 layers)
# nsidc_subset <- subset(nsidc, 1:100)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
dir.create(results_dir, showWarnings = FALSE)

# Run the analysis on the subset
analyze_sea_ice_effect(c(.15, .30, .50), nsidc, results_dir)

# Display the results
all_duration_persistence_stats <- read.csv(file.path(results_dir, "sea_ice_duration_persistence_stats_subset.csv"))
head(all_duration_persistence_stats)



```


**Run GLS Models on lagged year for Sea ice concentration**

```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(lubridate)
library(data.table)

# Function to calculate monthly average standard deviation for Extent
calculate_monthly_sd <- function(ice_data) {
  ice_data <- ice_data %>%
    mutate(date = as.Date(date, format = "%Y-%m-%d"),
           year_month = format(date, "%Y-%m")) %>%
    group_by(year_month, Threshold, HomeRangeSize) %>%
    summarize(Extent_SD = sd(ice_extent_km2, na.rm = TRUE), .groups = 'drop') %>%
    mutate(date = as.Date(paste0(year_month, "-01"))) %>%
    select(-year_month)
  return(ice_data)
}

# Function to merge monthly Extent_SD with daily data
merge_monthly_sd <- function(daily_data, monthly_sd_data) {
  daily_data <- daily_data %>%
    mutate(date = as.Date(date, format = "%Y-%m-%d"),
           year_month = format(date, "%Y-%m")) %>%
    left_join(monthly_sd_data, by = c("date", "Threshold", "HomeRangeSize")) %>%
    select(-year_month)
  return(daily_data)
}

# General function to compute annual ice metrics
compute_annual_ice_metrics <- function(ice_data, metrics) {
  ice_data <- ice_data %>%
    mutate(date = as.Date(date, format = "%Y-%m-%d"),
           year = year(date))
  
  annual_ice_metrics <- ice_data %>%
    group_by(year, Threshold, HomeRangeSize) %>%
    summarize(across(all_of(metrics), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}"),
              across(all_of(metrics), ~sd(.x, na.rm = TRUE), .names = "sd_{.col}"),
              .groups = 'drop')
  
  return(annual_ice_metrics)
}

# Function to get individual precomputed ice metrics
get_individual_ice_metrics <- function(year, ice_data, metric) {
  ice_value <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_value) > 0) {
    return(ice_value)
  } else {
    return(NA)
  }
}

# Function to fit GLS models for individual lag years
fit_gls_models_indiv <- function(penguin_data, ice_data, metric, threshold, home_range_size) {
  results <- list()
  
  for (lag in 1:5) {
    penguin_data <- penguin_data %>%
      rowwise() %>%
      mutate(overwinter_ice = get_individual_ice_metrics(year - lag, ice_data, metric)) %>%
      ungroup() %>%
      filter(!is.na(overwinter_ice) & !is.na(growth_rate) & growth_rate <= 3)
    
    if (nrow(penguin_data) < 3) next
    
    print(paste("Fitting GLS model for individual lag", lag, "year(s)"))
    formula <- as.formula("growth_rate ~ overwinter_ice")
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
    summary_model <- summary(model)
    bonferroni_p_value <- p.adjust(summary_model$tTable[2, 4], method = "bonferroni", n = 5)
    
    if (bonferroni_p_value < 0.05) {
      results[[paste("Lag_Indiv", lag)]] <- list(
        AIC = AIC(model),
        BIC = BIC(model),
        coefficients = summary_model$tTable,
        p_value = summary_model$tTable[2, 4],
        bonferroni_p_value = bonferroni_p_value,
        model = model,
        penguin_data = penguin_data
      )
    }
  }
  
  return(results)
}

# Main function to run GLS analysis for each condition and save results
run_gls_analysis <- function(penguin_abundance_data, ice_data, metrics, results_dir) {
  # Initialize results list
  all_gls_results_indiv <- list()
  all_results_df_indiv <- list()
  significant_results_df_indiv <- list()
  
  # Compute monthly average standard deviation
  monthly_sd_data <- calculate_monthly_sd(ice_data)
  
  # Merge monthly SD data with daily data
  ice_data <- merge_monthly_sd(ice_data, monthly_sd_data)
  
  # Compute annual ice metrics
  annual_ice_metrics <- compute_annual_ice_metrics(ice_data, metrics)
  
  # List unique thresholds and home range sizes
  thresholds <- unique(annual_ice_metrics$Threshold)
  home_range_sizes <- unique(annual_ice_metrics$HomeRangeSize)
  
  for (metric in metrics) {
    for (threshold in thresholds) {
      for (home_range_size in home_range_sizes) {
        ice_subset <- annual_ice_metrics %>%
          filter(Threshold == threshold, HomeRangeSize == home_range_size)
        
        print(paste("Running GLS analysis for metric:", metric, "threshold:", threshold, "home range size:", home_range_size))
        
        # Perform GLS analysis for individual lag years
        gls_results_indiv <- fit_gls_models_indiv(penguin_abundance_data, ice_subset, paste("mean_", metric, sep = ""), threshold, home_range_size)
        
        # Store results for individual lag years
        all_gls_results_indiv[[paste("Annual_Indiv", metric, threshold, home_range_size, sep = "_")]] <- gls_results_indiv
        
        results_list_indiv <- lapply(names(gls_results_indiv), function(lag) {
          result <- gls_results_indiv[[lag]]
          data.frame(
            Metric = metric,
            HomeRangeSize = home_range_size,
            Threshold = threshold,
            Lag = lag,
            AIC = result$AIC,
            BIC = result$BIC,
            Coefficient_Intercept = result$coefficients[1, 1],
            StdError_Intercept = result$coefficients[1, 2],
            tValue_Intercept = result$coefficients[1, 3],
            pValue_Intercept = result$coefficients[1, 4],
            Coefficient = result$coefficients[2, 1],
            StdError = result$coefficients[2, 2],
            tValue = result$coefficients[2, 3],
            pValue = result$coefficients[2, 4],
            Bonferroni_Adjusted_pValue = result$bonferroni_p_value
          )
        })
        
        all_results_df_indiv <- append(all_results_df_indiv, results_list_indiv)
        significant_results_df_indiv <- append(significant_results_df_indiv, Filter(function(x) x$Bonferroni_Adjusted_pValue < 0.05, results_list_indiv))
      }
    }
  }
  
  # Export the compiled results to CSV
  all_results_df_indiv <- do.call(rbind, all_results_df_indiv)
  significant_results_df_indiv <- do.call(rbind, significant_results_df_indiv)
  
  write.csv(all_results_df_indiv, file.path(results_dir, "model_results_indiv_ice_metrics_extent_extentsd.csv"), row.names = FALSE)
  write.csv(significant_results_df_indiv, file.path(results_dir, "significant_model_results_indiv_ice_metrics_extent_extentsd.csv"), row.names = FALSE)
  
  # Save only significant results to RDS
  significant_gls_results_indiv <- list()
  for (metric in names(all_gls_results_indiv)) {
    significant_gls_results_indiv[[metric]] <- Filter(Negate(is.null), all_gls_results_indiv[[metric]])
  }
  saveRDS(significant_gls_results_indiv, file.path(results_dir, "gls_analysis_results_indiv_concentration_extent_extentsd.rds"))
}

### Load Data and Run Analysis

# Load Gentoo penguin data
penguin_abundance_data <- fread("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

str(penguin_abundance_data)

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Load the SIC statistics
metric_calculation_csv <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
ice_data <- fread(file.path(metric_calculation_csv, "daily_sic_statistics.csv"))
str(ice_data)

# Define the metrics to analyze
metrics <- c("mean_sic", "sd_sic", "ice_extent_km2", "Extent_SD")

# Define the results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"
dir.create(results_dir, showWarnings = FALSE)

# Run the GLS analysis on the full dataset
results_output <- run_gls_analysis(penguin_abundance_data, ice_data, metrics, results_dir)

```

**Run GLS Models on lagged year for Persistence and Duration**
```{r}
# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(nlme)
library(lubridate)
library(data.table)

# Function to get individual precomputed ice metrics
get_individual_ice_metrics <- function(year, ice_data, metric) {
  ice_values <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_values) > 0) {
    return(mean(ice_values, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Function: Fit GLS models for individual lag years
fit_gls_models_indiv <- function(penguin_data, ice_data, metric) {
  results <- list()
  
  for (lag in 1:5) {
    penguin_data <- penguin_data %>%
      rowwise() %>%
      mutate(overwinter_ice = get_individual_ice_metrics(year - lag, ice_data, metric)) %>%
      ungroup() %>%
      filter(!is.na(overwinter_ice))
    
    if (nrow(penguin_data) < 3) next
    
    print(paste("Fitting GLS model for individual lag", lag, "year(s)"))
    formula <- as.formula("growth_rate ~ overwinter_ice")
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
    summary_model <- summary(model)
    bonferroni_p_value <- p.adjust(summary_model$tTable[2, 4], method = "bonferroni", n = 5)
    
    if (bonferroni_p_value < 0.05) {
      results[[paste("Lag_Indiv", lag)]] <- list(
        AIC = AIC(model),
        BIC = BIC(model),
        coefficients = summary_model$tTable,
        p_value = summary_model$tTable[2, 4],
        bonferroni_p_value = bonferroni_p_value,
        model = model,
        penguin_data = penguin_data
      )
    }
  }
  
  return(results)
}

# Main function to run GLS analysis for each condition and save results
run_gls_analysis <- function(penguin_abundance_data, ice_data, metrics, results_dir) {
  # Initialize results list
  all_gls_results_indiv <- list()
  all_results_df_indiv <- list()
  significant_results_df_indiv <- list()
  
  # List unique thresholds and home range sizes
  thresholds <- unique(ice_data$Threshold)
  home_range_sizes <- unique(ice_data$HomeRangeSize)
  
  for (metric in metrics) {
    for (threshold in thresholds) {
      for (home_range_size in home_range_sizes) {
        ice_subset <- ice_data %>%
          filter(Threshold == threshold, HomeRangeSize == home_range_size)
        
        print(paste("Running GLS analysis for metric:", metric, "threshold:", threshold, "home range size:", home_range_size))
        
        # Perform GLS analysis for individual lag years
        gls_results_indiv <- fit_gls_models_indiv(penguin_abundance_data, ice_subset, metric)
        
        # Store results for individual lag years
        all_gls_results_indiv[[paste("Annual_Indiv", metric, threshold, home_range_size, sep = "_")]] <- gls_results_indiv
        
        results_list_indiv <- lapply(names(gls_results_indiv), function(lag) {
          result <- gls_results_indiv[[lag]]
          data.frame(
            Metric = metric,
            HomeRangeSize = home_range_size,
            Threshold = threshold,
            Lag = lag,
            AIC = result$AIC,
            BIC = result$BIC,
            Coefficient_Intercept = result$coefficients[1, 1],
            StdError_Intercept = result$coefficients[1, 2],
            tValue_Intercept = result$coefficients[1, 3],
            pValue_Intercept = result$coefficients[1, 4],
            Coefficient = result$coefficients[2, 1],
            StdError = result$coefficients[2, 2],
            tValue = result$coefficients[2, 3],
            pValue = result$coefficients[2, 4],
            Bonferroni_Adjusted_pValue = result$bonferroni_p_value
          )
        })
        
        all_results_df_indiv <- append(all_results_df_indiv, results_list_indiv)
        significant_results_df_indiv <- append(significant_results_df_indiv, Filter(function(x) x$Bonferroni_Adjusted_pValue < 0.05, results_list_indiv))
      }
    }
  }
  
  # Export the compiled results to CSV
  for (metric in metrics) {
    metric_results_df_indiv <- do.call(rbind, lapply(all_results_df_indiv, function(x) if (x$Metric == metric) x else NULL))
    metric_significant_results_df_indiv <- do.call(rbind, lapply(significant_results_df_indiv, function(x) if (x$Metric == metric) x else NULL))
    
    write.csv(metric_results_df_indiv, file.path(results_dir, paste0("model_results_indiv_", metric, ".csv")), row.names = FALSE)
    write.csv(metric_significant_results_df_indiv, file.path(results_dir, paste0("significant_model_results_indiv_", metric, ".csv")), row.names = FALSE)
  }
  
  # Save only significant results to RDS
  significant_gls_results_indiv <- list()
  for (metric in names(all_gls_results_indiv)) {
    significant_gls_results_indiv[[metric]] <- Filter(Negate(is.null), all_gls_results_indiv[[metric]])
  }
  saveRDS(significant_gls_results_indiv, file.path(results_dir, "gls_analysis_results_indiv.rds"))
}



# Example Usage for a Small Subset

# Load Gentoo penguin data
penguin_abundance_data <- fread("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv")

# Adjust the season column in the penguin abundance data
penguin_abundance_data <- penguin_abundance_data %>%
  mutate(year = 1970 + season - 1)

# Load the SIC statistics
metric_calculation_csv <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
ice_data <- fread(file.path(metric_calculation_csv, "sea_ice_duration_persistence_stats.csv"))

# Define the metrics to analyze
metrics <- c("mean_duration", "sd_duration", "mean_persistence", "sd_persistence")

# Define the results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"
dir.create(results_dir, showWarnings = FALSE)

run_gls_analysis(penguin_abundance_data, ice_data, metrics, results_dir)




# Load necessary libraries
library(dplyr)
library(readr)

# Define the directory containing the significant CSV files
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"

# Define the metrics to analyze
metrics <- c("mean_duration", "sd_duration", "mean_persistence", "sd_persistence")

# Initialize an empty list to store the data frames
all_significant_results <- list()

# Loop through each metric and read the corresponding CSV file
for (metric in metrics) {
  file_path <- file.path(results_dir, paste0("significant_model_results_indiv_", metric, ".csv"))
  
  # Check if the file exists
  if (file.exists(file_path)) {
    # Read the CSV file and add the metric as a new column
    data <- read_csv(file_path) %>%
      mutate(Metric_Type = metric)
    all_significant_results[[metric]] <- data
  } else {
    cat("File does not exist:", file_path, "\n")
  }
}

# Combine all data frames into a single data frame
merged_significant_results <- bind_rows(all_significant_results)

# Save the merged data frame to a new CSV file
merged_file_path <- file.path(results_dir, "merged_significant_model_results_duration_persistence.csv")
write_csv(merged_significant_results, merged_file_path)

cat("Merged CSV file saved to:", merged_file_path, "\n")



```



**Compile and organize Significant Results**
```{r}
# Load necessary libraries
library(data.table)
library(dplyr)

# Define the directory paths
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"
significant_results_dir <- file.path(results_dir, "bonferroni_significant_results")

# Create the new subdirectory if it doesn't exist
dir.create(significant_results_dir, showWarnings = FALSE)

# Function to clean HomeRangeSize column
clean_home_range_size <- function(df) {
  df %>%
    mutate(HomeRangeSize = as.numeric(gsub("km", "", HomeRangeSize)))
}

# Function to create subsets of data based on Bonferroni significance criteria and Metric
subset_significant_results <- function(file_path) {
  # Load the data
  data <- fread(file_path)
  
  # Clean the HomeRangeSize column
  data <- clean_home_range_size(data)
  
  # Get unique metrics
  metrics <- unique(data$Metric)
  
  # Initialize a list to store subsets
  metric_data_list <- list()
  
  # Loop through each metric and create subset
  for (metric in metrics) {
    metric_data <- data %>%
      filter(Metric == metric, Bonferroni_Adjusted_pValue < 0.05)
    
    if (nrow(metric_data) > 0) {
      metric_data_list[[metric]] <- metric_data
    }
  }
  
  return(metric_data_list)
}

# Get a list of CSV files in the directory
csv_files <- list.files(path = results_dir, pattern = "*.csv", full.names = TRUE)

# Apply the function to each CSV file and bind the results by Metric
all_significant_results <- lapply(csv_files, subset_significant_results)
all_significant_results <- do.call(rbind, unlist(all_significant_results, recursive = FALSE))

# Split the data by Metric and save to separate CSV files
split_data <- split(all_significant_results, all_significant_results$Metric)

# Save each subset to a separate CSV file
lapply(names(split_data), function(metric) {
  output_file_name <- paste0(metric, "_bonferroni_significant.csv")
  output_file_path <- file.path(significant_results_dir, output_file_name)
  write.csv(split_data[[metric]], output_file_path, row.names = FALSE)
})


```


**Plot SIC and Concentration Results for Single model**
```{r}
# Load necessary libraries
library(nlme)
library(data.table)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(showtext)
library(patchwork)

# Define file paths
model_results_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results/model_results_indiv_ice_metrics.csv"

daily_sic_statistics_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv/daily_sic_statistics.csv"

modeled_gentoo_parameters_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv"

# Load the data files
model_results_df <- fread(model_results_path)
daily_sic_statistics_df <- fread(daily_sic_statistics_path)
modeled_gentoo_parameters_df <- fread(modeled_gentoo_parameters_path)

# Compute annual ice metrics
compute_annual_ice_metrics <- function(ice_data, metrics) {
  ice_data <- ice_data %>%
    mutate(year = year(date))
  
  annual_ice_metrics <- ice_data %>%
    group_by(year, Threshold, HomeRangeSize) %>%
    summarize(across(all_of(metrics), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}"),
              across(all_of(metrics), ~sd(.x, na.rm = TRUE), .names = "sd_{.col}"),
              .groups = 'drop')
  
  return(annual_ice_metrics)
}

# Define the metrics to analyze
metrics <- c("mean_sic", "ice_extent_km2", "sd_sic")

# Compute the annual ice metrics
annual_ice_metrics <- compute_annual_ice_metrics(daily_sic_statistics_df, metrics)

# Function to get precomputed ice metrics
get_precomputed_ice_metrics <- function(year, ice_data, metric) {
  ice_value <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_value) > 0) {
    return(mean(ice_value, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Calculate the average sea ice concentration, extent, sd_sic, and sd_extent from the prior year
penguin_data_with_ice <- modeled_gentoo_parameters_df %>%
  rowwise() %>%
  mutate(overwinter_ice_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_mean_sic"),
         overwinter_extent_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_ice_extent_km2"),
         overwinter_sd_sic_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_sd_sic"),
         overwinter_sd_extent_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "sd_ice_extent_km2")) %>%
  ungroup()

# Filter out growth rates above 3
filtered_penguin_data <- penguin_data_with_ice %>%
  filter(growth_rate <= 3)

# Function to fit GLS model for average metric from 1 year with filtered data
fit_gls_model_avg <- function(penguin_data, metric) {
  penguin_data <- penguin_data %>%
    filter(!is.na(!!sym(metric)), !is.na(growth_rate))
  
  print(paste("Fitting GLS model for average lag 1 year for", metric))
  formula <- as.formula(paste("growth_rate ~", metric))
  model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
  
  return(model)
}

# Fit the models for the new metrics
gls_models <- list()
metrics_list <- c("overwinter_ice_1yr", "overwinter_extent_1yr", "overwinter_sd_sic_1yr", "overwinter_sd_extent_1yr")

for (metric in metrics_list) {
  gls_models[[metric]] <- fit_gls_model_avg(filtered_penguin_data, metric)
}

# Get confidence intervals for the model coefficients
conf_intervals <- lapply(gls_models, intervals)

# Extract the confidence intervals for the intercept and slope
conf_intervals_list <- lapply(conf_intervals, function(ci) {
  list(
    intercept_ci = ci$coef[1, ],
    slope_ci = ci$coef[2, ]
  )
})

# Plot settings
font <- "Gudea"
font_add_google(family = font, font, db_cache = TRUE)
showtext_auto()

theme_set(theme_minimal(base_family = font, base_size = 12))
bg <- "#F4F5F1"
txt_col <- "black"

# Function to create prediction data and plot
create_plot <- function(penguin_data, gls_model, intercept_ci, slope_ci, metric, x_label, title) {
  new_data <- data.frame(overwinter_metric = seq(min(penguin_data[[metric]], na.rm = TRUE),
                                                 max(penguin_data[[metric]], na.rm = TRUE),
                                                 length.out = 100))
  
  colnames(new_data) <- metric  # Rename the column back to its original name
  
  predicted_values <- predict(gls_model, new_data)
  
  new_data$fit <- predicted_values
  new_data$upper <- intercept_ci[3] + slope_ci[3] * new_data[[metric]]
  new_data$lower <- intercept_ci[1] + slope_ci[1] * new_data[[metric]]
  
  plot <- ggplot() +
    geom_point(data = penguin_data, aes_string(x = metric, y = "growth_rate"), color = "black", size = 1) +
    geom_line(data = new_data, aes_string(x = metric, y = "fit"), color = "blue") +
    geom_ribbon(data = new_data, aes_string(x = metric, ymin = "lower", ymax = "upper"), alpha = 0.2) +
    labs(title = title,
         x = x_label,
         y = "Growth Rate") +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      plot.caption = element_text(hjust = 0.5, size = 10),
      plot.margin = margin(20, 20, 20, 20)
    ) +
    ylim(NA, 1.5)
  
  return(plot)
}

# Create the plots with updated x-axis labels
plot_mean_sic <- create_plot(filtered_penguin_data, gls_models[["overwinter_ice_1yr"]],
                             conf_intervals_list[["overwinter_ice_1yr"]]$intercept_ci,
                             conf_intervals_list[["overwinter_ice_1yr"]]$slope_ci,
                             "overwinter_ice_1yr", "Winter SIC Mean", "Growth Rate vs. Mean SIC")

plot_sd_sic <- create_plot(filtered_penguin_data, gls_models[["overwinter_sd_sic_1yr"]],
                           conf_intervals_list[["overwinter_sd_sic_1yr"]]$intercept_ci,
                           conf_intervals_list[["overwinter_sd_sic_1yr"]]$slope_ci,
                           "overwinter_sd_sic_1yr", "Winter SIC SD", "Growth Rate vs. SD SIC")

plot_extent <- create_plot(filtered_penguin_data, gls_models[["overwinter_extent_1yr"]],
                           conf_intervals_list[["overwinter_extent_1yr"]]$intercept_ci,
                           conf_intervals_list[["overwinter_extent_1yr"]]$slope_ci,
                           "overwinter_extent_1yr", "Winter Extent", "Growth Rate vs. Ice Extent")

plot_sd_extent <- create_plot(filtered_penguin_data, gls_models[["overwinter_sd_extent_1yr"]],
                              conf_intervals_list[["overwinter_sd_extent_1yr"]]$intercept_ci,
                              conf_intervals_list[["overwinter_sd_extent_1yr"]]$slope_ci,
                              "overwinter_sd_extent_1yr", "Winter Extent SD", "Growth Rate vs. SD Ice Extent")

# Save the plot objects
save(plot_mean_sic, plot_sd_sic, plot_extent, plot_sd_extent, file = "penguin_growth_sic_extent_rate_plots.RData")

# Load the plot objects (if needed in the future)
# load("penguin_growth_sic_extent_rate_plots.RData")

# Combine the plots i
# n a 2x2 grid
final_plot <- (plot_mean_sic + plot_sd_sic) / (plot_extent + plot_sd_extent) +
  plot_layout(heights = c(2, 2)) +
  plot_annotation(
    theme = theme(
      plot.background = element_rect(fill = bg, color = NA),
      plot.margin = margin(20, 20, 20, 20)
    )
  )

# Display the final plot
print(final_plot)

# Save the figure
ggsave("growth_rate_effects_sic_extent.png", plot = final_plot, bg = bg, height = 10, width = 12, dpi = 300)


```



**Plot Forest Plots - Concentration/Extent**


```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(readr)

# Define the directory containing the significant CSV files
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results/concentration_extent"

# Define the metrics to analyze
metrics <- c("Sea Ice Concentration", "Concentration SD", "Sea Ice Extent", "Sea Ice Extent SD")

# Load the data
data <- read_csv(file.path(results_dir, "significant_model_results_indiv_ice_metrics_extent_extentsd.csv"))

# Reorder the factor levels for HomeRangeSize and Metric
data$HomeRangeSize <- factor(data$HomeRangeSize, levels = c("25km", "50km", "100km", "150km", "200km", "250km", "300km", "350km", "400km", "450km", "500km"))
data$Metric <- recode(data$Metric, 
                      'mean_sic' = 'Sea Ice Concentration', 
                      'sd_sic' = 'Concentration SD', 
                      'ice_extent_km2' = 'Sea Ice Extent',
                      'Extent_SD' = 'Sea Ice Extent SD')

# Recode the Lag column
data$Lag <- recode(data$Lag, 
                   'Lag_Indiv 1' = '1 Year Lag', 
                   'Lag_Indiv 2' = '2 Year Lag', 
                   'Lag_Indiv 3' = '3 Year Lag', 
                   'Lag_Indiv 4' = '4 Year Lag', 
                   'Lag_Indiv 5' = '5 Year Lag')

# Function to create and save forest plots
create_forest_plot <- function(data, metric, results_dir) {
  if (nrow(data) == 0) {
    cat("No data available for", metric, "\n")
    return(NULL)
  }
  
  plot <- ggplot(data, aes(x = HomeRangeSize, y = Coefficient, color = Metric)) +
    geom_point() +
    geom_errorbar(aes(ymin = Coefficient - StdError, ymax = Coefficient + StdError), width = 0.2) +
    facet_wrap(~ Lag, scales = 'free_x', nrow = 2) +
    labs(title = paste("Forest Plot of Coefficients for", metric),
         x = "Home Range Size",
         y = "Coefficient") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Print the plot
  print(plot)
  
  # Save the plot as PNG
  ggsave(file.path(results_dir, paste0("forest_plot_", metric, ".png")), plot, width = 10, height = 8)
  
  # Save the plot as PDF
  pdf(file.path(results_dir, paste0("forest_plot_", metric, ".pdf")), width = 10, height = 8)
  print(plot)
  dev.off()
}

# Generate forest plots for each metric
for (metric in metrics) {
  print(metric)
  metric_data <- data %>% filter(Metric == metric)
  create_forest_plot(metric_data, metric, results_dir)
}

# Generate the combined forest plot
combined_plot <- ggplot(data, aes(x = HomeRangeSize, y = Coefficient, color = Metric)) +
  geom_point() +
  geom_errorbar(aes(ymin = Coefficient - StdError, ymax = Coefficient + StdError), width = 0.2) +
  facet_wrap(~ Lag, scales = 'free_x', nrow = 2) +
  labs(title = "Forest Plot of Coefficients by Metric, Home Range Size, and Lag",
       x = "Home Range Size",
       y = "Coefficient") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the combined plot
print(combined_plot)

# Save the combined plot as PNG
ggsave(file.path(results_dir, "forest_plot_combined_metrics.png"), combined_plot, width = 12, height = 10)

# Save the combined plot as PDF
pdf(file.path(results_dir, "forest_plot_combined_metrics.pdf"), width = 12, height = 10)
print(combined_plot)
dev.off()


```


**Plot Persistence and Duration Forest Plots**

```{r}
# Load necessary libraries
library(dplyr)
library(readr)
library(ggplot2)

# Define the directory containing the significant CSV files
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/model-results"

# Define the metrics to analyze
metrics <- c("mean_duration", "sd_duration", "mean_persistence", "sd_persistence")

# Initialize an empty list to store the data frames
all_significant_results <- list()

# Loop through each metric and read the corresponding CSV file
for (metric in metrics) {
  file_path <- file.path(results_dir, paste0("significant_model_results_indiv_", metric, ".csv"))
  
  # Check if the file exists
  if (file.exists(file_path)) {
    # Read the CSV file and add the metric as a new column
    data <- read_csv(file_path) %>%
      mutate(Metric_Type = metric)
    all_significant_results[[metric]] <- data
  } else {
    cat("File does not exist:", file_path, "\n")
  }
}

# Combine all data frames into a single data frame
merged_significant_results <- bind_rows(all_significant_results)

# Save the merged data frame to a new CSV file
merged_file_path <- file.path(results_dir, "merged_significant_model_results_duration_persistence.csv")
write_csv(merged_significant_results, merged_file_path)

cat("Merged CSV file saved to:", merged_file_path, "\n")

# Load the merged data for plotting
data <- read_csv(merged_file_path)

# Reorder the factor levels for HomeRangeSize and Metric
data$HomeRangeSize <- factor(data$HomeRangeSize, levels = c("25km", "50km", "100km", "150km", "200km", "250km", "300km", "350km", "400km", "450km", "500km"))
data$Metric <- recode(data$Metric, 
                      'mean_duration' = 'Duration', 
                      'sd_duration' = 'Duration SD', 
                      'mean_persistence' = 'Open Water Frequency',
                      'sd_persistence' = 'Open Water Frequency SD')

# Recode the Lag column
data$Lag <- recode(data$Lag, 
                   'Lag_Indiv 1' = '1 Year Lag', 
                   'Lag_Indiv 2' = '2 Year Lag', 
                   'Lag_Indiv 3' = '3 Year Lag', 
                   'Lag_Indiv 4' = '4 Year Lag',
                   'Lag_Indiv 5' = '5 Year Lag')

# Function to create and save forest plots
create_forest_plot <- function(data, metric, threshold, results_dir) {
  if (nrow(data) == 0) {
    cat("No data available for", metric, "at", threshold * 100, "% threshold\n")
    return(NULL)
  }
  
  plot <- ggplot(data, aes(x = HomeRangeSize, y = Coefficient, color = Metric)) +
    geom_point() +
    geom_errorbar(aes(ymin = Coefficient - StdError, ymax = Coefficient + StdError), width = 0.2) +
    facet_wrap(~ Lag, scales = 'free_x', nrow = 2) +
    labs(title = paste("Forest Plot of Coefficients for", metric, "at", threshold * 100, "% Threshold"),
         x = "Home Range Size",
         y = "Coefficient") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Print the plot
  print(plot)
  
  # Save the plot as PNG
  ggsave(file.path(results_dir, paste0("forest_plot_", metric, "_", threshold * 100, "pct.png")), plot, width = 10, height = 8)
  
  # Save the plot as PDF
  pdf(file.path(results_dir, paste0("forest_plot_", metric, "_", threshold * 100, "pct.pdf")), width = 10, height = 8)
  print(plot)
  dev.off()
}

# Generate forest plots for each metric and threshold
thresholds <- unique(data$Threshold)
for (metric in metrics) {
  for (threshold in thresholds) {
    metric_data <- data %>% filter(Metric_Type == metric, Threshold == threshold)
    create_forest_plot(metric_data, metric, threshold, results_dir)
  }
}

# Generate the combined forest plot
combined_plot <- ggplot(data, aes(x = HomeRangeSize, y = Coefficient, color = Metric)) +
  geom_point() +
  geom_errorbar(aes(ymin = Coefficient - StdError, ymax = Coefficient + StdError), width = 0.2) +
  facet_wrap(~ Lag + Threshold, scales = 'free_x', nrow = 2) +
  labs(title = "Forest Plot of Coefficients by Metric, Home Range Size, Lag, and Threshold",
       x = "Home Range Size",
       y = "Coefficient") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the combined plot
print(combined_plot)

# Save the combined plot as PNG
ggsave(file.path(results_dir, "Figure_forest_plot_combined_metrics_thresholds_Durations-Persistence.png"), combined_plot, width = 12, height = 10)

# Save the combined plot as PDF
pdf(file.path(results_dir, "Figure_forest_plot_combined_metrics_thresholds_Durations-Persistence.pdf"), width = 12, height = 10)
print(combined_plot)
dev.off()


```

**Plot Persistence and Duration for single model**

```{r}
# Load necessary libraries
library(nlme)
library(data.table)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(showtext)
library(patchwork)

# Define file paths
sea_ice_duration_persistence_stats_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv/sea_ice_duration_persistence_stats.csv"
modeled_gentoo_parameters_path <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/inputs/modeled_gentoo_parameters.csv"

# Load the data files
sea_ice_duration_persistence_df <- fread(sea_ice_duration_persistence_stats_path)
modeled_gentoo_parameters_df <- fread(modeled_gentoo_parameters_path)

# Compute annual ice metrics
compute_annual_ice_metrics <- function(ice_data, metrics) {
  ice_data <- ice_data %>%
    filter(month == "Season-wide") %>%
    mutate(year = as.integer(year))
  
  annual_ice_metrics <- ice_data %>%
    group_by(year, Threshold, HomeRangeSize) %>%
    summarize(across(all_of(metrics), ~mean(.x, na.rm = TRUE), .names = "mean_{.col}"),
              .groups = 'drop')
  
  return(annual_ice_metrics)
}

# Define the metrics to analyze
metrics <- c("mean_duration", "sd_duration", "mean_persistence", "sd_persistence")

# Compute the annual ice metrics
annual_ice_metrics <- compute_annual_ice_metrics(sea_ice_duration_persistence_df, metrics)

# Function to get precomputed ice metrics
get_precomputed_ice_metrics <- function(year, ice_data, metric) {
  ice_value <- ice_data %>%
    filter(year == !!year) %>%
    pull(!!sym(metric))
  
  if (length(ice_value) > 0) {
    return(mean(ice_value, na.rm = TRUE))
  } else {
    return(NA)
  }
}

# Calculate the average ice metrics from the prior year
penguin_data_with_ice <- modeled_gentoo_parameters_df %>%
  rowwise() %>%
  mutate(
    overwinter_mean_duration_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_mean_duration"),
    overwinter_sd_duration_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_sd_duration"),
    overwinter_mean_persistence_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_mean_persistence"),
    overwinter_sd_persistence_1yr = get_precomputed_ice_metrics(year - 1, annual_ice_metrics, "mean_sd_persistence")
  ) %>%
  ungroup()

# Filter out growth rates above 3
filtered_penguin_data <- penguin_data_with_ice %>%
  filter(growth_rate <= 3)

# Function to fit GLS model for average metric from 1 year with filtered data
fit_gls_model_avg <- function(penguin_data, metric) {
  penguin_data <- penguin_data %>%
    filter(!is.na(!!sym(metric)), !is.na(growth_rate))
  
  print(paste("Fitting GLS model for average lag 1 year for", metric))
  formula <- as.formula(paste("growth_rate ~", metric))
  model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_data)
  
  return(model)
}

# Fit the models for the new metrics
gls_models <- list()
metrics_list <- c("overwinter_mean_duration_1yr", "overwinter_sd_duration_1yr", "overwinter_mean_persistence_1yr", "overwinter_sd_persistence_1yr")

for (metric in metrics_list) {
  gls_models[[metric]] <- fit_gls_model_avg(filtered_penguin_data, metric)
}

# Get confidence intervals for the model coefficients
conf_intervals <- lapply(gls_models, intervals)

# Extract the confidence intervals for the intercept and slope
conf_intervals_list <- lapply(conf_intervals, function(ci) {
  list(
    intercept_ci = ci$coef[1, ],
    slope_ci = ci$coef[2, ]
  )
})

# Plot settings
font <- "Gudea"
font_add_google(family = font, font, db_cache = TRUE)
showtext_auto()

theme_set(theme_minimal(base_family = font, base_size = 12))
bg <- "#F4F5F1"
txt_col <- "black"

# Function to create prediction data and plot
create_plot <- function(penguin_data, gls_model, intercept_ci, slope_ci, metric, title) {
  new_data <- data.frame(overwinter_metric = seq(min(penguin_data[[metric]], na.rm = TRUE),
                                                 max(penguin_data[[metric]], na.rm = TRUE),
                                                 length.out = 100))
  
  colnames(new_data) <- metric  # Rename the column back to its original name
  
  predicted_values <- predict(gls_model, new_data)
  
  new_data$fit <- predicted_values
  new_data$upper <- intercept_ci[3] + slope_ci[3] * new_data[[metric]]
  new_data$lower <- intercept_ci[1] + slope_ci[1] * new_data[[metric]]
  
  plot <- ggplot() +
    geom_point(data = penguin_data, aes_string(x = metric, y = "growth_rate"), color = "black", size = 1) +
    geom_line(data = new_data, aes_string(x = metric, y = "fit"), color = "blue") +
    geom_ribbon(data = new_data, aes_string(x = metric, ymin = "lower", ymax = "upper"), alpha = 0.2) +
    labs(title = title,
         x = metric,
         y = "Growth Rate") +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      plot.caption = element_text(hjust = 0.5, size = 10),
      plot.margin = margin(20, 20, 20, 20)
    ) +
    ylim(NA, 1.5)
  
  return(plot)
}

# Create the plots
plot_mean_duration <- create_plot(filtered_penguin_data, gls_models[["overwinter_mean_duration_1yr"]],
                                  conf_intervals_list[["overwinter_mean_duration_1yr"]]$intercept_ci,
                                  conf_intervals_list[["overwinter_mean_duration_1yr"]]$slope_ci,
                                  "overwinter_mean_duration_1yr", "Growth Rate vs. Mean Duration")

plot_sd_duration <- create_plot(filtered_penguin_data, gls_models[["overwinter_sd_duration_1yr"]],
                                conf_intervals_list[["overwinter_sd_duration_1yr"]]$intercept_ci,
                                conf_intervals_list[["overwinter_sd_duration_1yr"]]$slope_ci,
                                "overwinter_sd_duration_1yr", "Growth Rate vs. SD Duration")

plot_mean_persistence <- create_plot(filtered_penguin_data, gls_models[["overwinter_mean_persistence_1yr"]],
                                     conf_intervals_list[["overwinter_mean_persistence_1yr"]]$intercept_ci,
                                     conf_intervals_list[["overwinter_mean_persistence_1yr"]]$slope_ci,
                                     "overwinter_mean_persistence_1yr", "Growth Rate vs. Mean Persistence")

plot_sd_persistence <- create_plot(filtered_penguin_data, gls_models[["overwinter_sd_persistence_1yr"]],
                                   conf_intervals_list[["overwinter_sd_persistence_1yr"]]$intercept_ci,
                                   conf_intervals_list[["overwinter_sd_persistence_1yr"]]$slope_ci,
                                   "overwinter_sd_persistence_1yr", "Growth Rate vs. SD Persistence")

# Save the plot objects
save(plot_mean_duration, plot_sd_duration, plot_mean_persistence, plot_sd_persistence, file = "penguin_growth_persistence-duration_rate_plots.RData")

# Load the plot objects (if needed in the future)
# load("penguin_growth_rate_plots.RData")

# Combine the plots in a 2x2 grid
final_plot <- (plot_mean_duration + plot_sd_duration) / (plot_mean_persistence + plot_sd_persistence) +
  plot_layout(heights = c(2, 2)) +
  plot_annotation(
    theme = theme(
      plot.background = element_rect(fill = bg, color = NA),
      plot.margin = margin(20, 20, 20, 20)
    )
  )

# Display the final plot
print(final_plot)

# Save the figure
ggsave("growth_rate_effects.png", plot = final_plot, bg = bg, height = 10, width = 12, dpi = 300)



```



**Compile Significant Results**
```{r}

# Load necessary libraries
library(dplyr)
library(readr)

# Define the directory containing the CSV files
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/results/significant"

# Get a list of all CSV files in the directory
csv_files <- list.files(path = results_dir, pattern = "*.csv", full.names = TRUE)

# Initialize an empty list to store the dataframes
dfs <- list()

# Loop through each CSV file and read the data
for (file in csv_files) {
  # Read the current CSV file using readr
  df <- read_csv(file, col_types = cols(.default = "c"))  # Read all columns as character
  
  # Append the dataframe to the list
  dfs <- append(dfs, list(df))
}

# Combine all dataframes into one, filling missing columns with NA
combined_results_df <- bind_rows(dfs)

# Define the output file path
output_file <- file.path(results_dir, "combined_significant_results.csv")

# Export the combined results to a new CSV file
write_csv(combined_results_df, output_file)

# Print a message indicating the completion of the process
cat("Combined results have been saved to:", output_file, "\n")


```


```{r gentoo mixed effects models SOI}

###############################################################################
# Load Necessary Libraries
###############################################################################
library(httr)       # for GET()
library(dplyr)
library(tidyr)      # for pivot_longer()
library(lubridate)
library(nlme)       # for mixed-effects models (lme)
library(readr)      # for reading CSVs
library(ggplot2)    # for plotting
library(tibble)     # for rownames_to_column()
library(knitr)      # for kable()
library(lme4)       # for lmer() models and bootMer()
library(mgcv)       # for GAMM models

###############################################################################
# Define Data Directory and Load Data
###############################################################################
data_dir <- "C:/Users/michael.wethington.BRILOON/OneDrive - Biodiversity Research Institute/Documents/Manuscripts - Antarctica/FrostBound_AQ_temporary/gentoo-abundance-model/"
gentoo_params <- read_csv(file.path(data_dir, "modeled_gentoo_parameters.csv"))
str(gentoo_params)

###############################################################################
# Download and Parse SOI Data from NOAA
###############################################################################
url <- "https://www.cpc.ncep.noaa.gov/data/indices/soi"
soi_raw <- tryCatch(GET(url), error = function(e) { stop("Error fetching SOI data: ", e) })
soi_text <- content(soi_raw, "text")
soi_lines <- strsplit(soi_text, "\n")[[1]]
standardized_soi_start <- grep("STANDARDIZED    DATA", soi_lines)
soi_standardized_lines <- soi_lines[(standardized_soi_start + 3):length(soi_lines)]
soi_standardized_lines <- soi_standardized_lines[!grepl("-999.9", soi_standardized_lines)]

soi_data <- read.table(text = soi_standardized_lines, fill = TRUE, stringsAsFactors = FALSE)
colnames(soi_data) <- c("Year", month.abb)
soi_data <- soi_data %>% mutate(Year = as.integer(Year))

soi_data_long <- soi_data %>%
  pivot_longer(-Year, names_to = "Month", values_to = "SOI") %>%
  mutate(
    Month = match(Month, month.abb),
    SOI   = as.numeric(SOI)
  ) %>%
  filter(!is.na(SOI))

# Optionally save the processed data for future use
saveRDS(soi_data_long, file = file.path(data_dir, "soi_data_long.rds"))
head(soi_data_long)

###############################################################################
# Prepare Winter SOI and Create Lagged Variables (June–Sept)
###############################################################################
winter_soi <- soi_data_long %>%
  filter(Month %in% c(6, 7, 8, 9)) %>%
  group_by(Year) %>%
  summarise(winter_soi = mean(SOI, na.rm = TRUE), .groups = "drop")

max_lag <- 5
winter_soi_lags <- winter_soi
for (lag_i in 1:max_lag) {
  lag_col <- paste0("winter_soi_lag", lag_i)
  winter_soi_lags <- winter_soi_lags %>%
    mutate(!!lag_col := dplyr::lag(winter_soi, n = lag_i, order_by = Year))
}
head(winter_soi_lags)

###############################################################################
# Merge SOI Lags into Gentoo Growth Data
###############################################################################
merged_data <- gentoo_params %>%
  left_join(winter_soi_lags, by = c("year" = "Year"))
merged_data_complete <- merged_data %>% filter(!is.na(winter_soi_lag1))
head(merged_data_complete)

###############################################################################
# Example Diagnostic Plots for an nlme Model (Random Slopes)
###############################################################################
# Suppose we have a random-slope model named model_lme_slopes
par(mfrow = c(2, 2))
plot(model_lme_slopes)         # Residuals vs. Fitted, etc.
qqnorm(resid(model_lme_slopes))
qqline(resid(model_lme_slopes))

# Residual vs. Fitted and QQ-Plot (for your final random-intercept model)
par(mfrow=c(1,2))
plot(model_random_intercept, which = 1)  # Residuals vs. Fitted
qqnorm(resid(model_random_intercept, type = 'p'))  
qqline(resid(model_random_intercept, type = 'p'))  # QQ-Plot


###############################################################################
# Fit Mixed-Effects Models for Different Lag Terms using nlme
###############################################################################
model_list <- list()
for (lag in 1:5) {
  lag_term <- paste0("winter_soi_lag", lag)
  fixed_formula <- as.formula(paste("growth_rate ~", lag_term))
  random_formula <- as.formula(paste("~", lag_term, "| site"))
  
  model <- lme(
    fixed = fixed_formula,
    random = random_formula,
    correlation = corAR1(form = ~ year | site),
    data = merged_data,
    na.action = na.omit,
    control = lmeControl(opt = "optim", msMaxIter = 500)
  )
  
  model_list[[lag]] <- model
  cat("Lag", lag, "AIC:", AIC(model), "\n")
}

###############################################################################
# Refit a Model Using lmer (for Bootstrapping with bootMer)
###############################################################################
model_lmer <- lmer(growth_rate ~ winter_soi_lag1 + (winter_soi_lag1 | site),
                   data = merged_data, REML = TRUE)
summary(model_lmer)  # May give "boundary (singular) fit" warnings

boot_results <- bootMer(
  model_lmer,
  FUN = function(mod) fixef(mod),
  nsim = 1000
)
print(boot_results)

###############################################################################
# Geographical Visualization of Winter SOI (lag1)
###############################################################################
ggplot(merged_data_complete, aes(x = longitude, y = latitude)) +
  geom_point(aes(color = winter_soi_lag1), size = 3) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  labs(title = "Geographical Distribution of Winter SOI (lag1)",
       x = "Longitude", y = "Latitude",
       color = "Winter SOI (lag1)") +
  theme_minimal()

###############################################################################
# Fit a GAMM to Explore Nonlinear Effects
###############################################################################
gamm_model <- gamm(
  growth_rate ~ s(winter_soi_lag1),
  random = list(site = ~1),
  correlation = corAR1(form = ~ year | site),
  data = merged_data,
  na.action = na.omit
)
summary(gamm_model$gam)
plot(gamm_model$gam, pages = 1)

###############################################################################
# Compare Random-Intercept vs. Random-Slope Models
###############################################################################
# 1) Random-intercept-only model
model_random_intercept <- lme(
  fixed = growth_rate ~ winter_soi_lag1,
  random = ~ 1 | site,
  correlation = corAR1(form = ~ year | site),
  data = merged_data,
  na.action = na.omit,
  control = lmeControl(opt = "optim", msMaxIter = 500)
)
summary(model_random_intercept)
cat("Random-Intercept Model AIC:", AIC(model_random_intercept), "\n")

# 2) Compare via likelihood ratio test
anova_results <- anova(model_random_intercept, model_lme_slopes)
print(anova_results)


###############################################################################
# Visualize
###############################################################################


# Extract the fitted values and residuals from the final model
fitted_vals <- fitted(model_random_intercept)
resids      <- residuals(model_random_intercept)

# Reconstruct the "observed" values the model used
observed_vals <- fitted_vals + resids

# Put them in a dataframe with the same number of rows as the model
df_plot <- data.frame(
  observed = observed_vals,
  fitted   = fitted_vals
)

library(ggplot2)
ggplot(df_plot, aes(x = fitted, y = observed)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Observed vs. Fitted Growth Rates",
    x = "Fitted",
    y = "Observed"
  ) +
  theme_minimal()


# Convert fitted and residuals to data frame, preserving row names
fitted_df <- data.frame(fitted = fitted_vals, resids = resids)
fitted_df$observed <- fitted_df$fitted + fitted_df$resids
fitted_df$row_id   <- row.names(fitted_df)

# Convert your original 'merged_data' to have row names that match
# (This requires that merged_data was used directly by the model without reordering.)
model_data <- merged_data
row.names(model_data) <- as.character(seq_len(nrow(model_data)))

# Join them by row names
plot_df <- merge(model_data, fitted_df, by.x = "row.names", by.y = "row_id")

# Now you can plot 'observed' vs. 'fitted', colored by site or year, etc.
ggplot(plot_df, aes(x = fitted, y = observed, color = as.factor(site))) +
  geom_point(alpha = 0.5) +
  theme_minimal()


###############################################################################
# Caterpillar Plots for Random Intercepts
###############################################################################


# Extract random intercepts
ranef_int <- ranef(model_random_intercept)
ranef_df <- data.frame(site = rownames(ranef_int),
                       intercept = ranef_int[[1]])

# Order by intercept estimate
ranef_df <- ranef_df %>%
  arrange(intercept) %>%
  mutate(order = row_number())

ggplot(ranef_df, aes(x = intercept, y = reorder(site, intercept))) +
  geom_point() +
  geom_vline(xintercept = 0, linetype="dashed", color="grey50") +
  labs(title="Caterpillar Plot: Random Intercepts by Colony",
       x="Random Intercept Estimate", y="Colony") +
  theme_minimal()


final_fixef <- as.data.frame(summary(model_random_intercept)$tTable)
# Then you can convert it into a nicely formatted table
library(knitr)
kable(final_fixef, digits = 4,
      caption = "Fixed Effects for the Final Random-Intercept Model")


###############################################################################
# End of Script
###############################################################################


```



