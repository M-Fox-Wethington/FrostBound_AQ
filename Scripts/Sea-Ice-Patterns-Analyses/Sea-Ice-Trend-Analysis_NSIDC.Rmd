---
title: "Multidecadal Sea Ice Metrics Trend Analysis - Harmonized 12.5 km Sea Ice Index Dataset"
author: "Michael Wethington"
date: "2024-05-14"
output: html_document
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
library(terra)
library(dplyr)
library(lubridate)
```




**Calculate Sea Ice Extent, Concentration and variability dataframes**


This script calculates daily sea ice concentration (SIC), ice extent, and SIC variability from a raster stack of sea ice data for each day, and stores these metrics in a primary dataframe. It then filters the data for winter months (June-September) and calculates monthly variability metrics (standard deviation) for SIC and ice extent, which are stored in a separate dataframe. The script outputs both daily and monthly metrics to CSV and RDS files for further analysis. This allows for detailed daily and summarized monthly analysis of sea ice conditions within the study area.


Structure
    First Dataframe:
      1. Daily Sea Ice Concentration
      2. Daily Sea Ice Concentration variability (sd) (within individual raster layers)
      3. Daily Sea Ice Extent
    Second Dataframe:
      1. Average Monthly Sea Ice Concentration Variability (among rasters) 
      2. Average Monthly Sea Ice Extent variability

```{r Calculate Regional Ice Extent Metrics}
  # Load required libraries
library(terra)
library(dplyr)
library(lubridate)

# Function to calculate mean SIC, sea ice extent, and SIC variability for a given raster stack
calculate_stack_stats <- function(file_path) {
  # Load the raster stack from file
  raster_stack <- rast(file_path)
  
  # Extract dates from the raster stack
  layer_dates <- as.numeric(time(raster_stack))
  
  # Convert numeric dates to Date type
  layer_dates <- as.Date(layer_dates, origin = "1970-01-01")
  
  # Apply function to set all values < 0.15 to 0
  raster_stack <- app(raster_stack, fun = function(x) { x[x < 0.15] <- 0; return(x) })
  
  # Calculate mean SIC excluding NAs for each layer
  mean_sic <- global(raster_stack, fun = 'mean', na.rm = TRUE)[, 1]
  
  # Calculate the area of one cell in square meters (only once)
  cell_area_sq_meters <- prod(res(raster_stack[[1]]))
  
  # Handle sea ice extent: count cells >= 0.15 for each layer
  valid_ice_cells <- global(raster_stack >= 0.15, fun = 'sum', na.rm = TRUE)[, 1]
  total_ice_area_sq_km <- (valid_ice_cells * cell_area_sq_meters) / 1e6  # Convert total ice area to square kilometers
  
  # Calculate SIC variability (standard deviation of SIC) for each layer
  sic_sd <- global(raster_stack, fun = 'sd', na.rm = TRUE)[, 1]
  
  # Extract region name from the file path
  region_name <- tools::file_path_sans_ext(basename(file_path))
  region_name <- sub("NSIDC_25km_Harmonized_", "", region_name)  # Adjust based on actual pattern in filenames
  
  return(data.frame(date = layer_dates, mean_sic = mean_sic, ice_extent_km = total_ice_area_sq_km, 
                    sic_variability = sic_sd, region = region_name))
}

# Directory containing the .tif files
chunk_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif"

# List all .tif files for each region
chunk_files <- list.files(chunk_dir, pattern = "\\.tif$", full.names = TRUE)

# Initialize an empty dataframe to store the combined daily results
daily_df <- data.frame()

# Iterate through each .tif file and combine the results into daily_df
for (file in chunk_files) {
  cat("Processing file:", file, "\n")
  
  # Run the calculate_stack_stats function on the current file
  result <- calculate_stack_stats(file)
  
  # Combine the result into the master dataframe
  daily_df <- bind_rows(daily_df, result)
}

# Rename the region "nsidc_12_5km_harmonized_1979-2023" to "Complete Study Area"
daily_df <- daily_df %>%
  mutate(region = ifelse(region == "nsidc_12_5km_harmonized_1979-2023", "Complete Study Area", region))

# Rename columns
daily_df <- daily_df %>%
  rename(Date = date, Region = region, IceExtent_km = ice_extent_km, MeanSIC = mean_sic, SICVariability = sic_variability)

# If Date is not in Date format, convert it
if (!inherits(daily_df$Date, "Date")) {
  daily_df <- daily_df %>%
    mutate(Date = as.Date(Date, format = "%Y-%m-%d"))
}

# Filter for winter months (June - September) and add Year, Month, and Day columns
daily_df <- daily_df %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Create a separate dataframe for monthly variability
monthly_variability_df <- daily_df %>%
  group_by(Year, Month, Region) %>%
  summarize(MonthlySICVariability = sd(MeanSIC, na.rm = TRUE),
            MonthlyExtentVariability = sd(IceExtent_km, na.rm = TRUE))

# Save the daily metrics dataframe
write.csv(daily_df, file = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.csv", row.names = FALSE)
saveRDS(daily_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Save the monthly variability dataframe
write.csv(monthly_variability_df, file = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.csv", row.names = FALSE)
saveRDS(monthly_variability_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.rds")


```


**Calculate Sea Ice Mean Monthly Persistence and Duration and Concentration Dataframes**



*Persistence*: Measures the number of days within a month where SIC is above the threshold. This is useful to understand how often sea ice conditions are favorable within the specified months.

*Duration*: Measures the longest consecutive period within a month where SIC is above the threshold. This helps in understanding the consistency of sea ice conditions over time.

*Persistence_SD*: Variability in persistence values across the raster cells. Indicates how much the number of favorable sea ice days fluctuates within the month.

*Duration_SD*: Variability in the duration of consecutive favorable sea ice days. Indicates how consistent the longest periods of favorable conditions are within the month.



```{r}
# Load required libraries
library(terra)
library(dplyr)
library(lubridate)

# Define the function to calculate persistence and duration for a given raster stack
calculate_persistence_duration_stats <- function(file_path, ice_threshold = 0.15, winter_months = c(6, 7, 8, 9)) {
  # Load the raster stack from file
  raster_stack <- rast(file_path)
  
  # Extract dates from the raster stack
  layer_dates <- as.Date(as.numeric(time(raster_stack)), origin = "1970-01-01")
  
  # Filter for winter months (June - September)
  winter_indices <- which(month(layer_dates) %in% winter_months)
  winter_raster_stack <- subset(raster_stack, winter_indices)
  winter_dates <- layer_dates[winter_indices]
  
  # Calculate persistence: sum of days with SIC >= threshold for each month
  calculate_persistence <- function(x, threshold) {
    sum(x >= threshold, na.rm = TRUE)
  }
  
  # Calculate duration: maximum length of consecutive days with SIC >= threshold for each month
  calculate_duration <- function(x, threshold) {
    rle_result <- rle(as.vector(x) >= threshold)
    max_duration <- ifelse(any(rle_result$values), max(rle_result$lengths[rle_result$values]), 0)
    return(max_duration)
  }
  
  months <- unique(floor_date(winter_dates, "month"))
  persistence_list <- vector("list", length(months))
  duration_list <- vector("list", length(months))
  persistence_sd_list <- vector("list", length(months))
  duration_sd_list <- vector("list", length(months))
  names(persistence_list) <- months
  names(duration_list) <- months
  names(persistence_sd_list) <- months
  names(duration_sd_list) <- months
  
  for (month in months) {
    month_indices <- which(floor_date(winter_dates, "month") == month)
    month_raster_stack <- subset(winter_raster_stack, month_indices)
    
    persistence <- app(month_raster_stack, function(x) calculate_persistence(x, ice_threshold))
    duration <- app(month_raster_stack, function(x) calculate_duration(x, ice_threshold))
    
    persistence_list[[as.character(month)]] <- global(persistence, fun = mean, na.rm = TRUE)[, 1]
    duration_list[[as.character(month)]] <- global(duration, fun = mean, na.rm = TRUE)[, 1]
    
    # Calculate standard deviations
    persistence_sd_list[[as.character(month)]] <- global(persistence, fun = sd, na.rm = TRUE)[, 1]
    duration_sd_list[[as.character(month)]] <- global(duration, fun = sd, na.rm = TRUE)[, 1]
  }
  
  # Extract region name from the file path
  region_name <- tools::file_path_sans_ext(basename(file_path))
  region_name <- sub("NSIDC_25km_Harmonized_", "", region_name)
  
  # Combine results into a dataframe
  stats_df <- data.frame(
    Month = as.Date(names(persistence_list)),
    Persistence = unlist(persistence_list),
    Persistence_SD = unlist(persistence_sd_list),
    Mean_Monthly_Duration = unlist(duration_list),
    Duration_SD = unlist(duration_sd_list),
    Region = region_name
  )
  
  return(stats_df)
}

# Directory containing the .tif files
chunk_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif"

# List all .tif files for each region
chunk_files <- list.files(chunk_dir, pattern = "\\.tif$", full.names = TRUE)

# Initialize an empty dataframe to store the combined results
master_df <- data.frame()

# Iterate through each .tif file and combine the results into master_df
for (file in chunk_files) {
  cat("Processing file:", file, "\n")
  
  # Run the calculate_persistence_duration_stats function on the current file
  result <- calculate_persistence_duration_stats(file)
  
  # Combine the result into the master dataframe
  master_df <- bind_rows(master_df, result)
}

# Ensure the 'Date' column is in Date format and rename it
nsidc_metrics <- master_df %>%
  rename(Date = Month) %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d"))

# Check unique regions
cat("Unique regions:", unique(nsidc_metrics$Region), "\n")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Find all instances of NA in the Date column
na_subset <- nsidc_metrics %>%
  filter(is.na(Date))

# Find all instances with valid dates
valid_date_subset <- nsidc_metrics %>%
  filter(!is.na(Date))

# Check for matching rows between NA and valid date subsets
matching_rows <- na_subset %>%
  inner_join(valid_date_subset, by = c("Persistence", "Mean_Monthly_Duration", "Region", "Persistence_SD", "Duration_SD"), suffix = c("_na", "_valid"), relationship = "many-to-many")

# Find unique matches in matching_rows
unique_matches <- matching_rows %>%
  distinct(Persistence, Mean_Monthly_Duration, Region, .keep_all = TRUE)

# Print the number of unique matches
cat("Number of unique matches:", nrow(unique_matches), "\n")

# Check column names in unique_matches
cat("Column names in unique_matches:", names(unique_matches), "\n")

# Combine the unique matches with the original dataframe excluding the NA rows
cleaned_metrics <- valid_date_subset %>%
  bind_rows(unique_matches %>%
              select(-contains("_na"), Date = Date_valid))

# Check for any remaining NAs in the cleaned dataframe
remaining_na <- cleaned_metrics %>%
  filter(is.na(Date))

# Print the number of remaining NAs
cat("Number of remaining NAs:", nrow(remaining_na), "\n")
print(remaining_na)

# Ensure the cleaned_metrics does not have duplicated rows
cleaned_metrics <- cleaned_metrics %>%
  distinct()

# Print the final cleaned dataframe's structure
str(cleaned_metrics)

# Rename the region "nsidc_12_5km_harmonized_1979-2023" to "Complete Study Area"
cleaned_metrics <- cleaned_metrics %>%
  mutate(Region = ifelse(Region == "nsidc_12_5km_harmonized_1979-2023", "Complete Study Area", Region))


# Print the final cleaned dataframe's structure
str(cleaned_metrics)

# Save the master dataframe to a CSV file (optional)
write.csv(cleaned_metrics, file = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.csv", row.names = FALSE)

# Save the master dataframe as RDS (optional)
saveRDS(cleaned_metrics, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")



```





**Function to Run Analyses**

```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(nlme)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

analyze_sic_variability <- function(input_dataset, variables, time_periods, output_directory) {
  
  # Load your dataset
  nsidc_metrics <- readRDS(input_dataset)
  
  # Filter for winter months (June - September)
  winter_metrics <- nsidc_metrics %>%
    filter(Month %in% c(6, 7, 8, 9))
  
  # Initialize a list to store results
  gls_results <- list()
  
  for (variable in variables) {
    
    # Calculate monthly averages of the specified variable for each region
    monthly_averages <- winter_metrics %>%
      group_by(Year, Month, Region) %>%
      summarise(mean_var = mean(get(variable), na.rm = TRUE), 
                mean_var_sd = sd(get(variable), na.rm = TRUE), .groups = 'drop') %>%
      ungroup()
    
    # Create a time variable for trend analysis
    monthly_averages <- monthly_averages %>%
      mutate(Time = as.numeric(Year) + (Month - 1) / 12,
             month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))
    
    # Define a function to fit a GLS model for a given region and month
    fit_gls_model <- function(region_data) {
      gls_model <- gls(mean_var ~ Time, correlation = corAR1(), data = region_data)
      return(gls_model)
    }
    
    # Apply the model for each unique region and month combination and store the results
    regions <- unique(monthly_averages$Region)
    months <- unique(monthly_averages$Month)
    
    for (region in regions) {
      for (month in months) {
        region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
        if (nrow(region_month_data) > 1) {
          gls_results[[paste(variable, region, month, sep = "_")]] <- fit_gls_model(region_month_data)
        }
      }
    }
    
    if ("annual" %in% time_periods) {
      # Run GLS model for annual (June-September) metrics
      annual_metrics <- winter_metrics %>%
        group_by(Year, Region) %>%
        summarise(mean_var = mean(get(variable), na.rm = TRUE), 
                  mean_var_sd = sd(get(variable), na.rm = TRUE), .groups = 'drop') %>%
        ungroup() %>%
        mutate(Time = as.numeric(Year))
      
      for (region in regions) {
        region_annual_data <- annual_metrics %>% filter(Region == region)
        if (nrow(region_annual_data) > 1) {
          gls_results[[paste(variable, region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
        }
      }
    }
  }
  
  # Initialize a dataframe to store the GLS results
  gls_summary_df <- data.frame(
    Variable = character(),
    Region = character(),
    Month = character(),
    Intercept_Estimate = numeric(),
    Intercept_StdError = numeric(),
    Intercept_tValue = numeric(),
    Intercept_pValue = numeric(),
    Time_Estimate = numeric(),
    Time_StdError = numeric(),
    Time_tValue = numeric(),
    Time_pValue = numeric(),
    Adjusted_Time_pValue = numeric(),
    AIC = numeric(),
    BIC = numeric(),
    LogLik = numeric(),
    Residual_StdError = numeric(),
    DF_Residual = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Extract and organize GLS model results
  num_tests <- length(gls_results)
  
  for (key in names(gls_results)) {
    model <- gls_results[[key]]
    summary_model <- summary(model)
    
    # Extract coefficients
    coefficients <- summary_model$tTable
    intercept_estimate <- coefficients["(Intercept)", "Value"]
    intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
    intercept_t_value <- coefficients["(Intercept)", "t-value"]
    intercept_p_value <- coefficients["(Intercept)", "p-value"]
    
    time_estimate <- coefficients["Time", "Value"]
    time_std_error <- coefficients["Time", "Std.Error"]
    time_t_value <- coefficients["Time", "t-value"]
    time_p_value <- coefficients["Time", "p-value"]
    
    # Bonferroni correction for multiple comparisons
    adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
    
    # Extract other statistics
    aic <- AIC(model)
    bic <- BIC(model)
    log_lik <- logLik(model)
    residual_std_error <- summary_model$sigma
    df_residual <- summary_model$dims$N - summary_model$dims$p
    
    # Split key to get variable, region, and month
    key_split <- unlist(strsplit(key, "_"))
    variable <- key_split[1]
    region <- key_split[2]
    month <- ifelse(length(key_split) > 2, key_split[3], "Annual")
    
    # Append to dataframe
    gls_summary_df <- rbind(gls_summary_df, data.frame(
      Variable = variable,
      Region = region,
      Month = month,
      Intercept_Estimate = intercept_estimate,
      Intercept_StdError = intercept_std_error,
      Intercept_tValue = intercept_t_value,
      Intercept_pValue = intercept_p_value,
      Time_Estimate = time_estimate,
      Time_StdError = time_std_error,
      Time_tValue = time_t_value,
      Time_pValue = time_p_value,
      Adjusted_Time_pValue = adjusted_time_p_value,
      AIC = aic,
      BIC = bic,
      LogLik = log_lik,
      Residual_StdError = residual_std_error,
      DF_Residual = df_residual
    ))
  }
  
  # Save the organized GLS summary dataframe to a CSV file for each variable
  for (variable in variables) {
    output_file <- file.path(output_directory, paste0("GLS_Summary_Results_with_Bonferroni_", variable, ".csv"))
    write.csv(gls_summary_df %>% filter(Variable == variable), output_file, row.names = FALSE)
  }
  
  # Optional: View the organized GLS summary dataframe
  return(gls_summary_df)
}

# Example usage:
# result <- analyze_sic_variability(
#   input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.rds",
#   variables = c("MonthlySICVariability", "MonthlyExtentVariability"),
#   time_periods = c("monthly", "annual"),
#   output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis"
# )



```















This script fits Generalized Least Squares (GLS) models to analyze trends in sea ice extent over time for different regions and months during the winter period (June - September). It performs model diagnostics including residual vs. fitted plots, Q-Q plots, autocorrelation function (ACF) plots, the Breusch-Pagan test for heteroscedasticity, and the Shapiro-Wilk test for normality of residuals to ensure the assumptions of the models are met and to identify any potential issues.






**Analysis - Sea Ice Extent**


```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_extent = mean(IceExtent_km, na.rm = TRUE), 
            sd_extent = sd(IceExtent_km, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(mean_extent ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(mean_extent = mean(IceExtent_km, na.rm = TRUE), 
            sd_extent = sd(IceExtent_km, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "IceExtent_km",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
# write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/GLS_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

# Optional: View the organized GLS summary dataframe
gls_summary_df



```


**Monthly Extent Variability**

```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.rds")

# Filter for winter months (June - September)
winter_metrics <- nsidc_metrics %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of SIC variability for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_sic_var = mean(MonthlySICVariability, na.rm = TRUE), 
            mean_sic_var_sd = sd(MonthlySICVariability, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(mean_sic_var ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(mean_sic_var = mean(MonthlySICVariability, na.rm = TRUE), 
            mean_sic_var_sd = sd(MonthlySICVariability, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
num_tests <- length(gls_results)

for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Mean_SIC_Variability",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/SIC-Variability_Mean_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

# Optional: View the organized GLS summary dataframe
gls_summary_df

```


**Analysis - Mean Monthly Sea Ice Concentration**


```{r}

# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_sic = mean(MeanSIC, na.rm = TRUE), 
            sd_sic = sd(MeanSIC, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(mean_sic ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(mean_sic = mean(MeanSIC, na.rm = TRUE), 
            sd_sic = sd(MeanSIC, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Mean_SIC",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/SIC-Mean_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

# Optional: View the organized GLS summary dataframe
gls_summary_df

```

**Analysis - Variability of Mean Monthly Sea Ice Concentration **

```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_sic_var  = mean(SICVariability, na.rm = TRUE), 
            mean_sic_var_sd = sd(SICVariability, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(mean_sic_var ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(mean_sic_var = mean(SICVariability, na.rm = TRUE), 
            mean_sic_var_sd = sd(SICVariability, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Mean_SIC_Variability",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/SIC-Variability_Mean_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

# Optional: View the organized GLS summary dataframe
gls_summary_df

```



**Analysis - Monthy/Annual Persistence**


```{r}

# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_Persistence  = mean(Persistence, na.rm = TRUE), 
            Persistence_sd = sd(Persistence, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(mean_Persistence ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(mean_Persistence = mean(Persistence, na.rm = TRUE), 
            Persistence_sd = sd(Persistence, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Persistence",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

gls_summary_df

```


**Analysis - Monthly/Annual Persistence Varaibility **

```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(Persistence_SD  = mean(Persistence_SD, na.rm = TRUE), 
            Persistence_SD_sd = sd(Persistence_SD, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(Persistence_SD ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(Persistence_SD = mean(Persistence_SD, na.rm = TRUE), 
            Persistence_SD_sd = sd(Persistence_SD, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Persistence-Variability",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence-Variability_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

gls_summary_df


```



**Analaysis - Mean Monthly Duration**

```{r}

# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(Mean_Monthly_Duration  = mean(Mean_Monthly_Duration, na.rm = TRUE), 
            Mean_Monthly_Duration_SD = sd(Mean_Monthly_Duration, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(Mean_Monthly_Duration ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(Mean_Monthly_Duration = mean(Mean_Monthly_Duration, na.rm = TRUE), 
            Mean_Monthly_Duration_sd = sd(Mean_Monthly_Duration, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Mean_Monthly_Duration",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Mean_Monthly_Duration_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

gls_summary_df

```


**Analysis - Variability of Mean Monthly Duration**

```{r}

# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(Duration_SD  = mean(Duration_SD, na.rm = TRUE), 
            Duration_SD_sd = sd(Duration_SD, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(Duration_SD ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(Duration_SD = mean(Duration_SD, na.rm = TRUE), 
            Duration_SD_sd = sd(Duration_SD, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Duration-Variability",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Duration-Variability_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

gls_summary_df

```









**Raster Visualization Duration + Persistence + SD  Analysis**


```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(nlme)
library(ggplot2)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Rename the region "nsidc_12_5km_harmonized_1979-2023" to "Complete Study Area"
nsidc_metrics <- nsidc_metrics %>%
  mutate(Region = ifelse(Region == "nsidc_12_5km_harmonized_1979-2023", "Complete Study Area", Region))

# Rename columns
nsidc_metrics <- nsidc_metrics %>%
  rename(
    Mean_Monthly_Persistence = Persistence,
    Mean_Monthly_Duration = Mean_Monthly_Duration,
    Persistence_SD = Persistence_SD,
    Duration_SD = Duration_SD
  )

# If Date is not in Date format, convert it
if (!inherits(nsidc_metrics$Date, "Date")) {
  nsidc_metrics <- nsidc_metrics %>%
    mutate(Date = as.Date(Date, format = "%Y-%m-%d"))
}

# Filter for winter months (June - September) and add Year, Month, and Day columns
nsidc_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Create a time variable for trend analysis
nsidc_metrics <- nsidc_metrics %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12)

# Function to fit a GLS model for a given region and metric
fit_gls_model <- function(region_data, response_var) {
  formula <- as.formula(paste(response_var, "~ Time"))
  gls_model <- gls(formula, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Function to apply the GLS model and store results
apply_gls_model <- function(data, regions, metrics, period_label) {
  gls_results <- list()
  for (region in regions) {
    for (metric in metrics) {
      region_data <- data %>% filter(Region == region)
      if (nrow(region_data) > 1) {
        gls_results[[paste(region, metric, period_label, sep = "_")]] <- fit_gls_model(region_data, metric)
      }
    }
  }
  return(gls_results)
}

# Define regions and metrics
regions <- unique(nsidc_metrics$Region)
metrics <- c("Mean_Monthly_Persistence", "Persistence_SD", "Mean_Monthly_Duration", "Duration_SD")

# Apply the model for the winter period (June-September)
winter_gls_results <- apply_gls_model(nsidc_metrics, regions, metrics, "Winter")

# Apply the model for each month separately
monthly_gls_results <- list()
for (month in 6:9) {
  monthly_data <- nsidc_metrics %>% filter(Month == month)
  month_label <- month.name[month]
  monthly_gls_results[[month_label]] <- apply_gls_model(monthly_data, regions, metrics, month_label)
}

# Combine results
all_gls_results <- c(winter_gls_results, unlist(monthly_gls_results, recursive = FALSE))

# Function to extract summary information from GLS model
extract_gls_summary <- function(model, region, metric, period) {
  summary_model <- summary(model)
  coef_table <- summary_model$tTable
  intercept <- coef_table[1, ]
  time_coef <- coef_table[2, ]
  
  return(data.frame(
    Region = region,
    Metric = metric,
    Period = period,
    Intercept_Estimate = intercept["Value"],
    Intercept_StdError = intercept["Std.Error"],
    Intercept_tValue = intercept["t-value"],
    Intercept_pValue = intercept["p-value"],
    Time_Estimate = time_coef["Value"],
    Time_StdError = time_coef["Std.Error"],
    Time_tValue = time_coef["t-value"],
    Time_pValue = time_coef["p-value"],
    AIC = AIC(model),
    BIC = BIC(model),
    LogLik = logLik(model)[1],
    Residual_StdError = summary_model$sigma,
    DF_Residual = summary_model$dims$N - summary_model$dims$p
  ))
}

# Extract summaries for all models
gls_summary_df <- do.call(rbind, lapply(names(all_gls_results), function(key) {
  parts <- unlist(strsplit(key, "_"))
  region <- parts[1]
  metric <- parts[2]
  period <- parts[3]
  model <- all_gls_results[[key]]
  extract_gls_summary(model, region, metric, period)
}))

# Save the summary dataframe
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence-and-Duration-GLS_Model_Summaries.csv", row.names = FALSE)
saveRDS(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence-and-Duration-GLS_Model_Summaries.rds")

# Print the first few rows of the summary dataframe
print(head(gls_summary_df))



```





**Southern Oscillation Index Analysis**

```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(httr)
library(tidyr)
library(broom)
library(gridExtra)
library(RColorBrewer)

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_extent = mean(IceExtent_km, na.rm = TRUE), 
            sd_extent = sd(IceExtent_km, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Load SOI data
url <- "https://www.cpc.ncep.noaa.gov/data/indices/soi"
soi_raw <- GET(url)
soi_text <- content(soi_raw, "text")

# Preprocess the SOI data
soi_lines <- strsplit(soi_text, "\n")[[1]]

# Identify the start of the standardized SOI data section
standardized_soi_start <- grep("STANDARDIZED    DATA", soi_lines)

# Extract standardized SOI data lines
soi_standardized_lines <- soi_lines[(standardized_soi_start + 3):length(soi_lines)]
soi_standardized_lines <- soi_standardized_lines[!grepl("-999.9", soi_standardized_lines)] # remove lines with invalid data

# Convert standardized SOI data to dataframe
soi_data <- read.table(text = soi_standardized_lines, fill = TRUE, stringsAsFactors = FALSE)
colnames(soi_data) <- c("Year", month.abb)
soi_data <- soi_data %>% mutate(Year = as.integer(Year))

# Convert to long format
soi_data_long <- soi_data %>%
  pivot_longer(-Year, names_to = "Month", values_to = "SOI") %>%
  mutate(Month = match(Month, month.abb), 
         Year = as.integer(Year),
         SOI = as.numeric(SOI)) %>%
  filter(!is.na(SOI))  # Remove rows with NA values in SOI

# Merge sea ice data with SOI data
merged_data <- merge(monthly_averages, soi_data_long, by = c("Year", "Month"))

# Remove duplicate rows
merged_data <- merged_data[!duplicated(merged_data), ]

# Calculate correlation for each region and month
correlation_results <- merged_data %>%
  group_by(Region, Month) %>%
  summarize(correlation = cor(mean_extent, SOI, use = "complete.obs"),
            p.value = cor.test(mean_extent, SOI)$p.value) %>%
  arrange(Region, Month)

# Print the correlation results
print(correlation_results)




# Visualize the correlation results using RColorBrewer color palette
ggplot(correlation_results, aes(x = factor(Month, levels = 6:9), y = correlation, fill = Region)) +
  geom_bar(stat = "identity", position = position_dodge(0.9)) +
  geom_text(aes(label = round(correlation, 2)), vjust = -1.5, position = position_dodge(0.9)) +
  scale_fill_brewer(palette = "Set3") +  # You can choose other palettes like "Paired", "Dark2", etc.
  labs(title = "Correlation between SOI and Sea Ice Extent by Region and Month",
       x = "Month", y = "Correlation") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )




# Linear regression analysis to quantify the relationship between SOI and sea ice extent
regression_results <- merged_data %>%
  group_by(Region, Month) %>%
  do(tidy(lm(mean_extent ~ SOI, data = .)))

# Print regression results
print(regression_results)

# Filter the regression results for the SOI term
soi_coefficients <- regression_results %>%
  filter(term == "SOI")

# Function to create a plot for each region
plot_soi_effect <- function(region_data) {
  ggplot(region_data, aes(x = factor(Month), y = estimate, color = factor(Month))) +
    geom_point(size = 3) +
    geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(
      title = paste("Effect of SOI on Sea Ice Extent:", unique(region_data$Region)),
      x = "Month",
      y = "SOI Coefficient Estimate",
      color = "Month"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5),
      legend.position = "none"
    )
}

# Apply the function for each region and plot
unique_regions <- unique(soi_coefficients$Region)
plots <- lapply(unique_regions, function(region) {
  region_data <- soi_coefficients %>% filter(Region == region)
  plot_soi_effect(region_data)
})

# Display the plots
do.call(grid.arrange, c(plots, ncol = 2))

# # Create a combined plot showing the SOI coefficients for all regions and months
# ggplot(soi_coefficients, aes(x = factor(Month), y = estimate, color = Region)) +
#   geom_point(size = 3) +
#   geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
#   geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
#   labs(
#     title = "Effect of SOI on Sea Ice Extent Across Regions and Months",
#     x = "Month",
#     y = "SOI Coefficient Estimate",
#     color = "Region"
#   ) +
#   theme_minimal() +
#   theme(
#     plot.title = element_text(hjust = 0.5)
  # )

# Create individual plots for each region and arrange them using facet_wrap
ggplot(soi_coefficients, aes(x = factor(Month), y = estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  facet_wrap(~ Region, scales = "free_y") +
  labs(
    title = "Effect of SOI on Sea Ice Extent Across Regions and Months",
    x = "Month",
    y = "SOI Coefficient Estimate"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )


```







**Duration and Persistence + Trend analayis - Basic glm**


 
   Processing of Sea Ice Data

    Loading Data:
        The script loads the sea ice data files (raster stacks) and the study area shapefile.
        Each raster stack represents sea ice concentration for a specific year.

    Metric Calculation for Each Year:
        For each year in the range start_year to end_year, the script calculates the specified metrics (persistence, duration, concentration) for each cell in the raster stack.

    Persistence Calculation:
        Persistence: The number of days per year where sea ice concentration is above a threshold (0.15).
        This is calculated using a sum function over the raster stack for the year.

    Duration Calculation:
        Duration: The maximum consecutive days per year where sea ice concentration is above the threshold.
        This is calculated using a custom function that finds the longest run of days exceeding the threshold.

    Concentration Calculation:
        Concentration: The average sea ice concentration per year.
        This is calculated using a mean function over the raster stack for the year.

Applying Linear Models

    Time Series Data:
        After calculating the yearly metrics for each cell, the script builds a time series for each cell. This time series spans from start_year to end_year.

    Linear Model Fitting:
        For each cell, the script fits a linear model (lm) to the time series of yearly metric values.
        The response variable (y) is the metric value for each year.
        The predictor variable (years) is the sequence of years.
        
        

```{r}
# Load necessary libraries
library(terra)
library(dplyr)
library(lubridate)
library(nlme)
library(ggplot2)
library(grDevices)  # For colorRampPalette
library(gridExtra)  # For arranging plots side by side

# Define the function
analyze_sea_ice <- function(start_year, end_year, use_mask = TRUE, report_metrics = c("concentration", "duration", "persistence")) {
  
  # Define file paths
  chunk_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif"
  chunk_files <- list.files(chunk_dir, pattern = "\\.tif$", full.names = TRUE)
  study_area_shapefile <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
  
  # Load the study area shapefile
  if (use_mask) {
    study_area <- vect(study_area_shapefile)
  }
  
  # Define the threshold for sea ice concentration
  ice_threshold <- .15
  
  # Function to calculate consecutive days above threshold
  calculate_duration <- function(x, threshold) {
    if (all(is.na(x))) return(NA)
    rle_result <- rle(x > threshold)
    max_duration <- ifelse(any(rle_result$values), max(rle_result$lengths[rle_result$values]), 0)
    # print(paste0("Max Duration: ", max_duration))
    return(max_duration)
  }
  
  # Simplified function to process each chunk file using app
  process_chunk <- function(file_path) {
    region_name <- gsub(".*/|\\.tif$", "", file_path)
    message(paste("Processing file:", file_path))
    
    # Load the NSIDC sea ice concentration data for the subregion
    nsidc <- rast(file_path)
    
    annual_persistence <- list()
    annual_duration <- list()
    annual_concentration <- list()
    
    for (year in start_year:end_year) {
      message(paste("Processing year:", year))
      start_date <- as.Date(paste0(year, "-01-01"))
      end_date <- as.Date(paste0(year, "-12-31"))
      
      # Filter the sea ice data for the current year
      nsidc_year <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))
      
      if (nlyr(nsidc_year) == 0) {
        message(paste("No data for year:", year))
        next
      }
      
      # Calculate metrics based on options
      if ("persistence" %in% report_metrics) {
        persistence <- app(nsidc_year, function(x) sum(ifelse(x < ice_threshold, 0, 1), na.rm = FALSE))
        print(paste0("Persistence: ", persistence ))
        # summary(persistence[[1]])
        names(persistence) <- paste0("Persistence_", year)
        annual_persistence[[as.character(year)]] <- persistence
        print(paste0("Annaul peristence:" , annual_persistence))
      }

      
      if ("duration" %in% report_metrics) {
        duration <- app(nsidc_year, function(x) calculate_duration(x, ice_threshold))
        names(duration) <- paste0("Duration_", year)
        annual_duration[[as.character(year)]] <- duration
      }
      
      if ("concentration" %in% report_metrics) {
        concentration <- app(nsidc_year, function(x) mean(x, na.rm = TRUE))
        str(concentration)
        names(concentration) <- paste0("Concentration_", year)
        annual_concentration[[as.character(year)]] <- concentration
      }
    }
    
    message(paste("Finished processing file:", file_path))
    return(list(persistence = annual_persistence, duration = annual_duration, concentration = annual_concentration))
  }
  
  # Process each chunk file sequentially
  results <- lapply(chunk_files, process_chunk)
  
  # Combine all annual rasters into a single stack for the selected metrics
  combine_rasters <- function(raster_list) {
    raster_stack <- rast()
    for (year in names(raster_list)) {
      if (!is.null(raster_list[[year]])) {
        raster_stack <- c(raster_stack, raster_list[[year]])
      }
    }
    return(raster_stack)
  }
  
  persistence_stack <- NULL
  duration_stack <- NULL
  concentration_stack <- NULL
  
  if ("persistence" %in% report_metrics) {
    persistence_stack <- combine_rasters(do.call(c, lapply(results, `[[`, "persistence")))
    if (use_mask) persistence_stack <- mask(persistence_stack, study_area)
  }
  
  if ("duration" %in% report_metrics) {
    duration_stack <- combine_rasters(do.call(c, lapply(results, `[[`, "duration")))
    if (use_mask) duration_stack <- mask(duration_stack, study_area)
  }
  
  if ("concentration" %in% report_metrics) {
    concentration_stack <- combine_rasters(do.call(c, lapply(results, `[[`, "concentration")))
    if (use_mask) concentration_stack <- mask(concentration_stack, study_area)
  }
  
  # Function to apply linear model to each pixel and return slope and p-value
  apply_lm <- function(y) {
    if (all(is.na(y))) {
      return(c(NA, NA))
    } else {
      years <- start_year:end_year
      fit <- tryCatch(lm(y ~ years), error = function(e) return(NULL))
      if (is.null(fit)) {
        return(c(NA, NA))
      } else {
        slope <- coef(fit)[2]  # Slope of the linear model
        p_value <- summary(fit)$coefficients[2, 4]  # p-value for the slope
        return(c(slope, p_value))
      }
    }
  }
  
  persistence_trend <- NULL
  duration_trend <- NULL
  concentration_trend <- NULL
  
  if (!is.null(persistence_stack)) persistence_trend <- app(persistence_stack, apply_lm)
  if (!is.null(duration_stack)) duration_trend <- app(duration_stack, apply_lm)
  if (!is.null(concentration_stack)) concentration_trend <- app(concentration_stack, apply_lm)
  
  # Extract slopes and p-values from the results
  persistence_slope <- NULL
  persistence_pvalue <- NULL
  duration_slope <- NULL
  duration_pvalue <- NULL
  concentration_slope <- NULL
  concentration_pvalue <- NULL
  
  if (!is.null(persistence_trend)) {
    persistence_slope <- persistence_trend[[1]]
    persistence_pvalue <- persistence_trend[[2]]
  }
  
  if (!is.null(duration_trend)) {
    duration_slope <- duration_trend[[1]]
    duration_pvalue <- duration_trend[[2]]
  }
  
  if (!is.null(concentration_trend)) {
    concentration_slope <- concentration_trend[[1]]
    concentration_pvalue <- concentration_trend[[2]]
  }
  
  # Verify the integrity of the data
  if (!is.null(persistence_slope)) {
    print("Summary of persistence slope layer:")
    print(summary(persistence_slope))
    print("Summary of persistence p-value layer:")
    print(summary(persistence_pvalue))
  }
  
  if (!is.null(duration_slope)) {
    print("Summary of duration slope layer:")
    print(summary(duration_slope))
    print("Summary of duration p-value layer:")
    print(summary(duration_pvalue))
  }
  
  if (!is.null(concentration_slope)) {
    print("Summary of concentration slope layer:")
    print(summary(concentration_slope))
    print("Summary of concentration p-value layer:")
    print(summary(concentration_pvalue))
  }
  
  # Define a significance level
  alpha <- 0.05
  
  # Calculate statistics for persistence
  if (!is.null(persistence_slope)) {
    persistence_values <- values(persistence_slope)
    persistence_pvalues <- values(persistence_pvalue)
    persistence_data <- cbind(persistence_values, persistence_pvalues)
    persistence_data <- persistence_data[complete.cases(persistence_data), ]
    persistence_values <- persistence_data[, 1]
    persistence_pvalues <- persistence_data[, 2]
    persistence_mean <- mean(persistence_values)
    persistence_median <- median(persistence_values)
    persistence_min <- min(persistence_values)
    persistence_max <- max(persistence_values)
    persistence_significant <- sum(persistence_pvalues <= alpha) / length(persistence_pvalues) * 100
    persistence_positive <- sum(persistence_pvalues <= alpha & persistence_values > 0) / length(persistence_pvalues) * 100
    persistence_negative <- sum(persistence_pvalues <= alpha & persistence_values < 0) / length(persistence_pvalues) * 100
    persistence_nonsignificant <- 100 - persistence_significant
  }
  
  # Calculate statistics for duration
  if (!is.null(duration_slope)) {
    duration_values <- values(duration_slope)
    duration_pvalues <- values(duration_pvalue)
    duration_data <- cbind(duration_values, duration_pvalues)
    duration_data <- duration_data[complete.cases(duration_data), ]
    duration_values <- duration_data[, 1]
    duration_pvalues <- duration_data[, 2]
    duration_mean <- mean(duration_values)
    duration_median <- median(duration_values)
    duration_min <- min(duration_values)
    duration_max <- max(duration_values)
    duration_significant <- sum(duration_pvalues <= alpha) / length(duration_pvalues) * 100
    duration_positive <- sum(duration_pvalues <= alpha & duration_values > 0) / length(duration_pvalues) * 100
    duration_negative <- sum(duration_pvalues <= alpha & duration_values < 0) / length(duration_pvalues) * 100
    duration_nonsignificant <- 100 - duration_significant
  }
  
  # Calculate statistics for concentration
  if (!is.null(concentration_slope)) {
    concentration_values <- values(concentration_slope)
    concentration_pvalues <- values(concentration_pvalue)
    concentration_data <- cbind(concentration_values, concentration_pvalues)
    concentration_data <- concentration_data[complete.cases(concentration_data), ]
    concentration_values <- concentration_data[, 1]
    concentration_pvalues <- concentration_data[, 2]
    concentration_mean <- mean(concentration_values)
    concentration_median <- median(concentration_values)
    concentration_min <- min(concentration_values)
    concentration_max <- max(concentration_values)
    concentration_significant <- sum(concentration_pvalues <= alpha) / length(concentration_pvalues) * 100
    concentration_positive <- sum(concentration_pvalues <= alpha & concentration_values > 0) / length(concentration_pvalues) * 100
    concentration_negative <- sum(concentration_pvalues <= alpha & concentration_values < 0) / length(concentration_pvalues) * 100
    concentration_nonsignificant <- 100 - concentration_significant
  }
  
  # Print summary statistics for reporting
  if (!is.null(persistence_slope)) {
    persistence_summary <- data.frame(
      Metric = c("Mean slope", "Median slope", "Minimum slope", "Maximum slope", "% Significant", "% Positive", "% Negative", "% Non-significant"),
      Persistence = c(persistence_mean, persistence_median, persistence_min, persistence_max, persistence_significant, persistence_positive, persistence_negative, persistence_nonsignificant)
    )
    print(persistence_summary)
  }
  
  if (!is.null(duration_slope)) {
    duration_summary <- data.frame(
      Metric = c("Mean slope", "Median slope", "Minimum slope", "Maximum slope", "% Significant", "% Positive", "% Negative", "% Non-significant"),
      Duration = c(duration_mean, duration_median, duration_min, duration_max, duration_significant, duration_positive, duration_negative, duration_nonsignificant)
    )
    print(duration_summary)
  }
  
  if (!is.null(concentration_slope)) {
    concentration_summary <- data.frame(
      Metric = c("Mean slope", "Median slope", "Minimum slope", "Maximum slope", "% Significant", "% Positive", "% Negative", "% Non-significant"),
      Concentration = c(concentration_mean, concentration_median, concentration_min, concentration_max, concentration_significant, concentration_positive, concentration_negative, concentration_nonsignificant)
    )
    print(concentration_summary)
  }
  
  # Define a color palette for trends
  trend_colors <- colorRampPalette(c("red", "white", "blue"))
  
  # Set up multi-panel plotting environment
  par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))
  
  if (!is.null(persistence_slope)) {
    plot(persistence_slope, col=trend_colors(100), main="Trend of Persistence Over Time", legend=TRUE, colNA="black")
  }
  
  if (!is.null(duration_slope)) {
    plot(duration_slope, col=trend_colors(100), main="Trend of Duration Over Time", legend=TRUE, colNA="black")
  }
  
  if (!is.null(concentration_slope)) {
    plot(concentration_slope, col=trend_colors(100), main="Trend of Concentration Over Time", legend=TRUE, colNA="black")
  }
  
  # Classify the trends based on significance and directionality using ifel function
  if (!is.null(persistence_pvalue)) {
    significant_cells <- persistence_pvalue <= alpha
    significant_mask <- ifel(significant_cells, 1, NA)
    positive_trend <- ifel(significant_mask == 1 & persistence_slope > 0, 1, NA)
    negative_trend <- ifel(significant_mask == 1 & persistence_slope < 0, 2, NA)
    na_mask <- ifel(is.na(persistence_slope), 3, NA)
    combined_trend <- positive_trend
    combined_trend[!is.na(negative_trend)] <- 2
    combined_trend[!is.na(na_mask)] <- 3
    color_table <- data.frame(value = c(1, 2, 3), col = c("blue", "red", "black"))
    coltab(combined_trend) <- color_table
    plot(combined_trend, main="Significance and Directionality of Persistence Trends")
    legend("topright", legend=c("Positive Trend", "Negative Trend", "NA Cells"), fill=color_table$col)
  }
  
  if (!is.null(duration_pvalue)) {
    significant_cells <- duration_pvalue <= alpha
    significant_mask <- ifel(significant_cells, 1, NA)
    positive_trend <- ifel(significant_mask == 1 & duration_slope > 0, 1, NA)
    negative_trend <- ifel(significant_mask == 1 & duration_slope < 0, 2, NA)
    na_mask <- ifel(is.na(duration_slope), 3, NA)
    combined_trend <- positive_trend
    combined_trend[!is.na(negative_trend)] <- 2
    combined_trend[!is.na(na_mask)] <- 3
    color_table <- data.frame(value = c(1, 2, 3), col = c("blue", "red", "black"))
    coltab(combined_trend) <- color_table
    plot(combined_trend, main="Significance and Directionality of Duration Trends")
    legend("topright", legend=c("Positive Trend", "Negative Trend", "NA Cells"), fill=color_table$col)
  }
  
  if (!is.null(concentration_pvalue)) {
    significant_cells <- concentration_pvalue <= alpha
    significant_mask <- ifel(significant_cells, 1, NA)
    positive_trend <- ifel(significant_mask == 1 & concentration_slope > 0, 1, NA)
    negative_trend <- ifel(significant_mask == 1 & concentration_slope < 0, 2, NA)
    na_mask <- ifel(is.na(concentration_slope), 3, NA)
    combined_trend <- positive_trend
    combined_trend[!is.na(negative_trend)] <- 2
    combined_trend[!is.na(na_mask)] <- 3
    color_table <- data.frame(value = c(1, 2, 3), col = c("blue", "red", "black"))
    coltab(combined_trend) <- color_table
    plot(combined_trend, main="Significance and Directionality of Concentration Trends")
    legend("topright", legend=c("Positive Trend", "Negative Trend", "NA Cells"), fill=color_table$col)
  }
}

# Example usage
analyze_sea_ice(start_year = 1981, end_year = 2023, use_mask = FALSE, report_metrics = c("concentration", "duration", "persistence"))

```

