---
title: "Multidecadal Sea Ice Metrics Trend Analysis - Harmonized 12.5 km Sea Ice Index Dataset"
author: "Michael Wethington"
date: "2024-05-14"
output: html_document
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
library(terra)
library(dplyr)
library(lubridate)
```



**Calculate Sea Ice Extent, Concentration and variability dataframes**

This script calculates daily sea ice concentration (SIC), ice extent, and SIC variability from a raster stack of sea ice data for each day, and stores these metrics in a primary dataframe. It then filters the data for winter months (June-September) and calculates monthly variability metrics (standard deviation) for SIC and ice extent, which are stored in a separate dataframe. The script outputs both daily and monthly metrics to CSV and RDS files for further analysis. This allows for detailed daily and summarized monthly analysis of sea ice conditions within the study area.


Structure
    First Dataframe:
      1. Daily Sea Ice Concentration
      2. Daily Sea Ice Concentration variability (sd) (within individual raster layers)
      3. Daily Sea Ice Extent
    Second Dataframe:
      1. Average Monthly Sea Ice Concentration Variability (among rasters) 
      2. Average Monthly Sea Ice Extent variability



```{r Calculate Regional Ice Extent Metrics}
  # Load required libraries
library(terra)
library(dplyr)
library(lubridate)

# Function to calculate mean SIC, sea ice extent, and SIC variability for a given raster stack
calculate_stack_stats <- function(file_path) {
  # Load the raster stack from file
  raster_stack <- rast(file_path)
  
  # Extract dates from the raster stack
  layer_dates <- as.numeric(time(raster_stack))
  
  # Convert numeric dates to Date type
  layer_dates <- as.Date(layer_dates, origin = "1970-01-01")
  
  # Apply function to set all values < 0.15 to 0
  raster_stack <- app(raster_stack, fun = function(x) { x[x < 0.15] <- 0; return(x) })
  
  # Calculate mean SIC excluding NAs for each layer
  mean_sic <- global(raster_stack, fun = 'mean', na.rm = TRUE)[, 1]
  
  # Calculate the area of one cell in square meters (only once)
  cell_area_sq_meters <- prod(res(raster_stack[[1]]))
  
  # Handle sea ice extent: count cells >= 0.15 for each layer
  valid_ice_cells <- global(raster_stack >= 0.15, fun = 'sum', na.rm = TRUE)[, 1]
  total_ice_area_sq_km <- (valid_ice_cells * cell_area_sq_meters) / 1e6  # Convert total ice area to square kilometers
  
  # Calculate SIC variability (standard deviation of SIC) for each layer
  sic_sd <- global(raster_stack, fun = 'sd', na.rm = TRUE)[, 1]
  
  # Extract region name from the file path
  region_name <- tools::file_path_sans_ext(basename(file_path))
  region_name <- sub("NSIDC_25km_Harmonized_", "", region_name)  # Adjust based on actual pattern in filenames
  
  return(data.frame(date = layer_dates, mean_sic = mean_sic, ice_extent_km = total_ice_area_sq_km, 
                    sic_variability = sic_sd, region = region_name))
}

# Directory containing the .tif files
chunk_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif"

# List all .tif files for each region
chunk_files <- list.files(chunk_dir, pattern = "\\.tif$", full.names = TRUE)

# Initialize an empty dataframe to store the combined daily results
daily_df <- data.frame()

# Iterate through each .tif file and combine the results into daily_df
for (file in chunk_files) {
  cat("Processing file:", file, "\n")
  
  # Run the calculate_stack_stats function on the current file
  result <- calculate_stack_stats(file)
  
  # Combine the result into the master dataframe
  daily_df <- bind_rows(daily_df, result)
}

# Rename the region "nsidc_12_5km_harmonized_1979-2023" to "Complete Study Area"
daily_df <- daily_df %>%
  mutate(region = ifelse(region == "nsidc_12_5km_harmonized_1979-2023", "Complete Study Area", region))

# Rename columns
daily_df <- daily_df %>%
  rename(Date = date, Region = region, IceExtent_km = ice_extent_km, MeanSIC = mean_sic, SICVariability = sic_variability)

# If Date is not in Date format, convert it
if (!inherits(daily_df$Date, "Date")) {
  daily_df <- daily_df %>%
    mutate(Date = as.Date(Date, format = "%Y-%m-%d"))
}

# Filter for winter months (June - September) and add Year, Month, and Day columns
daily_df <- daily_df %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Create a separate dataframe for monthly variability
monthly_variability_df <- daily_df %>%
  group_by(Year, Month, Region) %>%
  summarize(MonthlySICVariability = sd(MeanSIC, na.rm = TRUE),
            MonthlyExtentVariability = sd(IceExtent_km, na.rm = TRUE))

# Save the daily metrics dataframe
write.csv(daily_df, file = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.csv", row.names = FALSE)
saveRDS(daily_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Save the monthly variability dataframe
write.csv(monthly_variability_df, file = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.csv", row.names = FALSE)
saveRDS(monthly_variability_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.rds")


```

**Calculate Sea Ice Mean Monthly Persistence and Duration and Concentration Dataframes**

*Persistence*: Measures the number of days within a month where SIC is above the threshold. This is useful to understand how often sea ice conditions are favorable within the specified months.

*Duration*: Measures the longest consecutive period within a month where SIC is above the threshold. This helps in understanding the consistency of sea ice conditions over time.

*Persistence_SD*: Variability in persistence values across the raster cells. Indicates how much the number of favorable sea ice days fluctuates within the month.

*Duration_SD*: Variability in the duration of consecutive favorable sea ice days. Indicates how consistent the longest periods of favorable conditions are within the month.



```{r Calculate Monthly Persistence and Duration Metrics}
# Load required libraries
library(terra)
library(dplyr)
library(lubridate)

# Define the function to calculate persistence and duration for a given raster stack
calculate_persistence_duration_stats <- function(file_path, ice_threshold = 0.15, winter_months = c(6, 7, 8, 9)) {
  # Load the raster stack from file
  raster_stack <- rast(file_path)
  
  # Extract dates from the raster stack
  layer_dates <- as.Date(as.numeric(time(raster_stack)), origin = "1970-01-01")
  
  # Filter for winter months (June - September)
  winter_indices <- which(month(layer_dates) %in% winter_months)
  winter_raster_stack <- subset(raster_stack, winter_indices)
  winter_dates <- layer_dates[winter_indices]
  
  # Calculate persistence: sum of days with SIC >= threshold for each month
  calculate_persistence <- function(x, threshold) {
    sum(x >= threshold, na.rm = TRUE)
  }
  
  # Calculate duration: maximum length of consecutive days with SIC >= threshold for each month
  calculate_duration <- function(x, threshold) {
    rle_result <- rle(as.vector(x) >= threshold)
    max_duration <- ifelse(any(rle_result$values), max(rle_result$lengths[rle_result$values]), 0)
    return(max_duration)
  }
  
  months <- unique(floor_date(winter_dates, "month"))
  persistence_list <- vector("list", length(months))
  duration_list <- vector("list", length(months))
  persistence_sd_list <- vector("list", length(months))
  duration_sd_list <- vector("list", length(months))
  names(persistence_list) <- months
  names(duration_list) <- months
  names(persistence_sd_list) <- months
  names(duration_sd_list) <- months
  
  for (month in months) {
    month_indices <- which(floor_date(winter_dates, "month") == month)
    month_raster_stack <- subset(winter_raster_stack, month_indices)
    
    persistence <- app(month_raster_stack, function(x) calculate_persistence(x, ice_threshold))
    duration <- app(month_raster_stack, function(x) calculate_duration(x, ice_threshold))
    
    persistence_list[[as.character(month)]] <- global(persistence, fun = mean, na.rm = TRUE)[, 1]
    duration_list[[as.character(month)]] <- global(duration, fun = mean, na.rm = TRUE)[, 1]
    
    # Calculate standard deviations
    persistence_sd_list[[as.character(month)]] <- global(persistence, fun = sd, na.rm = TRUE)[, 1]
    duration_sd_list[[as.character(month)]] <- global(duration, fun = sd, na.rm = TRUE)[, 1]
  }
  
  # Extract region name from the file path
  region_name <- tools::file_path_sans_ext(basename(file_path))
  region_name <- sub("NSIDC_25km_Harmonized_", "", region_name)
  
  # Combine results into a dataframe
  stats_df <- data.frame(
    Month = as.Date(names(persistence_list)),
    Persistence = unlist(persistence_list),
    Persistence_SD = unlist(persistence_sd_list),
    Mean_Monthly_Duration = unlist(duration_list),
    Duration_SD = unlist(duration_sd_list),
    Region = region_name
  )
  
  return(stats_df)
}

# Directory containing the .tif files
chunk_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif"

# List all .tif files for each region
chunk_files <- list.files(chunk_dir, pattern = "\\.tif$", full.names = TRUE)

# Initialize an empty dataframe to store the combined results
master_df <- data.frame()

# Iterate through each .tif file and combine the results into master_df
for (file in chunk_files) {
  cat("Processing file:", file, "\n")
  
  # Run the calculate_persistence_duration_stats function on the current file
  result <- calculate_persistence_duration_stats(file)
  
  # Combine the result into the master dataframe
  master_df <- bind_rows(master_df, result)
}

# Ensure the 'Date' column is in Date format and rename it
nsidc_metrics <- master_df %>%
  rename(Date = Month) %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d"))

# Check unique regions
cat("Unique regions:", unique(nsidc_metrics$Region), "\n")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Find all instances of NA in the Date column
na_subset <- nsidc_metrics %>%
  filter(is.na(Date))

# Find all instances with valid dates
valid_date_subset <- nsidc_metrics %>%
  filter(!is.na(Date))

# Check for matching rows between NA and valid date subsets
matching_rows <- na_subset %>%
  inner_join(valid_date_subset, by = c("Persistence", "Mean_Monthly_Duration", "Region", "Persistence_SD", "Duration_SD"), suffix = c("_na", "_valid"), relationship = "many-to-many")

# Find unique matches in matching_rows
unique_matches <- matching_rows %>%
  distinct(Persistence, Mean_Monthly_Duration, Region, .keep_all = TRUE)

# Print the number of unique matches
cat("Number of unique matches:", nrow(unique_matches), "\n")

# Check column names in unique_matches
cat("Column names in unique_matches:", names(unique_matches), "\n")

# Combine the unique matches with the original dataframe excluding the NA rows
cleaned_metrics <- valid_date_subset %>%
  bind_rows(unique_matches %>%
              select(-contains("_na"), Date = Date_valid))

# Check for any remaining NAs in the cleaned dataframe
remaining_na <- cleaned_metrics %>%
  filter(is.na(Date))

# Print the number of remaining NAs
cat("Number of remaining NAs:", nrow(remaining_na), "\n")
print(remaining_na)

# Ensure the cleaned_metrics does not have duplicated rows
cleaned_metrics <- cleaned_metrics %>%
  distinct()

# Print the final cleaned dataframe's structure
str(cleaned_metrics)

# Rename the region "nsidc_12_5km_harmonized_1979-2023" to "Complete Study Area"
cleaned_metrics <- cleaned_metrics %>%
  mutate(Region = ifelse(Region == "nsidc_12_5km_harmonized_1979-2023", "Complete Study Area", Region))


# Print the final cleaned dataframe's structure
str(cleaned_metrics)

# Save the master dataframe to a CSV file (optional)
write.csv(cleaned_metrics, file = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.csv", row.names = FALSE)

# Save the master dataframe as RDS (optional)
saveRDS(cleaned_metrics, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")



```



**Changepoint Analysis**

Penalty Options

The penalty argument in the cpt.mean function specifies how the cost of adding a changepoint is penalized. Here are the different penalty options:

    None: No penalty is applied. This may lead to overfitting, as many changepoints might be identified.
    SIC (Schwarz Information Criterion): Also known as BIC (Bayesian Information Criterion). It includes a penalty term for the number of changepoints, preventing overfitting by penalizing more complex models.
    BIC (Bayesian Information Criterion): Similar to SIC, penalizes more complex models to avoid overfitting.
    MBIC (Modified BIC): A modified version of BIC that can be more robust in certain scenarios.
    AIC (Akaike Information Criterion): Penalizes the number of parameters in the model, with less penalty than BIC, often leading to more changepoints being identified.
    Hannan-Quinn: A criterion similar to BIC and AIC but with a different penalty structure, typically lying between AIC and BIC in terms of strictness.
    Asymptotic: Uses an asymptotic penalty based on a theoretical Type I error rate. Requires the pen.value argument to specify the error rate (e.g., 0.05).
    Manual: Allows you to manually specify a penalty formula or value using the pen.value argument.
    CROPS (Change-in-Rank-Penalty Search): Searches over a range of penalty values, specified by a two-element vector in pen.value. Only works with the PELT method.

Method Options

The method argument in the cpt.mean function specifies the algorithm used for detecting changepoints. Here are the different method options:

    AMOC (At Most One Changepoint): Assumes at most one changepoint in the data. This method is simpler and faster but only detects a single changepoint.
    PELT (Pruned Exact Linear Time): A powerful and efficient method that can handle multiple changepoints. It uses a pruning technique to reduce the number of potential changepoints, making it suitable for larger datasets.
    SegNeigh (Segment Neighbourhood): Searches for the optimal segmentation by considering all possible segmentations up to a specified number of changepoints (Q). It is computationally intensive but can be more accurate for smaller datasets.
    BinSeg (Binary Segmentation): Uses a binary segmentation approach to recursively divide the data and identify changepoints. This method is faster than SegNeigh and can handle multiple changepoints, but it may be less accurate in some cases.

Example Adjustments

Depending on your data and the goals of your analysis, you can adjust the penalty and method arguments to better suit your needs. For example:

    For a conservative approach with fewer changepoints, you might use penalty = "BIC" or penalty = "SIC".
    To explore a broader range of possible changepoints, you might use penalty = "AIC" or penalty = "None".
    For large datasets where computational efficiency is important, you might use method = "PELT".
    For smaller datasets where precision is critical, you might use method = "SegNeigh" with an appropriate value for Q.

```{r Changepoint Analysis}

# Install necessary packages if not already installed
if (!require("changepoint")) install.packages("changepoint")
if (!require("dplyr")) install.packages("dplyr")

# Load libraries
library(changepoint)
library(dplyr)

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Inspect the structure of the dataset
str(nsidc_metrics)

# Aggregate the data to get the maximum extent for each year
annual_max_extent <- nsidc_metrics %>%
  group_by(Year) %>%
  summarize(Max_Extent = min(IceExtent_km, na.rm = TRUE))

# Define changepoint parameters
penalty_type <- "BIC"       # Options: "None", "SIC", "BIC", "MBIC", "AIC", "Hannan-Quinn", "Asymptotic", "Manual", "CROPS"
pen_value <- 0               # Value for manual or asymptotic penalty, or vector for CROPS
method_type <- "SegNeigh"        # Options: "AMOC", "PELT", "SegNeigh", "BinSeg"
max_changepoints <- 15        # Maximum number of changepoints for "BinSeg" or "SegNeigh" methods
test_statistic <- "Normal"   # Options: "Normal", "CUSUM"
min_segment_length <- 1      # Minimum segment length

# Run changepoint analysis on the maximum annual extent with specified parameters
cpt_annual_max <- cpt.mean(
  data = annual_max_extent$Max_Extent, 
  penalty = penalty_type, 
  pen.value = pen_value, 
  method = method_type, 
  Q = max_changepoints, 
  test.stat = test_statistic, 
  minseglen = min_segment_length
)

# Plot the results
plot(cpt_annual_max, main = "Changepoint Analysis of Maximum Annual Sea Ice Extent")
abline(v = annual_max_extent$Year[cpts(cpt_annual_max)], col = "red", lty = 2)

# Print the changepoints
changepoints_annual_max <- cpts(cpt_annual_max)
changepoints_annual_max_years <- annual_max_extent$Year[changepoints_annual_max]
print(changepoints_annual_max_years)


```





**Function to Run Analyses**

This script loads a dataset of sea ice metrics, filters it for winter months (June - September), and calculates both monthly and annual averages of a specified variable for different regions. The script fits a generalized least squares (GLS) model to these averages to identify trends over time. To address the monthly vs. annual issue, it ensures that annual metrics are calculated by aggregating all daily values within the specified months by year and then averaging them, rather than averaging pre-calculated monthly means, thus avoiding potential biases. The results are saved to a CSV file with appropriate metadata.

```{r GLS Analysis Function}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(nlme)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

gls_analysis_function <- function(input_dataset, variable, output_directory, start_year = NULL, end_year = NULL) {
  
  # Load your dataset
  nsidc_metrics <- readRDS(input_dataset)
  
  # Ensure Year, Month, Day columns exist
  nsidc_metrics <- nsidc_metrics %>%
    mutate(Year = year(Date), Month = month(Date), Day = day(Date))
  
  # Filter for winter months (June - September)
  winter_metrics <- nsidc_metrics %>%
    filter(Month %in% c(6, 7, 8, 9))
  
  # Filter dataset by the specified year range if provided
  if (!is.null(start_year)) {
    winter_metrics <- winter_metrics %>% filter(Year >= start_year)
  }
  if (!is.null(end_year)) {
    winter_metrics <- winter_metrics %>% filter(Year <= end_year)
  }
  
  # Initialize a list to store results
  gls_results <- list()
  
  # Calculate monthly averages of the specified variable for each region
  monthly_averages <- winter_metrics %>%
    group_by(Year, Month, Region) %>%
    summarise(mean_var = mean(get(variable), na.rm = TRUE), .groups = 'drop') %>%
    ungroup()
  
  # Create a time variable for trend analysis
  monthly_averages <- monthly_averages %>%
    mutate(Time = as.numeric(Year) + (Month - 1) / 12,
           month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))
  
  # Define a function to fit a GLS model for a given region and month
  fit_gls_model <- function(region_data) {
    gls_model <- gls(mean_var ~ Time, correlation = corAR1(), data = region_data)
    return(gls_model)
  }
  
  # Apply the model for each unique region and month combination and store the results
  regions <- unique(monthly_averages$Region)
  months <- unique(monthly_averages$Month)
  
  for (region in regions) {
    for (month in months) {
      region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
      if (nrow(region_month_data) > 1) {
        gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
      }
    }
  }
  
  # Calculate annual (June-September) metrics by aggregating all daily values
  annual_metrics <- winter_metrics %>%
    group_by(Year, Region) %>%
    summarise(mean_var = mean(get(variable), na.rm = TRUE), .groups = 'drop') %>%
    ungroup() %>%
    mutate(Time = as.numeric(Year))
  
  for (region in regions) {
    region_annual_data <- annual_metrics %>% filter(Region == region)
    if (nrow(region_annual_data) > 1) {
      gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
    }
  }
  
  # Initialize a dataframe to store the GLS results
  gls_summary_df <- data.frame(
    Region = character(),
    Metric = character(),
    Month = character(),
    Intercept_Estimate = numeric(),
    Intercept_StdError = numeric(),
    Intercept_tValue = numeric(),
    Intercept_pValue = numeric(),
    Time_Estimate = numeric(),
    Time_StdError = numeric(),
    Time_tValue = numeric(),
    Time_pValue = numeric(),
    Adjusted_Time_pValue = numeric(),
    AIC = numeric(),
    BIC = numeric(),
    LogLik = numeric(),
    Residual_StdError = numeric(),
    DF_Residual = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Extract and organize GLS model results
  num_tests <- length(gls_results)
  
  for (key in names(gls_results)) {
    model <- gls_results[[key]]
    summary_model <- summary(model)
    
    # Extract coefficients
    coefficients <- summary_model$tTable
    intercept_estimate <- coefficients["(Intercept)", "Value"]
    intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
    intercept_t_value <- coefficients["(Intercept)", "t-value"]
    intercept_p_value <- coefficients["(Intercept)", "p-value"]
    
    time_estimate <- coefficients["Time", "Value"]
    time_std_error <- coefficients["Time", "Std.Error"]
    time_t_value <- coefficients["Time", "t-value"]
    time_p_value <- coefficients["Time", "p-value"]
    
    # Bonferroni correction for multiple comparisons
    adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
    
    # Extract other statistics
    aic <- AIC(model)
    bic <- BIC(model)
    log_lik <- logLik(model)
    residual_std_error <- summary_model$sigma
    df_residual <- summary_model$dims$N - summary_model$dims$p
    
    # Split key to get variable, region, and month
    key_split <- unlist(strsplit(key, "_"))
    region <- key_split[1]
    month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
    
    # Append to dataframe
    gls_summary_df <- rbind(gls_summary_df, data.frame(
      Region = region,
      Metric = variable,
      Month = month,
      Intercept_Estimate = intercept_estimate,
      Intercept_StdError = intercept_std_error,
      Intercept_tValue = intercept_t_value,
      Intercept_pValue = intercept_p_value,
      Time_Estimate = time_estimate,
      Time_StdError = time_std_error,
      Time_tValue = time_t_value,
      Time_pValue = time_p_value,
      Adjusted_Time_pValue = adjusted_time_p_value,
      AIC = aic,
      BIC = bic,
      LogLik = log_lik,
      Residual_StdError = residual_std_error,
      DF_Residual = df_residual
    ))
  }
  
  # Save the organized GLS summary dataframe to a CSV file
  year_range <- paste(start_year, end_year, sep = "-")
  output_file <- file.path(output_directory, paste0("GLS_Summary_Results_with_Bonferroni_", variable, "_", year_range, ".csv"))
  write.csv(gls_summary_df, output_file, row.names = FALSE)
  
  # Optional: View the organized GLS summary dataframe
  return(gls_summary_df)
}

# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds",
  variable = "MeanSIC",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis",
  start_year = 1981,
  end_year = 2023
)


```


**Adjusted Function**

This accounts for missing day column in monthly datasets 

```{r GLS Analysis Function}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(nlme)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

gls_analysis_function <- function(input_dataset, variable, output_directory, start_year = NULL, end_year = NULL) {
  
  # Load your dataset
  nsidc_metrics <- readRDS(input_dataset)
  
  # Ensure Year, Month, Day columns exist if Date column exists
  if ("Date" %in% colnames(nsidc_metrics)) {
    nsidc_metrics <- nsidc_metrics %>%
      mutate(Year = year(Date), Month = month(Date), Day = day(Date))
  } else if ("Year" %in% colnames(nsidc_metrics) & "Month" %in% colnames(nsidc_metrics)) {
    nsidc_metrics <- nsidc_metrics %>%
      mutate(Day = 1) # Adding a default Day column
  } else {
    stop("The dataset must contain either a 'Date' column or 'Year' and 'Month' columns.")
  }
  
  # Filter for winter months (June - September)
  winter_metrics <- nsidc_metrics %>%
    filter(Month %in% c(6, 7, 8, 9))
  
  # Filter dataset by the specified year range if provided
  if (!is.null(start_year)) {
    winter_metrics <- winter_metrics %>% filter(Year >= start_year)
  }
  if (!is.null(end_year)) {
    winter_metrics <- winter_metrics %>% filter(Year <= end_year)
  }
  
  # Initialize a list to store results
  gls_results <- list()
  
  # Calculate monthly averages of the specified variable for each region
  monthly_averages <- winter_metrics %>%
    group_by(Year, Month, Region) %>%
    summarise(mean_var = mean(get(variable), na.rm = TRUE), .groups = 'drop') %>%
    ungroup()
  
  # Create a time variable for trend analysis
  monthly_averages <- monthly_averages %>%
    mutate(Time = as.numeric(Year) + (Month - 1) / 12,
           month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))
  
  # Define a function to fit a GLS model for a given region and month
  fit_gls_model <- function(region_data) {
    gls_model <- gls(mean_var ~ Time, correlation = corAR1(), data = region_data)
    return(gls_model)
  }
  
  # Apply the model for each unique region and month combination and store the results
  regions <- unique(monthly_averages$Region)
  months <- unique(monthly_averages$Month)
  
  for (region in regions) {
    for (month in months) {
      region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
      if (nrow(region_month_data) > 1) {
        gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
      }
    }
  }
  
  # Calculate annual (June-September) metrics by aggregating all daily values
  annual_metrics <- winter_metrics %>%
    group_by(Year, Region) %>%
    summarise(mean_var = mean(get(variable), na.rm = TRUE), .groups = 'drop') %>%
    ungroup() %>%
    mutate(Time = as.numeric(Year))
  
  for (region in regions) {
    region_annual_data <- annual_metrics %>% filter(Region == region)
    if (nrow(region_annual_data) > 1) {
      gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
    }
  }
  
  # Initialize a dataframe to store the GLS results
  gls_summary_df <- data.frame(
    Region = character(),
    Metric = character(),
    Month = character(),
    Intercept_Estimate = numeric(),
    Intercept_StdError = numeric(),
    Intercept_tValue = numeric(),
    Intercept_pValue = numeric(),
    Time_Estimate = numeric(),
    Time_StdError = numeric(),
    Time_tValue = numeric(),
    Time_pValue = numeric(),
    Adjusted_Time_pValue = numeric(),
    AIC = numeric(),
    BIC = numeric(),
    LogLik = numeric(),
    Residual_StdError = numeric(),
    DF_Residual = numeric(),
    Start_Year = integer(),
    End_Year = integer(),
    stringsAsFactors = FALSE
  )
  
  # Extract and organize GLS model results
  num_tests <- length(gls_results)
  
  for (key in names(gls_results)) {
    model <- gls_results[[key]]
    summary_model <- summary(model)
    
    # Extract coefficients
    coefficients <- summary_model$tTable
    intercept_estimate <- coefficients["(Intercept)", "Value"]
    intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
    intercept_t_value <- coefficients["(Intercept)", "t-value"]
    intercept_p_value <- coefficients["(Intercept)", "p-value"]
    
    time_estimate <- coefficients["Time", "Value"]
    time_std_error <- coefficients["Time", "Std.Error"]
    time_t_value <- coefficients["Time", "t-value"]
    time_p_value <- coefficients["Time", "p-value"]
    
    # Bonferroni correction for multiple comparisons
    adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
    
    # Extract other statistics
    aic <- AIC(model)
    bic <- BIC(model)
    log_lik <- logLik(model)
    residual_std_error <- summary_model$sigma
    df_residual <- summary_model$dims$N - summary_model$dims$p
    
    # Split key to get variable, region, and month
    key_split <- unlist(strsplit(key, "_"))
    region <- key_split[1]
    month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
    
    # Append to dataframe
    gls_summary_df <- rbind(gls_summary_df, data.frame(
      Region = region,
      Metric = variable,
      Month = month,
      Intercept_Estimate = intercept_estimate,
      Intercept_StdError = intercept_std_error,
      Intercept_tValue = intercept_t_value,
      Intercept_pValue = intercept_p_value,
      Time_Estimate = time_estimate,
      Time_StdError = time_std_error,
      Time_tValue = time_t_value,
      Time_pValue = time_p_value,
      Adjusted_Time_pValue = adjusted_time_p_value,
      AIC = aic,
      BIC = bic,
      LogLik = log_lik,
      Residual_StdError = residual_std_error,
      DF_Residual = df_residual,
      Start_Year = start_year,
      End_Year = end_year
    ))
  }
  
  # Save the organized GLS summary dataframe to a CSV file
  year_range <- paste(start_year, end_year, sep = "-")
  output_file <- file.path(output_directory, paste0("GLS_Summary_Results_with_Bonferroni_", variable, "_", year_range, ".csv"))
  write.csv(gls_summary_df, output_file, row.names = FALSE)
  
  # Optional: View the organized GLS summary dataframe
  return(gls_summary_df)
}

# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.rds",
  variable = "MonthlySICVariability",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis",
  start_year = 1979,
  end_year = 2024
)


```


**Deploy the function**

```{r Deploy the function}

# IceExtent_km:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds",
  variable = "IceExtent_km",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 1979,
  end_year = 2024
)

# IceExtent_km:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds",
  variable = "IceExtent_km",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 2010,
  end_year = 2024
)


# IceExtent_km:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds",
  variable = "MeanSIC",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 1979,
  end_year = 2024
)

# IceExtent_km:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds",
  variable = "MeanSIC",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 2010,
  end_year = 2024
)



#Daily SIC Variability 
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds",
  variable = "SICVariability",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 1979,
  end_year = 2024
)

#Daily SIC Variability 
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds",
  variable = "SICVariability",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 2010,
  end_year = 2024
)

###############
###############

# Monthly SIC Variability 
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.rds",
  variable = "MonthlySICVariability",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 2010,
  end_year = 2024
)

# Monthly SIC Variability 
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.rds",
  variable = "MonthlySICVariability",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 1979,
  end_year = 2024
)

# Monthly Extent Variability 
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.rds",
  variable = "MonthlyExtentVariability",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 2010,
  end_year = 2024
)

# Monthly Extent Variability 
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics_Monthly_Variability_Metrics.rds",
  variable = "MonthlyExtentVariability",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 1979,
  end_year = 2024
)

###############
###############

#Persistence
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds",
  variable = "Persistence",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 2010,
  end_year = 2024
)

#Persistence
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds",
  variable = "Persistence",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 1979,
  end_year = 2024
)

#Persistence SD
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds",
  variable = "Persistence_SD",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 2010,
  end_year = 2024
)

#Persistence SD
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds",
  variable = "Persistence_SD",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 1979,
  end_year = 2024
)

#Duration
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds",
  variable = "Mean_Monthly_Duration",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 2010,
  end_year = 2024
)

#Duration
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds",
  variable = "Mean_Monthly_Duration",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 1979,
  end_year = 2024
)

#Duration SD
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds",
  variable = "Duration_SD",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 2010,
  end_year = 2024
)

#Duration SD
# Example usage:
result <- gls_analysis_function(
  input_dataset = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds",
  variable = "Duration_SD",  # Specify the metric here
  output_directory = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends",
  start_year = 1979,
  end_year = 2024
)




```



**Walk though output csvs and pull significant results**

```{r Walk through csvs and pull significant results}
# Load necessary libraries
library(dplyr)

# Define the function to extract significant results from CSV files
extract_significant_results <- function(input_directory, output_file, significance_level = 0.05) {
  
  # List all CSV files in the directory
  csv_files <- list.files(path = input_directory, pattern = "\\.csv$", full.names = TRUE)
  
  # Initialize an empty dataframe to store significant results
  significant_results <- data.frame()
  
  # Loop through each CSV file
  for (file in csv_files) {
    # Read the CSV file
    data <- read.csv(file)
    
    # Filter rows with significant Adjusted_Time_pValue
    significant_data <- data %>%
      filter(Adjusted_Time_pValue < significance_level)
    
    # Append significant data to the results dataframe
    significant_results <- bind_rows(significant_results, significant_data)
  }
  
  # Save the combined significant results to a new CSV file
  write.csv(significant_results, output_file, row.names = FALSE)
  
  # Return the significant results dataframe
  return(significant_results)
}

# Usage example:
input_directory <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends"
output_file <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends/significant_results.csv"

# Extract significant results and save to a new CSV file
significant_results <- extract_significant_results(input_directory, output_file)

# View the significant results
print(significant_results)


```



**Walk through csvs and pull significant results - rename not working but most recent version**

```{r Walk through csvs and pull significant results - rename not working but most recent version}
# Load necessary libraries
library(dplyr)

# Define the function to extract significant results, calculate CIs, and save the results
extract_and_compile_significant_results <- function(input_directory, output_file, significance_level = 0.05) {
  
  # List all CSV files in the directory
  csv_files <- list.files(path = input_directory, pattern = "\\.csv$", full.names = TRUE)
  
  # Initialize an empty dataframe to store significant results
  significant_results <- data.frame()
  
  # Loop through each CSV file
  for (file in csv_files) {
    # Read the CSV file
    data <- read.csv(file)
    
    # Filter rows with significant Adjusted_Time_pValue
    significant_data <- data %>%
      filter(Adjusted_Time_pValue < significance_level)
    
    # Calculate confidence intervals for Intercept and Time estimates
    significant_data <- significant_data %>%
      mutate(
        Intercept_CI_Lower = Intercept_Estimate - 1.96 * Intercept_StdError,
        Intercept_CI_Upper = Intercept_Estimate + 1.96 * Intercept_StdError,
        Time_CI_Lower = Time_Estimate - 1.96 * Time_StdError,
        Time_CI_Upper = Time_Estimate + 1.96 * Time_StdError
      )
    
    # Append significant data to the results dataframe
    significant_results <- bind_rows(significant_results, significant_data)
  }
  
  # Save the compiled significant results to a new CSV file
  write.csv(significant_results, output_file, row.names = FALSE)
  
  # Return the significant results dataframe
  return(significant_results)
}

# Define the function to rename columns in the significant results CSV, but only if they exist
rename_columns_in_csv <- function(csv_file) {
  # Read the CSV file
  data <- read.csv(csv_file)
  
  # Explicitly rename columns
  colnames(data)[colnames(data) == "Mean_Monthly_Duration"] <- "Duration"
  colnames(data)[colnames(data) == "Duration_SD"] <- "Duration (Var.)"
  colnames(data)[colnames(data) == "IceExtent_km"] <- "Extent"
  colnames(data)[colnames(data) == "MonthlyExtentVariability"] <- "Extent (Var.)"
  colnames(data)[colnames(data) == "Persistence"] <- "Open Water Frequency"
  colnames(data)[colnames(data) == "Persistence_SD"] <- "Open Water Frequency (Var.)"
  colnames(data)[colnames(data) == "MeanSIC"] <- "SIC"
  colnames(data)[colnames(data) == "SICVariability"] <- "SIC (Var.)"
  
  # Save the renamed data back to the same CSV file
  write.csv(data, csv_file, row.names = FALSE)
  
  # Return the renamed dataframe
  return(data)
}

# Usage example:
input_directory <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends/2010-2024"
output_file <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Trends/2010-2024/significant_results_with_CI.csv"

# Step 1: Extract significant results, calculate CIs, and save to a new CSV file
significant_results <- extract_and_compile_significant_results(input_directory, output_file)

# Step 2: Rename columns in the compiled significant results CSV
renamed_results <- rename_columns_in_csv(output_file)

# View the renamed significant results
print(renamed_results)

```





**Southern Oscillation Index Analysis**

```{r Southern Oscillation Index Analysis}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(httr)
library(tidyr)
library(broom)
library(gridExtra)
library(RColorBrewer)

# Load your dataset with error handling
nsidc_metrics <- tryCatch({
  readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")
}, error = function(e) {
  stop("Error loading NSIDC metrics: ", e)
})

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_extent = mean(IceExtent_km, na.rm = TRUE), 
            sd_extent = sd(IceExtent_km, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Load SOI data with error handling
url <- "https://www.cpc.ncep.noaa.gov/data/indices/soi"
soi_raw <- tryCatch({
  GET(url)
}, error = function(e) {
  stop("Error fetching SOI data: ", e)
})

soi_text <- content(soi_raw, "text")

# Preprocess the SOI data
soi_lines <- strsplit(soi_text, "\n")[[1]]

# Identify the start of the standardized SOI data section
standardized_soi_start <- grep("STANDARDIZED    DATA", soi_lines)

# Extract standardized SOI data lines
soi_standardized_lines <- soi_lines[(standardized_soi_start + 3):length(soi_lines)]
soi_standardized_lines <- soi_standardized_lines[!grepl("-999.9", soi_standardized_lines)] # remove lines with invalid data

# Convert standardized SOI data to dataframe
soi_data <- read.table(text = soi_standardized_lines, fill = TRUE, stringsAsFactors = FALSE)
colnames(soi_data) <- c("Year", month.abb)
soi_data <- soi_data %>% mutate(Year = as.integer(Year))

# Convert to long format
soi_data_long <- soi_data %>%
  pivot_longer(-Year, names_to = "Month", values_to = "SOI") %>%
  mutate(Month = match(Month, month.abb), 
         Year = as.integer(Year),
         SOI = as.numeric(SOI)) %>%
  filter(!is.na(SOI))  # Remove rows with NA values in SOI

# Merge sea ice data with SOI data
merged_data <- merge(monthly_averages, soi_data_long, by = c("Year", "Month"))

# Remove duplicate rows
merged_data <- merged_data[!duplicated(merged_data), ]

# Calculate correlation for each region and month
correlation_results <- merged_data %>%
  group_by(Region, Month) %>%
  summarize(correlation = cor(mean_extent, SOI, use = "complete.obs"),
            p.value = cor.test(mean_extent, SOI)$p.value) %>%
  arrange(Region, Month)

# Print the correlation results
print(correlation_results)

# Visualize the correlation results using RColorBrewer color palette
ggplot(correlation_results, aes(x = factor(Month, levels = 6:9), y = correlation, fill = Region)) +
  geom_bar(stat = "identity", position = position_dodge(0.9)) +
  geom_text(aes(label = round(correlation, 2)), vjust = -1.5, position = position_dodge(0.9)) +
  scale_fill_brewer(palette = "Set3") +  # You can choose other palettes like "Paired", "Dark2", etc.
  labs(title = "Correlation between SOI and Sea Ice Extent by Region and Month",
       x = "Month", y = "Correlation") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )

# Linear regression analysis to quantify the relationship between SOI and sea ice extent
regression_results <- merged_data %>%
  group_by(Region, Month) %>%
  do(tidy(lm(mean_extent ~ SOI, data = .)))

# Print regression results
print(regression_results)

# Filter the regression results for the SOI term
soi_coefficients <- regression_results %>%
  filter(term == "SOI")

# Function to create a plot for each region
plot_soi_effect <- function(region_data) {
  ggplot(region_data, aes(x = factor(Month), y = estimate, color = factor(Month))) +
    geom_point(size = 3) +
    geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(
      title = paste("Effect of SOI on Sea Ice Extent:", unique(region_data$Region)),
      x = "Month",
      y = "SOI Coefficient Estimate",
      color = "Month"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5),
      legend.position = "none"
    )
}

# Apply the function for each region and plot
unique_regions <- unique(soi_coefficients$Region)
plots <- lapply(unique_regions, function(region) {
  region_data <- soi_coefficients %>% filter(Region == region)
  plot_soi_effect(region_data)
})

# Display the plots
do.call(grid.arrange, c(plots, ncol = 2))

# Create individual plots for each region and arrange them using facet_wrap
ggplot(soi_coefficients, aes(x = factor(Month), y = estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  facet_wrap(~ Region, scales = "free_y") +
  labs(
    title = "Effect of SOI on Sea Ice Extent Across Regions and Months",
    x = "Month",
    y = "SOI Coefficient Estimate"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )



```







**Spatial Visualization - Duration and Persistence + Trend analayis - Basic glm**


 
   Processing of Sea Ice Data

    Loading Data:
        The script loads the sea ice data files (raster stacks) and the study area shapefile.
        Each raster stack represents sea ice concentration for a specific year.

    Metric Calculation for Each Year:
        For each year in the range start_year to end_year, the script calculates the specified metrics (persistence, duration, concentration) for each cell in the raster stack.

    Persistence Calculation:
        Persistence: The number of days per year where sea ice concentration is above a threshold (0.15).
        This is calculated using a sum function over the raster stack for the year.

    Duration Calculation:
        Duration: The maximum consecutive days per year where sea ice concentration is above the threshold.
        This is calculated using a custom function that finds the longest run of days exceeding the threshold.

    Concentration Calculation:
        Concentration: The average sea ice concentration per year.
        This is calculated using a mean function over the raster stack for the year.

Applying Linear Models

    Time Series Data:
        After calculating the yearly metrics for each cell, the script builds a time series for each cell. This time series spans from start_year to end_year.

```{r}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(mean_sic = mean(MeanSIC, na.rm = TRUE), 
            sd_sic = sd(MeanSIC, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Mean_SIC",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/SIC-Mean_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

# Optional: View the organized GLS summary dataframe
gls_summary_df

```

**Analysis - Variability of Mean Monthly Sea Ice Concentration **

```{r Analysis - Variability of Mean Monthly Sea Ice Concentration }
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_sic_var  = mean(SICVariability, na.rm = TRUE), 
            mean_sic_var_sd = sd(SICVariability, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(mean_sic_var ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(mean_sic_var = mean(SICVariability, na.rm = TRUE), 
            mean_sic_var_sd = sd(SICVariability, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Mean_SIC_Variability",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/SIC-Variability_Mean_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

# Optional: View the organized GLS summary dataframe
gls_summary_df

```



**Analysis - Monthy/Annual Persistence**


```{r Analysis - Monthy/Annual Persistence}

# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_Persistence  = mean(Persistence, na.rm = TRUE), 
            Persistence_sd = sd(Persistence, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(mean_Persistence ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(mean_Persistence = mean(Persistence, na.rm = TRUE), 
            Persistence_sd = sd(Persistence, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Persistence",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

gls_summary_df

```


**Analysis - Monthly/Annual Persistence Varaibility **

```{r Analysis - Monthly/Annual Persistence Varaibility }
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(Persistence_SD  = mean(Persistence_SD, na.rm = TRUE), 
            Persistence_SD_sd = sd(Persistence_SD, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(Persistence_SD ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(Persistence_SD = mean(Persistence_SD, na.rm = TRUE), 
            Persistence_SD_sd = sd(Persistence_SD, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Persistence-Variability",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence-Variability_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

gls_summary_df


```



**Analaysis - Mean Monthly Duration**

```{r Analysis - Mean Monthly Duration}

# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(Mean_Monthly_Duration  = mean(Mean_Monthly_Duration, na.rm = TRUE), 
            Mean_Monthly_Duration_SD = sd(Mean_Monthly_Duration, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(Mean_Monthly_Duration ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(Mean_Monthly_Duration = mean(Mean_Monthly_Duration, na.rm = TRUE), 
            Mean_Monthly_Duration_sd = sd(Mean_Monthly_Duration, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Mean_Monthly_Duration",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Mean_Monthly_Duration_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

gls_summary_df

```


**Analysis - Variability of Mean Monthly Duration**

```{r Analysis - Variability of Mean Monthly Duration}

# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(Duration_SD  = mean(Duration_SD, na.rm = TRUE), 
            Duration_SD_sd = sd(Duration_SD, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(Duration_SD ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Run GLS model for annual (June-September) metrics
annual_metrics <- winter_metrics %>%
  group_by(Year, Region) %>%
  summarise(Duration_SD = mean(Duration_SD, na.rm = TRUE), 
            Duration_SD_sd = sd(Duration_SD, na.rm = TRUE), .groups = 'drop') %>%
  ungroup() %>%
  mutate(Time = as.numeric(Year))

for (region in regions) {
  region_annual_data <- annual_metrics %>% filter(Region == region)
  if (nrow(region_annual_data) > 1) {
    gls_results[[paste(region, "Annual", sep = "_")]] <- fit_gls_model(region_annual_data)
  }
}

# Initialize a dataframe to store the GLS results
gls_summary_df <- data.frame(
  Region = character(),
  Metric = character(),
  Month = character(),
  Intercept_Estimate = numeric(),
  Intercept_StdError = numeric(),
  Intercept_tValue = numeric(),
  Intercept_pValue = numeric(),
  Time_Estimate = numeric(),
  Time_StdError = numeric(),
  Time_tValue = numeric(),
  Time_pValue = numeric(),
  Adjusted_Time_pValue = numeric(),
  AIC = numeric(),
  BIC = numeric(),
  LogLik = numeric(),
  Residual_StdError = numeric(),
  DF_Residual = numeric(),
  stringsAsFactors = FALSE
)

# Extract and organize GLS model results
for (key in names(gls_results)) {
  model <- gls_results[[key]]
  summary_model <- summary(model)
  
  # Extract coefficients
  coefficients <- summary_model$tTable
  intercept_estimate <- coefficients["(Intercept)", "Value"]
  intercept_std_error <- coefficients["(Intercept)", "Std.Error"]
  intercept_t_value <- coefficients["(Intercept)", "t-value"]
  intercept_p_value <- coefficients["(Intercept)", "p-value"]
  
  time_estimate <- coefficients["Time", "Value"]
  time_std_error <- coefficients["Time", "Std.Error"]
  time_t_value <- coefficients["Time", "t-value"]
  time_p_value <- coefficients["Time", "p-value"]
  
  # Bonferroni correction for multiple comparisons
  num_tests <- length(gls_results)
  adjusted_time_p_value <- p.adjust(time_p_value, method = "bonferroni", n = num_tests)
  
  # Extract other statistics
  aic <- AIC(model)
  bic <- BIC(model)
  log_lik <- logLik(model)
  residual_std_error <- summary_model$sigma
  df_residual <- summary_model$dims$N - summary_model$dims$p
  
  # Split key to get region and month
  key_split <- unlist(strsplit(key, "_"))
  region <- key_split[1]
  month <- ifelse(length(key_split) > 1, key_split[2], "Annual")
  
  # Append to dataframe
  gls_summary_df <- rbind(gls_summary_df, data.frame(
    Region = region,
    Metric = "Duration-Variability",
    Month = month,
    Intercept_Estimate = intercept_estimate,
    Intercept_StdError = intercept_std_error,
    Intercept_tValue = intercept_t_value,
    Intercept_pValue = intercept_p_value,
    Time_Estimate = time_estimate,
    Time_StdError = time_std_error,
    Time_tValue = time_t_value,
    Time_pValue = time_p_value,
    Adjusted_Time_pValue = adjusted_time_p_value,
    AIC = aic,
    BIC = bic,
    LogLik = log_lik,
    Residual_StdError = residual_std_error,
    DF_Residual = df_residual
  ))
}

# Save the organized GLS summary dataframe to a CSV file
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Duration-Variability_Summary_Results_with_Bonferroni.csv", row.names = FALSE)

gls_summary_df

```









**Raster Visualization Duration + Persistence + SD  Analysis**

```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(nlme)
library(ggplot2)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence_Duration_Metrics.rds")

# Rename the region "nsidc_12_5km_harmonized_1979-2023" to "Complete Study Area"
nsidc_metrics <- nsidc_metrics %>%
  mutate(Region = ifelse(Region == "nsidc_12_5km_harmonized_1979-2023", "Complete Study Area", Region))

# Rename columns
nsidc_metrics <- nsidc_metrics %>%
  rename(
    Mean_Monthly_Persistence = Persistence,
    Mean_Monthly_Duration = Mean_Monthly_Duration,
    Persistence_SD = Persistence_SD,
    Duration_SD = Duration_SD
  )

# If Date is not in Date format, convert it
if (!inherits(nsidc_metrics$Date, "Date")) {
  nsidc_metrics <- nsidc_metrics %>%
    mutate(Date = as.Date(Date, format = "%Y-%m-%d"))
}

# Filter for winter months (June - September) and add Year, Month, and Day columns
nsidc_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Create a time variable for trend analysis
nsidc_metrics <- nsidc_metrics %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12)

# Function to fit a GLS model for a given region and metric
fit_gls_model <- function(region_data, response_var) {
  formula <- as.formula(paste(response_var, "~ Time"))
  gls_model <- gls(formula, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Function to apply the GLS model and store results
apply_gls_model <- function(data, regions, metrics, period_label) {
  gls_results <- list()
  for (region in regions) {
    for (metric in metrics) {
      region_data <- data %>% filter(Region == region)
      if (nrow(region_data) > 1) {
        gls_results[[paste(region, metric, period_label, sep = "_")]] <- fit_gls_model(region_data, metric)
      }
    }
  }
  return(gls_results)
}

# Define regions and metrics
regions <- unique(nsidc_metrics$Region)
metrics <- c("Mean_Monthly_Persistence", "Persistence_SD", "Mean_Monthly_Duration", "Duration_SD")

# Apply the model for the winter period (June-September)
winter_gls_results <- apply_gls_model(nsidc_metrics, regions, metrics, "Winter")

# Apply the model for each month separately
monthly_gls_results <- list()
for (month in 6:9) {
  monthly_data <- nsidc_metrics %>% filter(Month == month)
  month_label <- month.name[month]
  monthly_gls_results[[month_label]] <- apply_gls_model(monthly_data, regions, metrics, month_label)
}

# Combine results
all_gls_results <- c(winter_gls_results, unlist(monthly_gls_results, recursive = FALSE))

# Function to extract summary information from GLS model
extract_gls_summary <- function(model, region, metric, period) {
  summary_model <- summary(model)
  coef_table <- summary_model$tTable
  intercept <- coef_table[1, ]
  time_coef <- coef_table[2, ]
  
  return(data.frame(
    Region = region,
    Metric = metric,
    Period = period,
    Intercept_Estimate = intercept["Value"],
    Intercept_StdError = intercept["Std.Error"],
    Intercept_tValue = intercept["t-value"],
    Intercept_pValue = intercept["p-value"],
    Time_Estimate = time_coef["Value"],
    Time_StdError = time_coef["Std.Error"],
    Time_tValue = time_coef["t-value"],
    Time_pValue = time_coef["p-value"],
    AIC = AIC(model),
    BIC = BIC(model),
    LogLik = logLik(model)[1],
    Residual_StdError = summary_model$sigma,
    DF_Residual = summary_model$dims$N - summary_model$dims$p
  ))
}

# Extract summaries for all models
gls_summary_df <- do.call(rbind, lapply(names(all_gls_results), function(key) {
  parts <- unlist(strsplit(key, "_"))
  region <- parts[1]
  metric <- parts[2]
  period <- parts[3]
  model <- all_gls_results[[key]]
  extract_gls_summary(model, region, metric, period)
}))

# Save the summary dataframe
write.csv(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence-and-Duration-GLS_Model_Summaries.csv", row.names = FALSE)
saveRDS(gls_summary_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Persistence-and-Duration-GLS_Model_Summaries.rds")

# Print the first few rows of the summary dataframe
print(head(gls_summary_df))



```





**Southern Oscillation Index Analysis**

```{r Analysis - Southern Oscillation Index}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(httr)
library(tidyr)
library(broom)
library(gridExtra)
library(RColorBrewer)

# Load your dataset with error handling
nsidc_metrics <- tryCatch({
  readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")
}, error = function(e) {
  stop("Error loading NSIDC metrics: ", e)
})

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_extent = mean(IceExtent_km, na.rm = TRUE), 
            sd_extent = sd(IceExtent_km, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Load SOI data with error handling
url <- "https://www.cpc.ncep.noaa.gov/data/indices/soi"
soi_raw <- tryCatch({
  GET(url)
}, error = function(e) {
  stop("Error fetching SOI data: ", e)
})

soi_text <- content(soi_raw, "text")

# Preprocess the SOI data
soi_lines <- strsplit(soi_text, "\n")[[1]]

# Identify the start of the standardized SOI data section
standardized_soi_start <- grep("STANDARDIZED    DATA", soi_lines)

# Extract standardized SOI data lines
soi_standardized_lines <- soi_lines[(standardized_soi_start + 3):length(soi_lines)]
soi_standardized_lines <- soi_standardized_lines[!grepl("-999.9", soi_standardized_lines)] # remove lines with invalid data

# Convert standardized SOI data to dataframe
soi_data <- read.table(text = soi_standardized_lines, fill = TRUE, stringsAsFactors = FALSE)
colnames(soi_data) <- c("Year", month.abb)
soi_data <- soi_data %>% mutate(Year = as.integer(Year))

# Convert to long format
soi_data_long <- soi_data %>%
  pivot_longer(-Year, names_to = "Month", values_to = "SOI") %>%
  mutate(Month = match(Month, month.abb), 
         Year = as.integer(Year),
         SOI = as.numeric(SOI)) %>%
  filter(!is.na(SOI))  # Remove rows with NA values in SOI

# Merge sea ice data with SOI data
merged_data <- merge(monthly_averages, soi_data_long, by = c("Year", "Month"))

# Remove duplicate rows
merged_data <- merged_data[!duplicated(merged_data), ]

# Calculate correlation with confidence intervals for each region and month
correlation_results <- merged_data %>%
  group_by(Region, Month) %>%
  summarise(
    correlation = cor(mean_extent, SOI, use = "complete.obs"),
    cor_test = list(cor.test(mean_extent, SOI)),
    p.value = cor_test[[1]]$p.value,
    conf.low = cor_test[[1]]$conf.int[1],  # Lower bound of the CI
    conf.high = cor_test[[1]]$conf.int[2]  # Upper bound of the CI
  ) %>%
  ungroup() %>%
  arrange(Region, Month)

# Print the correlation results with confidence intervals
print(correlation_results)

# Visualize the correlation results with confidence intervals
ggplot(correlation_results, aes(x = factor(Month, levels = 6:9), y = correlation, fill = Region)) +
  geom_bar(stat = "identity", position = position_dodge(0.9)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, position = position_dodge(0.9)) +  # Adding CIs
  geom_text(aes(label = round(correlation, 2)), vjust = -1.5, position = position_dodge(0.9)) +
  scale_fill_brewer(palette = "Set3") +
  labs(
    title = "Correlation between SOI and Sea Ice Extent by Region and Month",
    x = "Month",
    y = "Correlation"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )



```


**Visualize Seasonal Sea Ice Trends - Direction + Strength
```{r Visualize Seasonal Sea Ice Trends - Direction + Strength}
# Load necessary libraries
library(ggplot2)
library(RColorBrewer)

# Load the data
data <- read.csv("D:/Manuscripts_localData/FrostBound_AQ/RStudioProject/Scripts/Sea-Ice-Patterns-Analyses/sea_ice_trends_table4.csv")

# View the first few rows to confirm it loaded correctly
head(data)

# Check the column names
colnames(data)

# Z-Score Standardization of Slopes within each Metric to preserve direction and magnitude
data$Standardized_Slope <- ave(data$`Slope..Time.`, data$Metric, 
                               FUN = function(x) scale(x))

# Reorder the regions on the y-axis
data$Region <- factor(data$Region, levels = c("Complete Area", "Offshore", "Northern Shelf", "Middle Shelf", "Southern Shelf"))

# Ensure related metrics are ordered correctly
data$Metric <- factor(data$Metric, levels = c("Extent", "Extent Variability", 
                                              "SIC", "SIC Variability", 
                                              "Duration", "Duration Variability", 
                                              "Open Water Frequency", "Open Water Frequency Var."))

# Create the heatmap with faceting, standardized slopes, increased spacing, and custom RdBu gradient
ggplot(data, aes(x = Month, y = Region, fill = Standardized_Slope)) +
  geom_tile(color = "white") +
  scale_fill_gradientn(colors = colorRampPalette(brewer.pal(3, "RdBu"))(256), 
                       name = "Standardized Slope (Trend Strength & Direction)") +
  labs(title = "Seasonal Trend Directions and Strength by Region ", fill = "Standardized Slope") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.spacing = unit(2, "lines")) +  # Increase spacing between columns
  facet_wrap(~ Metric, ncol = 2)


```



**Duration and Persistence + Trend analayis - Basic glm**


 
   Processing of Sea Ice Data

    Loading Data:
        The script loads the sea ice data files (raster stacks) and the study area shapefile.
        Each raster stack represents sea ice concentration for a specific year.

    Metric Calculation for Each Year:
        For each year in the range start_year to end_year, the script calculates the specified metrics (persistence, duration, concentration) for each cell in the raster stack.

    Persistence Calculation:
        Persistence: The number of days per year where sea ice concentration is above a threshold (0.15).
        This is calculated using a sum function over the raster stack for the year.

    Duration Calculation:
        Duration: The maximum consecutive days per year where sea ice concentration is above the threshold.
        This is calculated using a custom function that finds the longest run of days exceeding the threshold.

    Concentration Calculation:
        Concentration: The average sea ice concentration per year.
        This is calculated using a mean function over the raster stack for the year.

Applying Linear Models

    Time Series Data:
        After calculating the yearly metrics for each cell, the script builds a time series for each cell. This time series spans from start_year to end_year.


    Linear Model Fitting:
        For each cell, the script fits a linear model (lm) to the time series of yearly metric values.
        The response variable (y) is the metric value for each year.
        The predictor variable (years) is the sequence of years.
        
        

```{r }
# Load necessary libraries
library(terra)
library(dplyr)
library(lubridate)
library(nlme)
library(ggplot2)
library(grDevices)  # For colorRampPalette
library(gridExtra)  # For arranging plots side by side

# Define the function
analyze_sea_ice <- function(start_year, end_year, use_mask = TRUE, report_metrics = c("concentration", "duration", "persistence")) {
  
  # Define file paths
  chunk_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif"
  chunk_files <- list.files(chunk_dir, pattern = "\\.tif$", full.names = TRUE)
  study_area_shapefile <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
  
  # Load the study area shapefile
  if (use_mask) {
    study_area <- vect(study_area_shapefile)
  }
  
  # Define the threshold for sea ice concentration
  ice_threshold <- .15
  
  # Function to calculate consecutive days above threshold
  calculate_duration <- function(x, threshold) {
    if (all(is.na(x))) return(NA)
    rle_result <- rle(x > threshold)
    max_duration <- ifelse(any(rle_result$values), max(rle_result$lengths[rle_result$values]), 0)
    # print(paste0("Max Duration: ", max_duration))
    return(max_duration)
  }
  
  # Simplified function to process each chunk file using app
  process_chunk <- function(file_path) {
    region_name <- gsub(".*/|\\.tif$", "", file_path)
    message(paste("Processing file:", file_path))
    
    # Load the NSIDC sea ice concentration data for the subregion
    nsidc <- rast(file_path)
    
    annual_persistence <- list()
    annual_duration <- list()
    annual_concentration <- list()
    
    for (year in start_year:end_year) {
      message(paste("Processing year:", year))
      start_date <- as.Date(paste0(year, "-01-01"))
      end_date <- as.Date(paste0(year, "-12-31"))
      
      # Filter the sea ice data for the current year
      nsidc_year <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))
      
      if (nlyr(nsidc_year) == 0) {
        message(paste("No data for year:", year))
        next
      }
      
      # Calculate metrics based on options
      if ("persistence" %in% report_metrics) {
        persistence <- app(nsidc_year, function(x) sum(ifelse(x < ice_threshold, 0, 1), na.rm = FALSE))
        print(paste0("Persistence: ", persistence ))
        # summary(persistence[[1]])
        names(persistence) <- paste0("Persistence_", year)
        annual_persistence[[as.character(year)]] <- persistence
        print(paste0("Annaul peristence:" , annual_persistence))
      }

      
      if ("duration" %in% report_metrics) {
        duration <- app(nsidc_year, function(x) calculate_duration(x, ice_threshold))
        names(duration) <- paste0("Duration_", year)
        annual_duration[[as.character(year)]] <- duration
      }
      
      if ("concentration" %in% report_metrics) {
        concentration <- app(nsidc_year, function(x) mean(x, na.rm = TRUE))
        str(concentration)
        names(concentration) <- paste0("Concentration_", year)
        annual_concentration[[as.character(year)]] <- concentration
      }
    }
    
    message(paste("Finished processing file:", file_path))
    return(list(persistence = annual_persistence, duration = annual_duration, concentration = annual_concentration))
  }
  
  # Process each chunk file sequentially
  results <- lapply(chunk_files, process_chunk)
  
  # Combine all annual rasters into a single stack for the selected metrics
  combine_rasters <- function(raster_list) {
    raster_stack <- rast()
    for (year in names(raster_list)) {
      if (!is.null(raster_list[[year]])) {
        raster_stack <- c(raster_stack, raster_list[[year]])
      }
    }
    return(raster_stack)
  }
  
  persistence_stack <- NULL
  duration_stack <- NULL
  concentration_stack <- NULL
  
  if ("persistence" %in% report_metrics) {
    persistence_stack <- combine_rasters(do.call(c, lapply(results, `[[`, "persistence")))
    if (use_mask) persistence_stack <- mask(persistence_stack, study_area)
  }
  
  if ("duration" %in% report_metrics) {
    duration_stack <- combine_rasters(do.call(c, lapply(results, `[[`, "duration")))
    if (use_mask) duration_stack <- mask(duration_stack, study_area)
  }
  
  if ("concentration" %in% report_metrics) {
    concentration_stack <- combine_rasters(do.call(c, lapply(results, `[[`, "concentration")))
    if (use_mask) concentration_stack <- mask(concentration_stack, study_area)
  }
  
  # Function to apply linear model to each pixel and return slope and p-value
  apply_lm <- function(y) {
    if (all(is.na(y))) {
      return(c(NA, NA))
    } else {
      years <- start_year:end_year
      fit <- tryCatch(lm(y ~ years), error = function(e) return(NULL))
      if (is.null(fit)) {
        return(c(NA, NA))
      } else {
        slope <- coef(fit)[2]  # Slope of the linear model
        p_value <- summary(fit)$coefficients[2, 4]  # p-value for the slope
        return(c(slope, p_value))
      }
    }
  }
  
  persistence_trend <- NULL
  duration_trend <- NULL
  concentration_trend <- NULL
  
  if (!is.null(persistence_stack)) persistence_trend <- app(persistence_stack, apply_lm)
  if (!is.null(duration_stack)) duration_trend <- app(duration_stack, apply_lm)
  if (!is.null(concentration_stack)) concentration_trend <- app(concentration_stack, apply_lm)
  
  # Extract slopes and p-values from the results
  persistence_slope <- NULL
  persistence_pvalue <- NULL
  duration_slope <- NULL
  duration_pvalue <- NULL
  concentration_slope <- NULL
  concentration_pvalue <- NULL
  
  if (!is.null(persistence_trend)) {
    persistence_slope <- persistence_trend[[1]]
    persistence_pvalue <- persistence_trend[[2]]
  }
  
  if (!is.null(duration_trend)) {
    duration_slope <- duration_trend[[1]]
    duration_pvalue <- duration_trend[[2]]
  }
  
  if (!is.null(concentration_trend)) {
    concentration_slope <- concentration_trend[[1]]
    concentration_pvalue <- concentration_trend[[2]]
  }
  
  # Verify the integrity of the data
  if (!is.null(persistence_slope)) {
    print("Summary of persistence slope layer:")
    print(summary(persistence_slope))
    print("Summary of persistence p-value layer:")
    print(summary(persistence_pvalue))
  }
  
  if (!is.null(duration_slope)) {
    print("Summary of duration slope layer:")
    print(summary(duration_slope))
    print("Summary of duration p-value layer:")
    print(summary(duration_pvalue))
  }
  
  if (!is.null(concentration_slope)) {
    print("Summary of concentration slope layer:")
    print(summary(concentration_slope))
    print("Summary of concentration p-value layer:")
    print(summary(concentration_pvalue))
  }
  
  # Define a significance level
  alpha <- 0.05
  
  # Calculate statistics for persistence
  if (!is.null(persistence_slope)) {
    persistence_values <- values(persistence_slope)
    persistence_pvalues <- values(persistence_pvalue)
    persistence_data <- cbind(persistence_values, persistence_pvalues)
    persistence_data <- persistence_data[complete.cases(persistence_data), ]
    persistence_values <- persistence_data[, 1]
    persistence_pvalues <- persistence_data[, 2]
    persistence_mean <- mean(persistence_values)
    persistence_median <- median(persistence_values)
    persistence_min <- min(persistence_values)
    persistence_max <- max(persistence_values)
    persistence_significant <- sum(persistence_pvalues <= alpha) / length(persistence_pvalues) * 100
    persistence_positive <- sum(persistence_pvalues <= alpha & persistence_values > 0) / length(persistence_pvalues) * 100
    persistence_negative <- sum(persistence_pvalues <= alpha & persistence_values < 0) / length(persistence_pvalues) * 100
    persistence_nonsignificant <- 100 - persistence_significant
  }
  
  # Calculate statistics for duration
  if (!is.null(duration_slope)) {
    duration_values <- values(duration_slope)
    duration_pvalues <- values(duration_pvalue)
    duration_data <- cbind(duration_values, duration_pvalues)
    duration_data <- duration_data[complete.cases(duration_data), ]
    duration_values <- duration_data[, 1]
    duration_pvalues <- duration_data[, 2]
    duration_mean <- mean(duration_values)
    duration_median <- median(duration_values)
    duration_min <- min(duration_values)
    duration_max <- max(duration_values)
    duration_significant <- sum(duration_pvalues <= alpha) / length(duration_pvalues) * 100
    duration_positive <- sum(duration_pvalues <= alpha & duration_values > 0) / length(duration_pvalues) * 100
    duration_negative <- sum(duration_pvalues <= alpha & duration_values < 0) / length(duration_pvalues) * 100
    duration_nonsignificant <- 100 - duration_significant
  }
  
  # Calculate statistics for concentration
  if (!is.null(concentration_slope)) {
    concentration_values <- values(concentration_slope)
    concentration_pvalues <- values(concentration_pvalue)
    concentration_data <- cbind(concentration_values, concentration_pvalues)
    concentration_data <- concentration_data[complete.cases(concentration_data), ]
    concentration_values <- concentration_data[, 1]
    concentration_pvalues <- concentration_data[, 2]
    concentration_mean <- mean(concentration_values)
    concentration_median <- median(concentration_values)
    concentration_min <- min(concentration_values)
    concentration_max <- max(concentration_values)
    concentration_significant <- sum(concentration_pvalues <= alpha) / length(concentration_pvalues) * 100
    concentration_positive <- sum(concentration_pvalues <= alpha & concentration_values > 0) / length(concentration_pvalues) * 100
    concentration_negative <- sum(concentration_pvalues <= alpha & concentration_values < 0) / length(concentration_pvalues) * 100
    concentration_nonsignificant <- 100 - concentration_significant
  }
  
  # Print summary statistics for reporting
  if (!is.null(persistence_slope)) {
    persistence_summary <- data.frame(
      Metric = c("Mean slope", "Median slope", "Minimum slope", "Maximum slope", "% Significant", "% Positive", "% Negative", "% Non-significant"),
      Persistence = c(persistence_mean, persistence_median, persistence_min, persistence_max, persistence_significant, persistence_positive, persistence_negative, persistence_nonsignificant)
    )
    print(persistence_summary)
  }
  
  if (!is.null(duration_slope)) {
    duration_summary <- data.frame(
      Metric = c("Mean slope", "Median slope", "Minimum slope", "Maximum slope", "% Significant", "% Positive", "% Negative", "% Non-significant"),
      Duration = c(duration_mean, duration_median, duration_min, duration_max, duration_significant, duration_positive, duration_negative, duration_nonsignificant)
    )
    print(duration_summary)
  }
  
  if (!is.null(concentration_slope)) {
    concentration_summary <- data.frame(
      Metric = c("Mean slope", "Median slope", "Minimum slope", "Maximum slope", "% Significant", "% Positive", "% Negative", "% Non-significant"),
      Concentration = c(concentration_mean, concentration_median, concentration_min, concentration_max, concentration_significant, concentration_positive, concentration_negative, concentration_nonsignificant)
    )
    print(concentration_summary)
  }
  
  # Define a color palette for trends
  trend_colors <- colorRampPalette(c("red", "white", "blue"))
  
  # Set up multi-panel plotting environment
  par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))
  
  if (!is.null(persistence_slope)) {
    plot(persistence_slope, col=trend_colors(100), main="Trend of Persistence Over Time", legend=TRUE, colNA="black")
  }
  
  if (!is.null(duration_slope)) {
    plot(duration_slope, col=trend_colors(100), main="Trend of Duration Over Time", legend=TRUE, colNA="black")
  }
  
  if (!is.null(concentration_slope)) {
    plot(concentration_slope, col=trend_colors(100), main="Trend of Concentration Over Time", legend=TRUE, colNA="black")
  }
  
  # Classify the trends based on significance and directionality using ifel function
  if (!is.null(persistence_pvalue)) {
    significant_cells <- persistence_pvalue <= alpha
    significant_mask <- ifel(significant_cells, 1, NA)
    positive_trend <- ifel(significant_mask == 1 & persistence_slope > 0, 1, NA)
    negative_trend <- ifel(significant_mask == 1 & persistence_slope < 0, 2, NA)
    na_mask <- ifel(is.na(persistence_slope), 3, NA)
    combined_trend <- positive_trend
    combined_trend[!is.na(negative_trend)] <- 2
    combined_trend[!is.na(na_mask)] <- 3
    color_table <- data.frame(value = c(1, 2, 3), col = c("blue", "red", "black"))
    coltab(combined_trend) <- color_table
    plot(combined_trend, main="Significance and Directionality of Persistence Trends")
    legend("topright", legend=c("Positive Trend", "Negative Trend", "NA Cells"), fill=color_table$col)
  }
  
  if (!is.null(duration_pvalue)) {
    significant_cells <- duration_pvalue <= alpha
    significant_mask <- ifel(significant_cells, 1, NA)
    positive_trend <- ifel(significant_mask == 1 & duration_slope > 0, 1, NA)
    negative_trend <- ifel(significant_mask == 1 & duration_slope < 0, 2, NA)
    na_mask <- ifel(is.na(duration_slope), 3, NA)
    combined_trend <- positive_trend
    combined_trend[!is.na(negative_trend)] <- 2
    combined_trend[!is.na(na_mask)] <- 3
    color_table <- data.frame(value = c(1, 2, 3), col = c("blue", "red", "black"))
    coltab(combined_trend) <- color_table
    plot(combined_trend, main="Significance and Directionality of Duration Trends")
    legend("topright", legend=c("Positive Trend", "Negative Trend", "NA Cells"), fill=color_table$col)
  }
  
  if (!is.null(concentration_pvalue)) {
    significant_cells <- concentration_pvalue <= alpha
    significant_mask <- ifel(significant_cells, 1, NA)
    positive_trend <- ifel(significant_mask == 1 & concentration_slope > 0, 1, NA)
    negative_trend <- ifel(significant_mask == 1 & concentration_slope < 0, 2, NA)
    na_mask <- ifel(is.na(concentration_slope), 3, NA)
    combined_trend <- positive_trend
    combined_trend[!is.na(negative_trend)] <- 2
    combined_trend[!is.na(na_mask)] <- 3
    color_table <- data.frame(value = c(1, 2, 3), col = c("blue", "red", "black"))
    coltab(combined_trend) <- color_table
    plot(combined_trend, main="Significance and Directionality of Concentration Trends")
    legend("topright", legend=c("Positive Trend", "Negative Trend", "NA Cells"), fill=color_table$col)
  }
}

# Example usage
analyze_sea_ice(start_year = 1981, end_year = 2023, use_mask = FALSE, report_metrics = c("concentration", "duration", "persistence"))

```




**Trend Visualization - Heatmap**

```{r Trend Visualization - Heatmap}
#!/usr/bin/env Rscript
#Note: Output this as a PDF file using cairo + portrait mode
#!/usr/bin/env Rscript

library(dplyr)
library(ggplot2)
library(scales)
library(readr)

# Load the data using the new file path
data <- read_csv("C:/Users/michael.wethington.BRILOON/OneDrive - Biodiversity Research Institute/Documents/Manuscripts - Antarctica/MEPS_Intial Manuscript Submission/Revision/Tables/Sea-Ice-Trends-Significant-Results.csv")

# Remove missing rows
data <- na.omit(data)

# Ensure the Metric and Region columns are correctly ordered and labeled
data$Metric <- factor(data$Metric, levels = c("Extent", "Extent (Var.)", 
                                              "SIC", "SIC (Var.)", 
                                              "Duration", "Duration (Var.)", 
                                              "Open Water Frequency", "Open Water Frequency (Var.)"))
data$Region <- factor(data$Region, levels = c("Complete Study Area", "Offshore", 
                                              "Northern Shelf", "Middle Shelf", "Southern Shelf"))

# Replace "Annual" with "Winter" in the Month column, then relabel
data$Month <- as.character(data$Month)
data$Month[data$Month == "Annual"] <- "Winter"
month_labels <- c("6" = "June", "7" = "July", "8" = "August", "9" = "September", "Winter" = "Winter")
data$Month <- factor(data$Month, levels = names(month_labels), labels = month_labels)

# Function to scale values while preserving the sign
relative_scale <- function(x) {
  min_x <- min(x)
  max_x <- max(x)
  if (min_x == max_x) {
    return(rep(0, length(x)))
  } else {
    scaled_x <- rescale(abs(x), to = c(0, 1))
    return(scaled_x * sign(x))
  }
}

# Apply the relative scaling function (using the correct column name "Time.Estimate")
data$Relative_Scaled_Slope <- ave(data$`Time Estimate`, data$Metric, FUN = relative_scale)

# Create a significance indicator (p <= 0.05)
data <- data %>% mutate(Significant = (`p-Value` <= 0.05))

# Filter rows: include those with significance or with a nonzero trend when rounded
data_filtered <- data %>% filter(Significant | abs(round(Relative_Scaled_Slope, 2)) > 0)
data_filtered <- data_filtered %>% filter(Relative_Scaled_Slope != 0)

# Create a label that displays only the direction:
data_filtered <- data_filtered %>% 
  mutate(label = ifelse(Relative_Scaled_Slope > 0, "+", ifelse(Relative_Scaled_Slope < 0, "–", "")))

# Create the heatmap using the filtered data.
heatmap_plot <- ggplot(data_filtered, aes(x = Month, y = Region, fill = Relative_Scaled_Slope)) +
  geom_tile(color = "white") +
  # Overlay black borders around all significant cells
  geom_tile(data = filter(data_filtered, Significant), fill = NA, color = "black", size = 0.2) +
  # Overlay text labels indicating trend direction
  geom_text(aes(label = label), size = 3, color = "black") +
  scale_fill_gradientn(colors = c("#E46726", "white", "#6D9EC1"), 
                       name = "Trend Strength & Direction",
                       limits = c(-1, 1),
                       oob = squish) +
  labs(title = "Seasonal Trend Directions and Strength by Region",
       fill = "Relative Scaled Slope") +
  theme_minimal(base_size = 13) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.spacing = unit(2, "lines"),
        plot.margin = margin(10, 10, 10, 10)) +
  facet_wrap(~ Metric, ncol = 2) +
  coord_fixed(ratio = 1, clip = "off")  # Prevent clipping of text that extends into margins

# Display the heatmap
print(heatmap_plot)

# Save the plot to PDF using the cairo_pdf device for better rendering of text
ggsave(filename = "heatmap_plot.pdf", plot = heatmap_plot, width = 10, height = 8, device = cairo_pdf)


```



