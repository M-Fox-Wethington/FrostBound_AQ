---
title: "Multidecadal Sea Ice Metrics Trend Analysis - Harmonized 12.5 km Sea Ice Index Dataset"
author: "Michael Wethington"
date: "2024-05-14"
output: html_document
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
library(terra)
library(dplyr)
library(lubridate)
```




**Calculate Sea Ice Extent and Concentration Dataframes**

```{r Calculate Regional Ice Extent Metrics}

# Load required libraries
library(terra)
library(dplyr)
library(lubridate)

# Function to calculate mean SIC and sea ice extent for a given raster stack
calculate_stack_stats <- function(file_path) {
  # Load the raster stack from file
  raster_stack <- rast(file_path)
  
  # Extract dates from the raster stack
  layer_dates <- as.numeric(time(raster_stack))
  
  # Debugging: Print the extracted dates
  cat("Extracted dates from raster stack (raw):", layer_dates, "\n")
  
  # Convert numeric dates to Date type
  layer_dates <- as.Date(layer_dates, origin = "1970-01-01")
  
  # Debugging: Print the extracted dates after conversion
  cat("Extracted dates from raster stack (converted):", layer_dates, "\n")
  
  # Print the structure of the raster stack for debugging
  cat("Structure of raster stack for file:", file_path, "\n")
  print(raster_stack)
  
  # Apply function to set all values < 0.15 to 0
  raster_stack <- app(raster_stack, fun = function(x) { x[x < 0.15] <- 0; return(x) })
  
  # Calculate mean SIC excluding NAs for each layer
  mean_sic <- global(raster_stack, fun = 'mean', na.rm = TRUE)[, 1]
  
  # Calculate the area of one cell in square meters (only once)
  cell_area_sq_meters <- prod(res(raster_stack[[1]]))
  
  # Handle sea ice extent: count cells >= 0.15 for each layer
  valid_ice_cells <- global(raster_stack >= 0.15, fun = 'sum', na.rm = TRUE)[, 1]
  total_ice_area_sq_km <- (valid_ice_cells * cell_area_sq_meters) / 1e6  # Convert total ice area to square kilometers
  
  # Extract region name from the file path
  region_name <- tools::file_path_sans_ext(basename(file_path))
  region_name <- sub("NSIDC_25km_Harmonized_", "", region_name)  # Adjust based on actual pattern in filenames
  
  return(data.frame(date = layer_dates, mean_sic = mean_sic, ice_extent_km = total_ice_area_sq_km, region = region_name))
}

# Directory containing the .tif files
chunk_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif"

# List all .tif files for each region
chunk_files <- list.files(chunk_dir, pattern = "\\.tif$", full.names = TRUE)

# Initialize an empty dataframe to store the combined results
master_df <- data.frame()

# Iterate through each .tif file and combine the results into master_df
for (file in chunk_files) {
  cat("Processing file:", file, "\n")
  
  # Run the calculate_stack_stats function on the current file
  result <- calculate_stack_stats(file)
  
  # Combine the result into the master dataframe
  master_df <- bind_rows(master_df, result)
}


# Rename the region "nsidc_12_5km_harmonized_1979-2023" to "Complete Study Area"
master_df <- master_df %>%
  mutate(region = ifelse(region == "nsidc_12_5km_harmonized_1979-2023", "Complete Study Area", region))


# Rename columns
master_df <- master_df %>%
  rename(Date = date, Region = region, IceExtent_km = ice_extent_km, MeanSIC = mean_sic)



# If Date is not in Date format, convert it
if (!inherits(master_df$Date, "Date")) {
  master_df <- master_df %>%
    mutate(Date = as.Date(Date, format = "%Y-%m-%d"))
}


# Filter for winter months (June - September) and add Year, Month, and Day columns
master_df <- master_df %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# 
# # Save the master dataframe to a CSV file (optional)
write.csv(master_df, file = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.csv", row.names = FALSE)
saveRDS(master_df, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")
# 

```






**Duration and Persistence + Trend analayis - Basic lm**


 
   Processing of Sea Ice Data

    Loading Data:
        The script loads the sea ice data files (raster stacks) and the study area shapefile.
        Each raster stack represents sea ice concentration for a specific year.

    Metric Calculation for Each Year:
        For each year in the range start_year to end_year, the script calculates the specified metrics (persistence, duration, concentration) for each cell in the raster stack.

    Persistence Calculation:
        Persistence: The number of days per year where sea ice concentration is above a threshold (0.15).
        This is calculated using a sum function over the raster stack for the year.

    Duration Calculation:
        Duration: The maximum consecutive days per year where sea ice concentration is above the threshold.
        This is calculated using a custom function that finds the longest run of days exceeding the threshold.

    Concentration Calculation:
        Concentration: The average sea ice concentration per year.
        This is calculated using a mean function over the raster stack for the year.

Applying Linear Models

    Time Series Data:
        After calculating the yearly metrics for each cell, the script builds a time series for each cell. This time series spans from start_year to end_year.

    Linear Model Fitting:
        For each cell, the script fits a linear model (lm) to the time series of yearly metric values.
        The response variable (y) is the metric value for each year.
        The predictor variable (years) is the sequence of years.
        
        

```{r}
# Load necessary libraries
library(terra)
library(dplyr)
library(lubridate)
library(nlme)
library(ggplot2)
library(grDevices)  # For colorRampPalette
library(gridExtra)  # For arranging plots side by side

# Define the function
analyze_sea_ice <- function(start_year, end_year, use_mask = TRUE, report_metrics = c("concentration", "duration", "persistence")) {
  
  # Define file paths
  chunk_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif"
  chunk_files <- list.files(chunk_dir, pattern = "\\.tif$", full.names = TRUE)
  study_area_shapefile <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gis-layers/study-area/shp/Frostbound_Study_Areas_EPSG_3976.shp"
  
  # Load the study area shapefile
  if (use_mask) {
    study_area <- vect(study_area_shapefile)
  }
  
  # Define the threshold for sea ice concentration
  ice_threshold <- .15
  
  # Function to calculate consecutive days above threshold
  calculate_duration <- function(x, threshold) {
    if (all(is.na(x))) return(NA)
    rle_result <- rle(x > threshold)
    max_duration <- ifelse(any(rle_result$values), max(rle_result$lengths[rle_result$values]), 0)
    # print(paste0("Max Duration: ", max_duration))
    return(max_duration)
  }
  
  # Simplified function to process each chunk file using app
  process_chunk <- function(file_path) {
    region_name <- gsub(".*/|\\.tif$", "", file_path)
    message(paste("Processing file:", file_path))
    
    # Load the NSIDC sea ice concentration data for the subregion
    nsidc <- rast(file_path)
    
    annual_persistence <- list()
    annual_duration <- list()
    annual_concentration <- list()
    
    for (year in start_year:end_year) {
      message(paste("Processing year:", year))
      start_date <- as.Date(paste0(year, "-01-01"))
      end_date <- as.Date(paste0(year, "-12-31"))
      
      # Filter the sea ice data for the current year
      nsidc_year <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))
      
      if (nlyr(nsidc_year) == 0) {
        message(paste("No data for year:", year))
        next
      }
      
      # Calculate metrics based on options
      if ("persistence" %in% report_metrics) {
        persistence <- app(nsidc_year, function(x) sum(ifelse(x < ice_threshold, 0, 1), na.rm = FALSE))
        print(paste0("Persistence: ", persistence ))
        # summary(persistence[[1]])
        names(persistence) <- paste0("Persistence_", year)
        annual_persistence[[as.character(year)]] <- persistence
        print(paste0("Annaul peristence:" , annual_persistence))
      }

      
      if ("duration" %in% report_metrics) {
        duration <- app(nsidc_year, function(x) calculate_duration(x, ice_threshold))
        names(duration) <- paste0("Duration_", year)
        annual_duration[[as.character(year)]] <- duration
      }
      
      if ("concentration" %in% report_metrics) {
        concentration <- app(nsidc_year, function(x) mean(x, na.rm = TRUE))
        str(concentration)
        names(concentration) <- paste0("Concentration_", year)
        annual_concentration[[as.character(year)]] <- concentration
      }
    }
    
    message(paste("Finished processing file:", file_path))
    return(list(persistence = annual_persistence, duration = annual_duration, concentration = annual_concentration))
  }
  
  # Process each chunk file sequentially
  results <- lapply(chunk_files, process_chunk)
  
  # Combine all annual rasters into a single stack for the selected metrics
  combine_rasters <- function(raster_list) {
    raster_stack <- rast()
    for (year in names(raster_list)) {
      if (!is.null(raster_list[[year]])) {
        raster_stack <- c(raster_stack, raster_list[[year]])
      }
    }
    return(raster_stack)
  }
  
  persistence_stack <- NULL
  duration_stack <- NULL
  concentration_stack <- NULL
  
  if ("persistence" %in% report_metrics) {
    persistence_stack <- combine_rasters(do.call(c, lapply(results, `[[`, "persistence")))
    if (use_mask) persistence_stack <- mask(persistence_stack, study_area)
  }
  
  if ("duration" %in% report_metrics) {
    duration_stack <- combine_rasters(do.call(c, lapply(results, `[[`, "duration")))
    if (use_mask) duration_stack <- mask(duration_stack, study_area)
  }
  
  if ("concentration" %in% report_metrics) {
    concentration_stack <- combine_rasters(do.call(c, lapply(results, `[[`, "concentration")))
    if (use_mask) concentration_stack <- mask(concentration_stack, study_area)
  }
  
  # Function to apply linear model to each pixel and return slope and p-value
  apply_lm <- function(y) {
    if (all(is.na(y))) {
      return(c(NA, NA))
    } else {
      years <- start_year:end_year
      fit <- tryCatch(lm(y ~ years), error = function(e) return(NULL))
      if (is.null(fit)) {
        return(c(NA, NA))
      } else {
        slope <- coef(fit)[2]  # Slope of the linear model
        p_value <- summary(fit)$coefficients[2, 4]  # p-value for the slope
        return(c(slope, p_value))
      }
    }
  }
  
  persistence_trend <- NULL
  duration_trend <- NULL
  concentration_trend <- NULL
  
  if (!is.null(persistence_stack)) persistence_trend <- app(persistence_stack, apply_lm)
  if (!is.null(duration_stack)) duration_trend <- app(duration_stack, apply_lm)
  if (!is.null(concentration_stack)) concentration_trend <- app(concentration_stack, apply_lm)
  
  # Extract slopes and p-values from the results
  persistence_slope <- NULL
  persistence_pvalue <- NULL
  duration_slope <- NULL
  duration_pvalue <- NULL
  concentration_slope <- NULL
  concentration_pvalue <- NULL
  
  if (!is.null(persistence_trend)) {
    persistence_slope <- persistence_trend[[1]]
    persistence_pvalue <- persistence_trend[[2]]
  }
  
  if (!is.null(duration_trend)) {
    duration_slope <- duration_trend[[1]]
    duration_pvalue <- duration_trend[[2]]
  }
  
  if (!is.null(concentration_trend)) {
    concentration_slope <- concentration_trend[[1]]
    concentration_pvalue <- concentration_trend[[2]]
  }
  
  # Verify the integrity of the data
  if (!is.null(persistence_slope)) {
    print("Summary of persistence slope layer:")
    print(summary(persistence_slope))
    print("Summary of persistence p-value layer:")
    print(summary(persistence_pvalue))
  }
  
  if (!is.null(duration_slope)) {
    print("Summary of duration slope layer:")
    print(summary(duration_slope))
    print("Summary of duration p-value layer:")
    print(summary(duration_pvalue))
  }
  
  if (!is.null(concentration_slope)) {
    print("Summary of concentration slope layer:")
    print(summary(concentration_slope))
    print("Summary of concentration p-value layer:")
    print(summary(concentration_pvalue))
  }
  
  # Define a significance level
  alpha <- 0.05
  
  # Calculate statistics for persistence
  if (!is.null(persistence_slope)) {
    persistence_values <- values(persistence_slope)
    persistence_pvalues <- values(persistence_pvalue)
    persistence_data <- cbind(persistence_values, persistence_pvalues)
    persistence_data <- persistence_data[complete.cases(persistence_data), ]
    persistence_values <- persistence_data[, 1]
    persistence_pvalues <- persistence_data[, 2]
    persistence_mean <- mean(persistence_values)
    persistence_median <- median(persistence_values)
    persistence_min <- min(persistence_values)
    persistence_max <- max(persistence_values)
    persistence_significant <- sum(persistence_pvalues <= alpha) / length(persistence_pvalues) * 100
    persistence_positive <- sum(persistence_pvalues <= alpha & persistence_values > 0) / length(persistence_pvalues) * 100
    persistence_negative <- sum(persistence_pvalues <= alpha & persistence_values < 0) / length(persistence_pvalues) * 100
    persistence_nonsignificant <- 100 - persistence_significant
  }
  
  # Calculate statistics for duration
  if (!is.null(duration_slope)) {
    duration_values <- values(duration_slope)
    duration_pvalues <- values(duration_pvalue)
    duration_data <- cbind(duration_values, duration_pvalues)
    duration_data <- duration_data[complete.cases(duration_data), ]
    duration_values <- duration_data[, 1]
    duration_pvalues <- duration_data[, 2]
    duration_mean <- mean(duration_values)
    duration_median <- median(duration_values)
    duration_min <- min(duration_values)
    duration_max <- max(duration_values)
    duration_significant <- sum(duration_pvalues <= alpha) / length(duration_pvalues) * 100
    duration_positive <- sum(duration_pvalues <= alpha & duration_values > 0) / length(duration_pvalues) * 100
    duration_negative <- sum(duration_pvalues <= alpha & duration_values < 0) / length(duration_pvalues) * 100
    duration_nonsignificant <- 100 - duration_significant
  }
  
  # Calculate statistics for concentration
  if (!is.null(concentration_slope)) {
    concentration_values <- values(concentration_slope)
    concentration_pvalues <- values(concentration_pvalue)
    concentration_data <- cbind(concentration_values, concentration_pvalues)
    concentration_data <- concentration_data[complete.cases(concentration_data), ]
    concentration_values <- concentration_data[, 1]
    concentration_pvalues <- concentration_data[, 2]
    concentration_mean <- mean(concentration_values)
    concentration_median <- median(concentration_values)
    concentration_min <- min(concentration_values)
    concentration_max <- max(concentration_values)
    concentration_significant <- sum(concentration_pvalues <= alpha) / length(concentration_pvalues) * 100
    concentration_positive <- sum(concentration_pvalues <= alpha & concentration_values > 0) / length(concentration_pvalues) * 100
    concentration_negative <- sum(concentration_pvalues <= alpha & concentration_values < 0) / length(concentration_pvalues) * 100
    concentration_nonsignificant <- 100 - concentration_significant
  }
  
  # Print summary statistics for reporting
  if (!is.null(persistence_slope)) {
    persistence_summary <- data.frame(
      Metric = c("Mean slope", "Median slope", "Minimum slope", "Maximum slope", "% Significant", "% Positive", "% Negative", "% Non-significant"),
      Persistence = c(persistence_mean, persistence_median, persistence_min, persistence_max, persistence_significant, persistence_positive, persistence_negative, persistence_nonsignificant)
    )
    print(persistence_summary)
  }
  
  if (!is.null(duration_slope)) {
    duration_summary <- data.frame(
      Metric = c("Mean slope", "Median slope", "Minimum slope", "Maximum slope", "% Significant", "% Positive", "% Negative", "% Non-significant"),
      Duration = c(duration_mean, duration_median, duration_min, duration_max, duration_significant, duration_positive, duration_negative, duration_nonsignificant)
    )
    print(duration_summary)
  }
  
  if (!is.null(concentration_slope)) {
    concentration_summary <- data.frame(
      Metric = c("Mean slope", "Median slope", "Minimum slope", "Maximum slope", "% Significant", "% Positive", "% Negative", "% Non-significant"),
      Concentration = c(concentration_mean, concentration_median, concentration_min, concentration_max, concentration_significant, concentration_positive, concentration_negative, concentration_nonsignificant)
    )
    print(concentration_summary)
  }
  
  # Define a color palette for trends
  trend_colors <- colorRampPalette(c("red", "white", "blue"))
  
  # Set up multi-panel plotting environment
  par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))
  
  if (!is.null(persistence_slope)) {
    plot(persistence_slope, col=trend_colors(100), main="Trend of Persistence Over Time", legend=TRUE, colNA="black")
  }
  
  if (!is.null(duration_slope)) {
    plot(duration_slope, col=trend_colors(100), main="Trend of Duration Over Time", legend=TRUE, colNA="black")
  }
  
  if (!is.null(concentration_slope)) {
    plot(concentration_slope, col=trend_colors(100), main="Trend of Concentration Over Time", legend=TRUE, colNA="black")
  }
  
  # Classify the trends based on significance and directionality using ifel function
  if (!is.null(persistence_pvalue)) {
    significant_cells <- persistence_pvalue <= alpha
    significant_mask <- ifel(significant_cells, 1, NA)
    positive_trend <- ifel(significant_mask == 1 & persistence_slope > 0, 1, NA)
    negative_trend <- ifel(significant_mask == 1 & persistence_slope < 0, 2, NA)
    na_mask <- ifel(is.na(persistence_slope), 3, NA)
    combined_trend <- positive_trend
    combined_trend[!is.na(negative_trend)] <- 2
    combined_trend[!is.na(na_mask)] <- 3
    color_table <- data.frame(value = c(1, 2, 3), col = c("blue", "red", "black"))
    coltab(combined_trend) <- color_table
    plot(combined_trend, main="Significance and Directionality of Persistence Trends")
    legend("topright", legend=c("Positive Trend", "Negative Trend", "NA Cells"), fill=color_table$col)
  }
  
  if (!is.null(duration_pvalue)) {
    significant_cells <- duration_pvalue <= alpha
    significant_mask <- ifel(significant_cells, 1, NA)
    positive_trend <- ifel(significant_mask == 1 & duration_slope > 0, 1, NA)
    negative_trend <- ifel(significant_mask == 1 & duration_slope < 0, 2, NA)
    na_mask <- ifel(is.na(duration_slope), 3, NA)
    combined_trend <- positive_trend
    combined_trend[!is.na(negative_trend)] <- 2
    combined_trend[!is.na(na_mask)] <- 3
    color_table <- data.frame(value = c(1, 2, 3), col = c("blue", "red", "black"))
    coltab(combined_trend) <- color_table
    plot(combined_trend, main="Significance and Directionality of Duration Trends")
    legend("topright", legend=c("Positive Trend", "Negative Trend", "NA Cells"), fill=color_table$col)
  }
  
  if (!is.null(concentration_pvalue)) {
    significant_cells <- concentration_pvalue <= alpha
    significant_mask <- ifel(significant_cells, 1, NA)
    positive_trend <- ifel(significant_mask == 1 & concentration_slope > 0, 1, NA)
    negative_trend <- ifel(significant_mask == 1 & concentration_slope < 0, 2, NA)
    na_mask <- ifel(is.na(concentration_slope), 3, NA)
    combined_trend <- positive_trend
    combined_trend[!is.na(negative_trend)] <- 2
    combined_trend[!is.na(na_mask)] <- 3
    color_table <- data.frame(value = c(1, 2, 3), col = c("blue", "red", "black"))
    coltab(combined_trend) <- color_table
    plot(combined_trend, main="Significance and Directionality of Concentration Trends")
    legend("topright", legend=c("Positive Trend", "Negative Trend", "NA Cells"), fill=color_table$col)
  }
}

# Example usage
analyze_sea_ice(start_year = 1981, end_year = 2023, use_mask = FALSE, report_metrics = c("concentration", "duration", "persistence"))

```









**Extent Analysis: Generalized Least Squares**


```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)
library(lmtest) # For Breusch-Pagan test
library(nortest) # For Shapiro-Wilk test

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_extent = mean(IceExtent_km, na.rm = TRUE), 
            sd_extent = sd(IceExtent_km, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(mean_extent ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Diagnostic plots and tests
for (key in names(gls_results)) {
  cat("Region and Month:", key, "\n")
  
  model <- gls_results[[key]]
  summary_model <- summary(model)
  print(summary_model)
  
  # Residuals
  residuals <- residuals(model, type = "normalized")
  fitted_values <- fitted(model)
  
  # Residual vs Fitted Plot
  plot(fitted_values, residuals, main = paste("Residuals vs Fitted for", key),
       xlab = "Fitted values", ylab = "Residuals")
  abline(h = 0, col = "red")
  
  # Q-Q Plot of Residuals
  qqnorm(residuals, main = paste("Q-Q Plot for", key))
  qqline(residuals, col = "red")
  
  # Autocorrelation Function Plot
  acf(residuals, main = paste("ACF of Residuals for", key))
  
  # Breusch-Pagan Test for Heteroscedasticity
  bp_test <- bptest(mean_extent ~ Time, data = region_month_data)
  print(bp_test)
  
  # Shapiro-Wilk Test for Normality of Residuals
  sw_test <- shapiro.test(residuals)
  print(sw_test)
  
  cat("\n")
}

```





**Concentration Analysis - Generalized Least Squares**

```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(nlme)
library(wesanderson)
library(scales)

# Load your dataset
nsidc_metrics <-readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

str(nsidc_metrics)

str(nsidc_metrics)
# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of sea ice concentration for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_concentration = mean(MeanSIC, na.rm = TRUE), 
            sd_concentration = sd(MeanSIC, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Define a function to fit a GLS model for a given region and month
fit_gls_model <- function(region_data) {
  gls_model <- gls(mean_concentration ~ Time, correlation = corAR1(), data = region_data)
  return(gls_model)
}

# Apply the model for each unique region and month combination and store the results
regions <- unique(monthly_averages$Region)
months <- unique(monthly_averages$Month)
gls_results <- list()

for (region in regions) {
  for (month in months) {
    region_month_data <- monthly_averages %>% filter(Region == region, Month == month)
    if (nrow(region_month_data) > 1) {
      gls_results[[paste(region, month, sep = "_")]] <- fit_gls_model(region_month_data)
    }
  }
}

# Display summary of the GLS model for each region and month
for (key in names(gls_results)) {
  cat("Region and Month:", key, "\n")
  print(summary(gls_results[[key]]))
  cat("\n")
}

# Directory to save the plots
output_directory <- "D:/Manuscripts_localData/FrostBound_AQ/Results/gls_trends"
dir.create(output_directory, recursive = TRUE, showWarnings = FALSE)

# Color settings
sd_color <- "#7AB8BF"  # Light red color 
axis_color <- "gray70"  # Light gray color for the axis lines

# Visualize the trend for each region and save the plots
for (region in regions) {
  region_data <- monthly_averages %>% filter(Region == region)
  
  p <- region_data %>%
    ggplot(aes(x = Year, y = mean_concentration)) +
    geom_errorbar(aes(ymin = mean_concentration - sd_concentration, ymax = mean_concentration + sd_concentration), 
                  width = 0.2, color = sd_color) +
    geom_point(color = "black") +
    geom_smooth(method = "lm", color = "blue", se = FALSE) +
    facet_wrap(~month2, scales = "fixed", ncol = 3) +
    xlab(NULL) +
    ylab("Mean Sea Ice Concentration (%)") +
    labs(
      title = sprintf(
        "%s: Sea ice concentration between %d and %d",
        region,
        min(region_data$Year),
        max(region_data$Year)
      ),
      subtitle = "The vertical bar at each point shows the standard deviation around the mean. The blue line represents the linear trend.",
      caption = "Source: NSIDC 25km Sea Ice Index dataset"
    ) +
    theme(
      plot.caption = element_text(size = 8, color = "black"),
      plot.margin = unit(c(5.5, 10, 5.5, 5.5), "points"),
      panel.background = element_rect(fill = "white"),
      strip.background = element_rect(fill = "white"),
      strip.text = element_text(colour = "black", size = 12, face = "bold"),
      panel.grid = element_blank(),
      axis.line = element_line(color = axis_color),
      legend.position = "none"
    )
  
  # Save the plot
  ggsave(filename = paste(output_directory, paste("25km_Concentration_trend_plot_", region, ".png", sep = ""), sep = "/"), plot = p)
  
  # Save the plot as EPS
  ggsave(filename = paste(output_directory, paste("25km_Concentration_trend_plot_", region, ".eps", sep = ""), sep = "/"), plot = p, device = "eps")
}

# Extract and print coefficients and p-values for each region and month
coefficients_gls <- lapply(gls_results, function(model) summary(model)$tTable)
names(coefficients_gls) <- names(gls_results)

# Print coefficients and p-values
for (key in names(coefficients_gls)) {
  cat("Region and Month:", key, "\n")
  print(coefficients_gls[[key]])
  cat("\n")
}


```


**Southern Oscillation Index Analysis**

```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)
library(ggplot2)
library(httr)
library(tidyr)
library(broom)
library(gridExtra)
library(RColorBrewer)

# Load your dataset
nsidc_metrics <- readRDS("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/trend-analysis/Harmonized_12km_Daily_Metrics.rds")

# Filter for winter months (June - September) and add Year, Month, and Day columns
winter_metrics <- nsidc_metrics %>%
  mutate(Year = year(Date), Month = month(Date), Day = day(Date)) %>%
  filter(Month %in% c(6, 7, 8, 9))

# Calculate monthly averages of ice extent for each region
monthly_averages <- winter_metrics %>%
  group_by(Year, Month, Region) %>%
  summarise(mean_extent = mean(IceExtent_km, na.rm = TRUE), 
            sd_extent = sd(IceExtent_km, na.rm = TRUE), .groups = 'drop') %>%
  ungroup()

# Create a time variable for trend analysis
monthly_averages <- monthly_averages %>%
  mutate(Time = as.numeric(Year) + (Month - 1) / 12,
         month2 = month(ymd(paste(Year, Month, 1)), label = TRUE, abbr = FALSE))

# Load SOI data
url <- "https://www.cpc.ncep.noaa.gov/data/indices/soi"
soi_raw <- GET(url)
soi_text <- content(soi_raw, "text")

# Preprocess the SOI data
soi_lines <- strsplit(soi_text, "\n")[[1]]

# Identify the start of the standardized SOI data section
standardized_soi_start <- grep("STANDARDIZED    DATA", soi_lines)

# Extract standardized SOI data lines
soi_standardized_lines <- soi_lines[(standardized_soi_start + 3):length(soi_lines)]
soi_standardized_lines <- soi_standardized_lines[!grepl("-999.9", soi_standardized_lines)] # remove lines with invalid data

# Convert standardized SOI data to dataframe
soi_data <- read.table(text = soi_standardized_lines, fill = TRUE, stringsAsFactors = FALSE)
colnames(soi_data) <- c("Year", month.abb)
soi_data <- soi_data %>% mutate(Year = as.integer(Year))

# Convert to long format
soi_data_long <- soi_data %>%
  pivot_longer(-Year, names_to = "Month", values_to = "SOI") %>%
  mutate(Month = match(Month, month.abb), 
         Year = as.integer(Year),
         SOI = as.numeric(SOI)) %>%
  filter(!is.na(SOI))  # Remove rows with NA values in SOI

# Merge sea ice data with SOI data
merged_data <- merge(monthly_averages, soi_data_long, by = c("Year", "Month"))

# Remove duplicate rows
merged_data <- merged_data[!duplicated(merged_data), ]

# Calculate correlation for each region and month
correlation_results <- merged_data %>%
  group_by(Region, Month) %>%
  summarize(correlation = cor(mean_extent, SOI, use = "complete.obs"),
            p.value = cor.test(mean_extent, SOI)$p.value) %>%
  arrange(Region, Month)

# Print the correlation results
print(correlation_results)




# Visualize the correlation results using RColorBrewer color palette
ggplot(correlation_results, aes(x = factor(Month, levels = 6:9), y = correlation, fill = Region)) +
  geom_bar(stat = "identity", position = position_dodge(0.9)) +
  geom_text(aes(label = round(correlation, 2)), vjust = -1.5, position = position_dodge(0.9)) +
  scale_fill_brewer(palette = "Set3") +  # You can choose other palettes like "Paired", "Dark2", etc.
  labs(title = "Correlation between SOI and Sea Ice Extent by Region and Month",
       x = "Month", y = "Correlation") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )




# Linear regression analysis to quantify the relationship between SOI and sea ice extent
regression_results <- merged_data %>%
  group_by(Region, Month) %>%
  do(tidy(lm(mean_extent ~ SOI, data = .)))

# Print regression results
print(regression_results)

# Filter the regression results for the SOI term
soi_coefficients <- regression_results %>%
  filter(term == "SOI")

# Function to create a plot for each region
plot_soi_effect <- function(region_data) {
  ggplot(region_data, aes(x = factor(Month), y = estimate, color = factor(Month))) +
    geom_point(size = 3) +
    geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    labs(
      title = paste("Effect of SOI on Sea Ice Extent:", unique(region_data$Region)),
      x = "Month",
      y = "SOI Coefficient Estimate",
      color = "Month"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5),
      legend.position = "none"
    )
}

# Apply the function for each region and plot
unique_regions <- unique(soi_coefficients$Region)
plots <- lapply(unique_regions, function(region) {
  region_data <- soi_coefficients %>% filter(Region == region)
  plot_soi_effect(region_data)
})

# Display the plots
do.call(grid.arrange, c(plots, ncol = 2))

# # Create a combined plot showing the SOI coefficients for all regions and months
# ggplot(soi_coefficients, aes(x = factor(Month), y = estimate, color = Region)) +
#   geom_point(size = 3) +
#   geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
#   geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
#   labs(
#     title = "Effect of SOI on Sea Ice Extent Across Regions and Months",
#     x = "Month",
#     y = "SOI Coefficient Estimate",
#     color = "Region"
#   ) +
#   theme_minimal() +
#   theme(
#     plot.title = element_text(hjust = 0.5)
  # )

# Create individual plots for each region and arrange them using facet_wrap
ggplot(soi_coefficients, aes(x = factor(Month), y = estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  facet_wrap(~ Region, scales = "free_y") +
  labs(
    title = "Effect of SOI on Sea Ice Extent Across Regions and Months",
    x = "Month",
    y = "SOI Coefficient Estimate"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )


```






