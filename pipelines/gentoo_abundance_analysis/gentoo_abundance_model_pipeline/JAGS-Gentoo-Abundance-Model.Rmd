---
title: "Gentoo Model Parameter Abundance Model"
author: "Michael J. Wethington"
date: "Lastest Version: 2025-10-04"
output: html_document
---


zi = latent nest abundance (mean-adjusted)
lz = logged abundance (re-expression of above):  lzi,t=log(zi,t). for the ith site in the tth year,
ri = intrinsic growth rate
lp = predicted population growth rate multiplier
la = actual population growth rate multiplier 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


**Generate Presence Absence Assumptions**
```{r}
library(tidyverse)


required_packages <- c("rlang", "fastmap", "digest", "fs", "cachem", "vctrs", "stringi", "glue", "cli", "utf8", "fansi", "dplyr")

for(pkg in required_packages){
  if(!require(pkg, character.only = TRUE)){
    install.packages(pkg)
  }
}

min_season <- 1970
max_season <- 2023
species <- "GEPE"

# assign the total number of seasons as n_seasons
(n_seasons <- (max_season - min_season) + 1)


SiteList <- mapppdr::penguin_obs %>%
  # keep all sites that have at least 1 count between min and max season
  dplyr::filter(count > 0 & species_id == species & season >= min_season & season <= max_season) %>%
  # create relative season index 
  mutate(season_relative = season - min_season + 1) %>%
  # determine first season a count is observed for each site
  group_by(site_id) %>%
  summarise(initial_season = min(season_relative)) %>%
  ungroup() %>%
  # join to get other site specific covariates for visualization purposes
  left_join(mapppdr::sites, by = "site_id") %>%
  # create site index for model and visualization
  mutate(site = as.numeric(as.factor(site_id))) %>%
  dplyr::select(site_id, site_name, ccamlr_id, site, initial_season, latitude, longitude)

(n_sites <- nrow(SiteList))

# create site x season template which is used throughout analysis
w_template <- SiteList %>%
  dplyr::select(site_id, site) %>%
  # expand each site by the number of seasons
  uncount(n_seasons) %>%
  # create relative season index for each site
  mutate(season_relative = rep(1:n_seasons, n_sites)) %>%
  # create season var from relative season index
  mutate(season = season_relative + min_season - 1) %>%
  arrange(season_relative, site)

w_df <- rbind(
  # keep all presence/absence data and assign observation type as 2 (observed)
  mapppdr::penguin_obs %>%
    dplyr::filter(species_id == species & season >= min_season & season <= max_season) %>%
    dplyr::select(site_id, season, presence) %>%
    mutate(known_w = 1),
  # append presence/absence assumption data which is not part of mapppd
  # and assign observation type of 1 (assumed)
  data.frame(read_csv(file = "D:/Manuscripts_localData/FrostBound_AQ/Datasets/mapppd/gentoo_presence_absence_assumptions.csv")) %>%
    mutate(known_w = 0)) %>%
  # determine for each site x season if breeding is observed or assumed
  group_by(site_id, season) %>%
  summarise(w = base::max(presence), known_w = base::max(known_w)) %>%
  ungroup() %>%
  # join with w_template to fill in missing site x seasons with no presence/absence data
  right_join(w_template, by = c("site_id", "season")) %>%
  # assign observation type as 0 (imputed)
  mutate(known_w = replace(known_w, is.na(known_w), 0)) %>%
  arrange(site_id, season) %>%
  # impute missing presence/absence data using the following assumptions
  # ASSSUMPTION: fill in NA between (1,1) with 1
  # ASSSUMPTION: fill in NA between (0,1) with 0
  # ASSSUMPTION: fill in NA between (1,0) with 1
  # ASSSUMPTION: fill in NA between (.,1) and (1,.) with 1
  # ASSSUMPTION: fill in NA between (.,0) and (0,.) with 0
  dplyr::group_by(site_id) %>%
  tidyr::fill(w, .direction = "downup") %>%
  dplyr::ungroup() %>%
  # create second site_id var for plotting sites alphabetically in ggplot
  mutate(site_id_rev = factor(site_id, levels = rev(sort(unique(site_id))))) %>%
  dplyr::select(site_id, site_id_rev, season, site, season_relative, w, known_w)

# convert w to matrix to be used in model
w <- w_df %>%
  dplyr::select(site, season_relative, w) %>%
  # create matrix where rows are sites and columns are seasons
  pivot_wider(names_from = season_relative, values_from = w, names_sort = TRUE) %>%
  dplyr::select(-site) %>%
  as.matrix()


abundance <- mapppdr::penguin_obs %>%
  # keep all counts between min and max season
  dplyr::filter(count > 0 & species_id == species & season >= min_season & season <= max_season) %>%
  # join to get site index and initial season
  right_join(SiteList, by = "site_id") %>%
  # create relative season index 
  mutate(season_relative = season - min_season + 1) %>%
  # ASSUMPTION: increase accuracy category of all adult counts by + 3 with a max error of 5
  rowwise() %>%
  mutate(accuracy = replace(accuracy, type == "adults", base::min((accuracy[type == "adults"] + 3), 5))) %>%
  ungroup() %>%  
  mutate(type = replace(type, type == "adults", "nests")) %>%
  # ASSUMPTION: keep maximum nest and chick count reported each season for a site
  group_by(site_id, season, season_relative, type) %>%
  arrange(desc(count), accuracy) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  # ASSUMPTION: convert accuracy to the following errors/precisions
  mutate(sigma = case_when(
    accuracy == 1 ~ 0.02490061, 
    accuracy == 2 ~ 0.04955838,
    accuracy == 3 ~ 0.1201131, 
    accuracy == 4 ~ 0.2212992, 
    accuracy == 5 ~ 0.4472728)) %>%
  mutate(precision = case_when(
    accuracy == 1 ~ 1/0.02490061^2, 
    accuracy == 2 ~ 1/0.04955838^2,
    accuracy == 3 ~ 1/0.1201131^2, 
    accuracy == 4 ~ 1/0.2212992^2, 
    accuracy == 5 ~ 1/0.4472728^2)) %>%  
  dplyr::select(site_id, site, season, season_relative, initial_season, type, 
                count, presence, accuracy, sigma, precision) %>%
  arrange(site, season_relative, type, -count, accuracy, sigma, precision)  

abundance_initial <- abundance %>%
  # keep first observed count for each site's time series
  dplyr::filter(initial_season == season_relative) %>%
  # ASSUMPTION: if no nest count is available in the initial season and a chick count is then
  # assume chick count is 1:1 nest count
  group_by(site_id, season, site, season_relative) %>%
  arrange(desc(type)) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  dplyr::select(site_id, season, site, season_relative, count, sigma, precision)

abundance_nests <- abundance %>%
  # keep all nest counts after the initial season
  dplyr::filter(initial_season != season_relative & type == "nests") %>%
  dplyr::select(site_id, season, site, season_relative, count, sigma, precision)

abundance_chicks <- rbind(
  # keep all chick counts after the initial season
  abundance %>%
    dplyr::filter(initial_season != season_relative & type == "chicks") %>%
    dplyr::select(site_id, season, site, season_relative, count, sigma, precision),
  # append chick counts from initial season that were not converted to nest counts
  # meaning there was both a chick and nest count in the initial season
  abundance %>%
    dplyr::filter(initial_season == season_relative) %>%
    group_by(site_id, season, site, season_relative) %>%
    arrange(desc(type)) %>%
    slice(2) %>%
    ungroup() %>%
    dplyr::select(site_id, season, site, season_relative, count, sigma, precision))

# moment match alpha shape and rate parameters for breeding productivity
mu <- .5
sigma <- .25
a <- (mu^2 - mu^3 - mu * sigma^3) / sigma^2
b <- (mu - 2* mu^2 + mu^3 - sigma^2 + mu * sigma^3) / sigma^2

# create the data list for the JAGS model
model_data <- list(
  nests = nrow(abundance_nests),
  y_n = log(abundance_nests$count), 
  precision_n = abundance_nests$precision,
  site_n = abundance_nests$site,
  season_n = abundance_nests$season_relative,
  chicks = nrow(abundance_chicks),
  y_c = log(abundance_chicks$count), 
  precision_c = abundance_chicks$precision,
  site_c = abundance_chicks$site,
  season_c = abundance_chicks$season_relative,
  y_i = log(abundance_initial$count),
  precision_i = abundance_initial$precision,
  n_sites = n_sites,
  n_seasons = n_seasons,
  s = as.vector(SiteList$initial_season),
  w = w,
  a = a,
  b = b)


```


**DEFINE AND RUN JAGS Model**
```{r}

# Install and load necessary libraries
# Optional installation lines are commented out. Uncomment to install if needed.
# if (!require("rjags")) install.packages("rjags", dependencies = TRUE)
# if (!require("coda")) install.packages("coda", dependencies = TRUE)
library(rjags)   # JAGS interface for Bayesian modeling
library(coda)    # Tools for MCMC output analysis
library(dplyr)
library(tidyr)
library(readr)
library(parallel) # Enables parallel processing

# Define the file path for the JAGS model script
model_file_path <- "D:/Manuscripts_localData/FrostBound_AQ/Results/gentoo-abundance-model/jags_model.jags"

# Write the JAGS model code to a text file
sink(model_file_path)
cat("
model {

# Define prior for process variance (sigma) and precision (tau)
sigma ~ dunif(0, 1)         # Uniform prior for sigma, allowing variation between 0 and 1
tau <- pow(sigma, -2)       # Precision is inverse squared sigma

# Breeding success prior for chick abundance
for (i in 1:chicks) {
  alpha[i] ~ dbeta(a, b)    # Beta distribution for breeding success, based on input parameters a and b
}

# Initial abundance for each site
for (i in 1:n_sites) {
  lz[i, s[i]] ~ dnorm(0, .001)  # Normally distributed initial abundance with high variance
}

# Define a prior for the intercept (beta) to allow variability in population growth
beta ~ dunif(-.5, .5)

# Site-specific effects (eta) for each site
for (i in 1:n_sites) {
  eta[i] ~ dnorm(0, tau_site)
}
sigma_site ~ dunif(0, 1)       # Uniform prior for site-level variation
tau_site <- pow(sigma_site, -2)

# Seasonal effects (epsilon) for each season
for (t in 1:n_seasons) {
  epsilon[t] ~ dnorm(0, tau_season)
}
sigma_season ~ dunif(0, 1)     # Uniform prior for season-level variation
tau_season <- pow(sigma_season, -2)

# Observation model for nest data
for (i in 1:nests) {
  y_n[i] ~ dnorm(mu_y_n[i], precision_n[i])   # Observed nest counts with specified precision
  mu_y_n[i] <- lz[site_n[i], season_n[i]] - 1/(2 * precision_n[i])  # Mean adjusted for precision
  y_n_new[i] ~ dnorm(mu_y_n[i], precision_n[i])   # Simulated nest counts for predictive checks
  y_n_sq[i] <- pow((y_n[i] - mu_y_n[i]), 2)       # Squared residuals for observed nest counts
  y_n_sq_new[i] <- pow((y_n_new[i] - mu_y_n[i]), 2)  # Squared residuals for simulated nest counts
}

# Observation model for chick data
for (i in 1:chicks) {
  N[i] <- 2 * round(exp(lz[site_c[i], season_c[i]]))   # Expected population size for chicks
  z_c[i] ~ dbin(alpha[i], N[i])                        # Binomially distributed chick abundance
  lz_c[i] <- log(z_c[i])                               # Log-transformed abundance
  y_c[i] ~ dnorm(mu_y_c[i], precision_c[i])            # Observed chick counts with specified precision
  mu_y_c[i] <- lz_c[i] - 1/(2 * precision_c[i])        # Mean for chick data, adjusted for precision
  y_c_new[i] ~ dnorm(mu_y_c[i], precision_c[i])        # Simulated chick counts for predictive checks
  y_c_sq[i] <- pow((y_c[i] - mu_y_c[i]), 2)            # Squared residuals for observed chick counts
  y_c_sq_new[i] <- pow((y_c_new[i] - mu_y_c[i]), 2)    # Squared residuals for simulated chick counts
}

# Process model for population growth
for (i in 1:n_sites) {
  for (t in 1:n_seasons) {
    zr[i, t] <- beta + eta[i] + epsilon[t]   # Growth rate influenced by site and seasonal effects
    lza[i, t] <- lz[i, t] * w[i, t]          # Adjusted abundance accounting for site-season presence
  }
}  

# Initial year abundance data model
for (i in 1:n_sites) {
  y_i[i] ~ dnorm(mu_y_i[i], precision_i[i])   # Observed abundance with precision for initial season
  mu_y_i[i] <- lz[i, s[i]] - 1/(2 * precision_i[i])  # Mean adjusted for precision
  y_i_new[i] ~ dnorm(mu_y_i[i], precision_i[i])      # Simulated initial counts for predictive checks
  y_i_sq[i] <- pow((y_i[i] - mu_y_i[i]), 2)          # Squared residuals for observed initial counts
  y_i_sq_new[i] <- pow((y_i_new[i] - mu_y_i[i]), 2)  # Squared residuals for simulated initial counts
}

# Population abundance dynamics for subsequent seasons
for (i in 1:n_sites) {
  for (t in (s[i] + 1):n_seasons) {
    lz[i, t] ~ dnorm(mu_lz[i, t], tau)              # Abundance in each season based on prior year
    mu_lz[i, t] <- lz[i, t - 1] + zr[i, t] - 1/(2 * tau)  # Process model with adjusted precision
  }
}

# Population abundance dynamics for pre-initial seasons
for (i in 1:n_sites) {
  for (t in 1:(s[i] - 1)) {
    lz[i, s[i] - t] ~ dnorm(mu_lz[i, s[i] - t], tau)       # Prior abundance modeled on next season
    mu_lz[i, s[i] - t] <- lz[i, s[i] - t + 1] - zr[i, s[i] - t + 1] - 1/(2 * tau)
  }
}
 
# Posterior predictive checks
y_n_sqs <- sum(y_n_sq[])      # Sum of squared residuals for nests (observed)
y_n_sqs_new <- sum(y_n_sq_new[])  # Sum of squared residuals for nests (simulated)
y_i_sqs <- sum(y_i_sq[])      # Sum of squared residuals for initial season (observed)
y_i_sqs_new <- sum(y_i_sq_new[])  # Sum of squared residuals for initial season (simulated)
y_c_sqs <- sum(y_c_sq[])      # Sum of squared residuals for chicks (observed)
y_c_sqs_new <- sum(y_c_sq_new[])  # Sum of squared residuals for chicks (simulated)

# Derived quantities for growth rate calculation
for (i in 1:n_sites) {
  for (t in 2:n_seasons) {
    l_a[i, t - 1] <- exp(lz[i, t] - lz[i, t - 1])   # Annual growth based on log-abundance change
    l_p[i, t - 1] <- exp(zr[i, t])                  # Growth rate (zr) transformation
    lw_a[i, t - 1] <- ifelse(sum(w[i, (t-1):t]) == 2, l_a[i, t - 1], 1)   # Weighted growth rate
    lw_p[i, t - 1] <- ifelse(sum(w[i, (t-1):t]) == 2, l_p[i, t - 1], 1)   # Weighted persistence rate
  }
}

# Geometric mean calculations for site-specific growth and persistence
for (i in 1:n_sites) {
  x[i, 1:n_seasons] <- ifelse(sum(w[i, 1:n_seasons]) > 1, w[i, 1:n_seasons], rep(1, n_seasons)) 
  gl_a[i] <- ifelse(sum(w[i, 1:n_seasons]) > 1, pow(prod(lw_a[i, ]), (1/(sum(x[i, 1:n_seasons]) - 1))), 0)
  gl_p[i] <- ifelse(sum(w[i, 1:n_seasons]) > 1, pow(prod(lw_p[i, ]), (1/(sum(x[i, 1:n_seasons]) - 1))), 0)
}

}", fill = TRUE)
sink() # Complete writing of the model to file

# Prepare the input data list for JAGS model
model_data <- list(
  nests = nrow(abundance_nests),    # Total number of nest observations
  y_n = log(abundance_nests$count), # Log-transformed nest counts
  precision_n = abundance_nests$precision, # Precision for each nest observation
  site_n = abundance_nests$site,    # Site indices for nest observations
  season_n = abundance_nests$season_relative, # Season indices for nest observations
  chicks = nrow(abundance_chicks),  # Total number of chick observations
  y_c = log(abundance_chicks$count), # Log-transformed chick counts
  precision_c = abundance_chicks$precision, # Precision for each chick observation
  site_c = abundance_chicks$site,    # Site indices for chick observations
  season_c = abundance_chicks$season_relative, # Season indices for chick observations
  y_i = log(abundance_initial$count), # Log-transformed initial season counts
  precision_i = abundance_initial$precision, # Precision for initial counts
  n_sites = n_sites,               # Number of unique sites
  n_seasons = n_seasons,           # Total number of seasons
  s = as.vector(SiteList$initial_season), # Initial seasons for each site
  w = w,                           # Presence-absence matrix for each site-season
  a = a,                           # Shape parameter for breeding productivity
  b = b                            # Rate parameter for breeding productivity
)



random_inits <- function(model_data) {
  # Generate a random seed for reproducibility
  seed = runif(1, 1, 100000)
  
  # Initialize model parameters with random values within specified ranges
  beta <- runif(1, -.025, .025)        # Random intercept for growth rate
  sigma_site <- runif(1, .025, .05)    # Standard deviation for site-level effects
  sigma_season <- runif(1, .05, .1)    # Standard deviation for season-level effects
  sigma <- runif(1, .05, .1)           # Standard deviation for process variance

  # Extract key values from model_data for use in initializations
  chicks <- model_data$chicks          # Total number of chick observations
  n_sites <- model_data$n_sites        # Number of unique sites
  n_seasons <- model_data$n_seasons    # Number of seasons in the study period
  s <- model_data$s                    # Initial season indices for each site
  y_c <- model_data$y_c                # Log-transformed chick counts
  y_i <- model_data$y_i                # Log-transformed initial season counts
  site_c <- model_data$site_c          # Site indices for chick observations
  season_c <- model_data$season_c      # Season indices for chick observations
  a <- model_data$a                    # Beta distribution shape parameter for chick survival
  b <- model_data$b                    # Beta distribution rate parameter for chick survival

  # Initialize random effects for sites and seasons
  eta <- rnorm(n_sites, 0, sigma_site)       # Site-specific random effects
  epsilon <- rnorm(n_seasons, 0, sigma_season) # Season-specific random effects
  
  # Initialize alpha for chick survival using the beta distribution
  alpha <- rbeta(chicks, a, b)               # Chick survival rates for each observation

  # Initialize abundance and growth rate matrices
  lz <- zr <- array(NA, dim = c(n_sites, n_seasons))  # Matrices for abundance (lz) and growth rate (zr)

  # Loop through each site to initialize abundance (lz) and growth rate (zr) across seasons
  for (i in 1:n_sites) {
    # Set initial abundance (lz) for the starting season based on mean initial chick count
    lz[i, s[i]] <- mean(y_i[i], na.rm = TRUE)
    
    # Forward initialization: calculate abundance for each season after the initial season
    for (t in (s[i] + 1):n_seasons) {
      zr[i, t] <- beta + eta[i] + epsilon[t]                # Growth rate as a function of intercept and random effects
      lz[i, t] <- rnorm(1, lz[i, (t - 1)] + zr[i, t] - sigma^2 / 2, sigma) # Abundance based on previous season and growth rate
    }
    
    # Backward initialization: calculate abundance for seasons before the initial season
    for (t in 1:(s[i] - 1)) {
      zr[i, (s[i] - t + 1)] <- beta + eta[i] + epsilon[(s[i] - t + 1)]  # Growth rate using random effects
      lz[i, (s[i] - t)] <- rnorm(1, lz[i, (s[i] - t + 1)] - zr[i, (s[i] - t + 1)] - sigma^2 / 2, sigma)
      # Calculate abundance by extrapolating backward from the initial season
    }
  }
  
  # Initialize chick abundance based on calculated abundance and survival rates
  z_c <- N <- NA  # Empty vectors for chick abundance and population size

  # Loop through each chick observation to calculate chick abundance (z_c)
  for (i in 1:chicks) {
    # Ensure abundance is non-negative for each chick observation's site and season
    if (lz[site_c[i], season_c[i]] < 0) lz[site_c[i], season_c[i]] <- 0
    
    # Calculate expected population size as twice the rounded abundance
    N[i] <- 2 * round(exp(lz[site_c[i], season_c[i]]))
    
    # Calculate chick abundance using a binomial distribution, with a minimum of 1 to avoid zeroes
    z_c[i] <- base::max(rbinom(1, prob = alpha[i], N[i]), 1)
  }
  
  # Return a list of initialized values for the JAGS model
  return(list(
    sigma = sigma,                # Process variance
    sigma_site = sigma_site,      # Site-level variance
    sigma_season = sigma_season,  # Season-level variance
    beta = beta,                  # Growth rate intercept
    eta = eta,                    # Site-specific random effects
    epsilon = epsilon,            # Season-specific random effects
    alpha = alpha,                # Chick survival rates
    lz = lz,                      # Site-season abundance matrix
    z_c = z_c,                    # Chick abundance values
    .RNG.name = "base::Mersenne-Twister", # RNG for reproducibility in JAGS
    .RNG.seed = seed              # Random seed for reproducibility
  ))
}

save(random_inits, file = "D:/Manuscripts_localData/FrostBound_AQ/Results/gentoo-abundance-model/random_inits.rda")
expect_error(random_inits(model_data), NA)


n.chains <- 6
n.adapt <- 3000
n.update <- 300000
n.iter <- 200000
thin <- 200
cl <- makeCluster(n.chains)

cvars <- c("model_data", "n.adapt", "n.update", "n.iter", "thin", "params", "random_inits")
params <- c("beta", "sigma", "sigma_site", "sigma_season", "alpha", "epsilon", "eta", "z_c", "lz", 
            "gl_a", "l_a", "y_i_new", "y_n_new", "y_c_new", "y_n_sqs", "y_n_sqs_new", "y_i_sqs_new",
            "y_i_sqs", "y_c_sqs", "y_c_sqs_new", "lz_c", "lza")

parallel::clusterExport(cl, cvars)

out <- clusterEvalQ(cl, {
  library(rjags)
  inits <- random_inits(model_data)
  jm = jags.model("D:/Manuscripts_localData/FrostBound_AQ/Results/gentoo-abundance-model/jags_model.jags", data = model_data, n.chains = 1, n.adapt = n.adapt, 
                  inits = inits)
  update(jm, n.iter = n.update)
  zm = coda.samples(jm, variable.names = params, n.iter = n.iter, thin = thin)
  return(as.mcmc(zm))
})
stopCluster(cl)
model_data_rinits_output = mcmc.list(out)  
save(model_data_rinits_output, file = "D:/Manuscripts_localData/FrostBound_AQ/Results/gentoo-abundance-model/model_data_rinits_output.rda")


model_data_rinits_output <- read

MCMCsummary(model_data_rinits_output, params = c("beta", "sigma", "sigma_site", "sigma_season"), 
            HPD = TRUE, hpd_prob = .95, round = 3)

#Posterior Predictive Checks
params <- c("y_i_sqs", "y_i_sqs_new", "y_n_sqs", "y_n_sqs_new", "y_c_sqs", "y_c_sqs_new")
MCMCsummary(model_data_rinits_output, params = params, n.eff = FALSE, round = 3)
```




**EXTRACT Gentoo Growth Parameters**

```{r}

# Load required libraries
library(tidyverse)
library(coda)
library(mapppdr)
library(patchwork)
library(leaflet)
library(CCAMLRGIS)
library(rjags)
library(MCMCvis)
library(parallel)
library(stringi)
library(pander)
library(testthat)

# Define parameters
min_season <- 1970
max_season <- 2023
species <- "GEPE"

# Construct Presence-Absence Assumptions CSV for the JAGS model
penguin_obs <- mapppdr::penguin_obs

penguin_obs_processed <- penguin_obs %>%
  filter(species_id == species) %>%
  mutate(
    presence = ifelse(!is.na(count), 1, 0),
    known_w = 1) %>%
  select(site_id, season, presence, known_w, count, accuracy, type)

presence_absence_assumptions <- expand.grid(
  site_id = unique(penguin_obs$site_id),
  season = min_season:max_season
) %>%
  left_join(penguin_obs_processed, by = c("site_id", "season")) %>%
  mutate(
    presence = ifelse(is.na(presence), 0, presence),
    known_w = ifelse(is.na(known_w), 0, known_w),
    count = ifelse(is.na(count), 0, count)) %>%
  group_by(site_id, season) %>%
  arrange(desc(type), desc(accuracy), .by_group = TRUE) %>%
  slice(1) %>%
  ungroup() %>%
  select(site_id, season, presence, known_w, count, accuracy)

# Load the JAGS MCMC Output File
load("D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/model_data_rinits_output.rda")

# Extract the Gentoo Abundance Estimates (lz)
model_samples <- as.matrix(model_data_rinits_output)

# Filter columns that are logged latent abundance (lz) parameters
lz_columns <- grep("^lz\\[", colnames(model_samples))
lz_samples <- model_samples[, lz_columns]

# Convert log-abundances to actual abundances
abundance_samples <- exp(lz_samples)

# Summarize the actual abundances
abundance_summary <- apply(abundance_samples, 2, function(x) {
  c(mean = mean(x), median = median(x), 
    lower_95 = quantile(x, 0.025), upper_95 = quantile(x, 0.975))
})

# Convert to a readable data frame (use t to transpose)
abundance_summary_df <- as.data.frame(t(abundance_summary))

# Extract site and season info from the indices
extract_indices <- function(colname) {
  indices <- gsub("[^0-9,]", "", colname)
  as.integer(unlist(strsplit(indices, ",")))
}

indices <- lapply(colnames(abundance_samples), extract_indices)
sites <- sapply(indices, `[`, 1)
seasons <- sapply(indices, `[`, 2)

abundance_summary_df$site <- sites
abundance_summary_df$season <- seasons

abundance_summary_df <- abundance_summary_df %>% 
  rename(mean_abundance = mean,
         median_abundance = median,
         lower_95_abundance = "lower_95.2.5%",
         upper_95_abundance = "upper_95.97.5%")

head(abundance_summary_df)

# Load and prepare SiteList
SiteList <- mapppdr::penguin_obs %>%
  filter(count > 0 & species_id == species & season >= min_season & season <= max_season) %>%
  mutate(season_relative = season - min_season + 1) %>%
  group_by(site_id) %>%
  summarise(initial_season = min(season_relative)) %>%
  ungroup() %>%
  left_join(mapppdr::sites, by = "site_id") %>%
  mutate(site = as.numeric(as.factor(site_id))) %>%
  select(site_id, site_name, ccamlr_id, site, initial_season, latitude, longitude)

(n_sites <- nrow(SiteList))

SiteList %>% 
  distinct(site, site_id, latitude, longitude)

# Join SiteList with abundance_summary_df
final_data <- left_join(SiteList, abundance_summary_df, by = "site")

# Ensure final_data is sorted by site and season
final_data <- final_data %>%
  arrange(site, season)

# Calculate the growth rate and append it to the data
final_data <- final_data %>%
  group_by(site) %>%
  mutate(growth_rate = mean_abundance / lag(mean_abundance)) %>%
  ungroup()

# Adjust the season column in the final data
final_data <- final_data %>%
  mutate(year = 1970 + season - 1)

# Display the adjusted final_data
print(head(final_data))

write.csv(final_data, "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/modeled_gentoo_parameters.csv")
# Print the head of the final data with growth rates
print(head(final_data))

```


**CALCULATE GROWTH RATES:  and plot geometric mean of growth rates** 
```{r}
library(ggplot2)
library(dplyr)

# Calculate geometric mean of growth rates
final_data <- final_data %>%
  group_by(site) %>%
  mutate(geometric_mean_growth_rate = exp(mean(log(growth_rate), na.rm = TRUE))) %>%
  ungroup()

# Filter data for the specific site
site_data <- final_data %>% filter(site_id == "AITC")

# Calculate the mean growth multiplier for the site
mean_growth <- site_data %>% 
  summarize(mean_growth_multiplier = exp(mean(log(growth_rate), na.rm = TRUE)),
            lower_ci = exp(mean(log(growth_rate), na.rm = TRUE) - 1.96 * sd(log(growth_rate), na.rm = TRUE)/sqrt(n())),
            upper_ci = exp(mean(log(growth_rate), na.rm = TRUE) + 1.96 * sd(log(growth_rate), na.rm = TRUE)/sqrt(n())))

# Check if the 'count' and 'type' columns exist and correct them if needed
if (!"count" %in% names(site_data)) {
  site_data$count <- site_data$mean_abundance # or any other logic that fits
}

if (!"type" %in% names(site_data)) {
  site_data$type <- "nests" # default type, adjust as necessary
}

# Add year column based on season
site_data <- site_data %>%
  mutate(year = 1970 + season - 1)

# Generate the plot
ggplot(site_data, aes(x = year, y = mean_abundance)) +
  geom_boxplot(aes(group = year), fill = "orange", alpha = 0.6) +
  geom_errorbar(aes(ymin = lower_95_abundance, ymax = upper_95_abundance), width = 0.2) +
  geom_point(data = site_data %>% filter(!is.na(count)), aes(y = count, color = type), size = 3, shape = 21, fill = "blue") +
  scale_color_manual(values = c("nests" = "red", "chicks" = "blue")) +
  labs(title = "AITC, Barrientos Island (Aitcho Islands), 48.1",
       subtitle = paste0("mean population growth multiplier = ", round(mean_growth$mean_growth_multiplier, 3), 
                         " (", round(mean_growth$lower_ci, 2), " - ", round(mean_growth$upper_ci, 2), ")"),
       x = "Year",
       y = "Abundance",
       color = "Type") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))


```


**CALCULATE METRICS: Sea Ice concentration within home range and pass to dataframe**

```{r}



# This script analyzes daily sea ice concentration (SIC) and extent within defined home ranges by calculating the daily mean SIC, standard deviation of SIC, and total ice-covered area (sea ice extent) above a threshold for each day. It loads NSIDC sea ice data, applies a threshold to set low values to zero, and computes the specified metrics for each home range. The sea ice extent is calculated as the total area of cells with SIC above the threshold, effectively representing the "total ice-covered area" within the home range. The results are compiled and exported to a CSV file for further analysis.

# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(stringr)
library(lubridate)

# Function to compute daily mean SIC, SD SIC, and sea ice extent above a threshold
compute_daily_sic_statistics <- function(buffer_path, nsidc, threshold = 0.15, cell_area_sq_meters) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  
  # Extract dates before masking
  dates <- time(nsidc)
  
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  
  # Set all values < threshold to 0 for the entire raster stack
  buffer_mask <- app(buffer_mask, fun = function(x) { x[x < threshold] <- 0; return(x) })
  
  # Calculate mean and SD SIC excluding NAs for each layer
  mean_sic <- global(buffer_mask, fun = 'mean', na.rm = TRUE)[, 1]
  sd_sic <- global(buffer_mask, fun = 'sd', na.rm = TRUE)[, 1]
  
  # Calculate sea ice extent (total cells above threshold in square kilometers)
  valid_ice_cells <- global(buffer_mask >= threshold, fun = 'sum', na.rm = TRUE)[, 1]
  total_ice_area_sq_km <- (valid_ice_cells * cell_area_sq_meters) / 1e6  # Convert total ice area to square kilometers
  
  results <- data.frame(
    date = as.Date(dates, origin = "1970-01-01"),
    mean_sic = mean_sic,
    sd_sic = sd_sic,
    ice_extent_km2 = total_ice_area_sq_km
  )
  
  return(results)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds, nsidc_subset, results_dir) {
  # Initialize dataframe to store all results
  all_results_df <- data.frame()
  
  # Calculate the cell area in square meters (only once)
  cell_area_sq_meters <- prod(res(nsidc_subset))
  
  # Loop through each home range shapefile and compute results
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      metrics <- compute_daily_sic_statistics(buffer_path, nsidc_subset, threshold = threshold, cell_area_sq_meters = cell_area_sq_meters)
      
      metrics <- metrics %>%
        mutate(Threshold = threshold, HomeRangeSize = home_range_size)
      
      all_results_df <- bind_rows(all_results_df, metrics)
    }
  }
  
  # Export the compiled results to CSV
  write.csv(all_results_df, file.path(results_dir, "daily_sic_statistics.csv"), row.names = FALSE)
  
  return(all_results_df)
}

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1981 to 2023
start_date <- as.Date("1981-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Subset the NSIDC data for testing (e.g., first 100 layers)
# nsidc_subset <- subset(nsidc, 1:100)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
dir.create(results_dir, showWarnings = FALSE)

# Run the analysis on the subset
all_results_df <- analyze_sea_ice_effect(c(0.15), nsidc, results_dir)

# Display the results
head(all_results_df)




```



**CALCULATE METRICS: Persistence (Open Water Frequency) and Duration metrics and pass to dataframe**
```{r}

# Load necessary libraries
library(terra)
library(sf)
library(dplyr)
library(stringr)

# Function to compute mean duration, standard deviation, mean persistence, and persistence standard deviation of sea ice concentration above a threshold
compute_duration_persistence_stats <- function(buffer_path, nsidc, threshold = .15, winter_months = c(6, 7, 8, 9)) {
  combined_buffers_sf <- st_read(buffer_path)
  dissolved_buffer <- st_union(combined_buffers_sf)
  
  # Extract dates before masking
  dates <- time(nsidc)
  
  buffer_mask <- mask(nsidc, vect(dissolved_buffer))
  
  all_years <- unique(year(dates))
  
  duration_persistence_stats <- data.frame(
    year = integer(),
    month = character(),
    mean_duration = numeric(),
    sd_duration = numeric(),
    mean_persistence = numeric(),
    sd_persistence = numeric()
  )
  
  calculate_metrics <- function(data, threshold) {
    durations <- app(data, function(x) {
      rle_result <- rle(x > threshold)
      durations <- rle_result$lengths[rle_result$values]
      return(mean(durations, na.rm = TRUE))
    })
    mean_duration <- mean(values(durations), na.rm = TRUE)
    sd_duration <- sd(values(durations), na.rm = TRUE)
    
    open_water_prop <- app(data, function(x) mean(x < threshold, na.rm = TRUE))
    mean_persistence <- mean(values(open_water_prop), na.rm = TRUE)
    sd_persistence <- sd(values(open_water_prop), na.rm = TRUE)
    
    return(list(mean_duration = mean_duration, sd_duration = sd_duration, mean_persistence = mean_persistence, sd_persistence = sd_persistence))
  }
  
  for (year in all_years) {
    for (month in winter_months) {
      monthly_indices <- which(year(dates) == year & month(dates) == month)
      if (length(monthly_indices) > 0) {
        monthly_data <- buffer_mask[[monthly_indices]]
        metrics <- calculate_metrics(monthly_data, threshold)
        
        duration_persistence_stats <- rbind(duration_persistence_stats, data.frame(
          year = year,
          month = month,
          mean_duration = metrics$mean_duration,
          sd_duration = metrics$sd_duration,
          mean_persistence = metrics$mean_persistence,
          sd_persistence = metrics$sd_persistence
        ))
      }
    }
    
    season_indices <- which(year(dates) == year & month(dates) %in% winter_months)
    if (length(season_indices) > 0) {
      season_data <- buffer_mask[[season_indices]]
      metrics <- calculate_metrics(season_data, threshold)
      
      duration_persistence_stats <- rbind(duration_persistence_stats, data.frame(
        year = year,
        month = "Season-wide",
        mean_duration = metrics$mean_duration,
        sd_duration = metrics$sd_duration,
        mean_persistence = metrics$mean_persistence,
        sd_persistence = metrics$sd_persistence
      ))
    }
  }
  
  return(duration_persistence_stats)
}

# Main analysis function
analyze_sea_ice_effect <- function(thresholds, nsidc_subset, results_dir) {
  # Initialize dataframe to store all results
  all_duration_persistence_stats <- data.frame()
  
  # Define home range directory
  home_range_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/home-ranges"
  home_range_files <- list.files(home_range_dir, pattern = "\\.shp$", full.names = TRUE)
  
  # Loop through each home range shapefile and compute results
  for (threshold in thresholds) {
    for (buffer_path in home_range_files) {
      home_range_size <- str_extract(buffer_path, "(\\d+)km")
      duration_persistence_stats <- compute_duration_persistence_stats(buffer_path, nsidc_subset, threshold = threshold)
      duration_persistence_stats$Threshold <- threshold
      duration_persistence_stats$HomeRangeSize <- home_range_size
      all_duration_persistence_stats <- bind_rows(all_duration_persistence_stats, duration_persistence_stats)
    }
  }
  
  # Export the compiled results to CSV
  write.csv(all_duration_persistence_stats, file.path(results_dir, "sea_ice_duration_persistence_stats_subset.csv"), row.names = FALSE)
  
  # Export the compiled results to RDS
  saveRDS(all_duration_persistence_stats, file.path(results_dir, "sea_ice_duration_persistence_stats.rds"))
}

# Load the NSIDC sea ice concentration data
nsidc <- rast("D:/Manuscripts_localData/FrostBound_AQ/Datasets/dataset-harmonization/complete-harmonized-dataset/tif/nsidc_12_5km_harmonized_1979-2023.tif")

# Filter the sea ice data from 1981 to 2023
start_date <- as.Date("1981-01-01")
end_date <- as.Date("2023-09-30")
nsidc <- subset(nsidc, which(time(nsidc) >= start_date & time(nsidc) <= end_date))

# Subset the NSIDC data for testing (e.g., first 100 layers)
# nsidc_subset <- subset(nsidc, 1:100)

# Create results directory
results_dir <- "D:/Manuscripts_localData/FrostBound_AQ/Datasets/gentoo-abundance-model/metric-calculation-csv"
dir.create(results_dir, showWarnings = FALSE)

# Run the analysis on the subset
analyze_sea_ice_effect(c(.15, .30, .50), nsidc, results_dir)

# Display the results
all_duration_persistence_stats <- read.csv(file.path(results_dir, "sea_ice_duration_persistence_stats_subset.csv"))
head(all_duration_persistence_stats)


```




**Run & Visualize: GLS Regional Analysis**

```{r}
# ============================================================================
# Regional GLS Analysis for Gentoo Penguin Growth Rates vs Sea Ice Metrics
# Performs SEPARATE analyses for Bransfield and Central WAP regions
# Includes: SIC, Extent, Duration, and Persistence (Open Water Frequency)
# ============================================================================

library(nlme)
library(data.table)
library(dplyr)
library(ggplot2)
library(lubridate)
library(showtext)
library(patchwork)

# Suppress showtext warnings
suppressMessages({
  font <- "Gudea"
  font_add_google(family = font, font, db_cache = TRUE)
  showtext_auto()
})

theme_set(theme_minimal(base_family = font, base_size = 12))

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

# Function: Define latitude column
pick_lat_col <- function(df) {
  cand <- c("lat", "latitude", "colony_lat", "site_lat", "lat_dd")
  hit <- cand[cand %in% names(df)]
  if (length(hit) == 0) return(NULL)
  hit[1]
}

# Function: Safe year extraction from date strings
safe_year_from_date <- function(x) {
  x <- as.character(x)
  y <- suppressWarnings(year(mdy(x)))
  idx <- is.na(y)
  if (any(idx)) y[idx] <- suppressWarnings(year(dmy(x[idx])))
  idx <- is.na(y)
  if (any(idx)) y[idx] <- suppressWarnings(year(ymd(x[idx])))
  y
}

# Function: Aggregate metrics to yearly (handles both daily and monthly data)
yearly_aggregate_metric <- function(df, metric_col) {
  if ("date" %in% names(df)) {
    # Daily data → aggregate to year
    out <- df %>%
      mutate(year = safe_year_from_date(date)) %>%
      group_by(year) %>%
      summarise(metric_value = mean(.data[[metric_col]], na.rm = TRUE), .groups = "drop")
  } else if ("year" %in% names(df)) {
    # Already aggregated (monthly/seasonal) → average to year
    out <- df %>%
      group_by(year) %>%
      summarise(metric_value = mean(.data[[metric_col]], na.rm = TRUE), .groups = "drop")
  } else {
    stop("Input lacks both 'date' and 'year' columns; cannot aggregate.")
  }
  out
}

# Function: Fit GLS models for individual lag years (1-5)
fit_gls_models_indiv <- function(penguin_data, metric_yearly, metric_name) {
  results <- list()
  
  for (lag in 1:5) {
    # Lag the metric data and merge with penguin data
    metric_lagged <- metric_yearly %>%
      mutate(year = year + lag)
    
    penguin_lagged <- penguin_data %>%
      left_join(metric_lagged, by = "year") %>%
      filter(!is.na(metric_value), !is.na(growth_rate), growth_rate <= 3)
    
    if (nrow(penguin_lagged) < 3) next
    
    cat(sprintf("    Fitting GLS model for lag %d year(s)\n", lag))
    
    formula <- as.formula("growth_rate ~ metric_value")
    model <- gls(formula, correlation = corAR1(form = ~1 | site_id), data = penguin_lagged)
    summary_model <- summary(model)
    bonferroni_p_value <- p.adjust(summary_model$tTable[2, 4], method = "bonferroni", n = 5)
    
    if (bonferroni_p_value < 0.05) {
      results[[paste("Lag_Indiv", lag)]] <- list(
        AIC = AIC(model),
        BIC = BIC(model),
        coefficients = summary_model$tTable,
        p_value = summary_model$tTable[2, 4],
        bonferroni_p_value = bonferroni_p_value,
        model = model,
        penguin_data = penguin_lagged,
        lag = lag,
        metric_name = metric_name
      )
    }
  }
  
  return(results)
}

# Function: Create publication-quality plot
create_plot <- function(penguin_data, gls_model, intercept_ci, slope_ci, 
                        metric, x_label, title, region_label) {
  new_data <- data.frame(overwinter_metric = seq(min(penguin_data[[metric]], na.rm = TRUE),
                                                 max(penguin_data[[metric]], na.rm = TRUE),
                                                 length.out = 100))
  
  colnames(new_data) <- metric
  
  predicted_values <- predict(gls_model, new_data)
  
  new_data$fit <- predicted_values
  new_data$upper <- intercept_ci[3] + slope_ci[3] * new_data[[metric]]
  new_data$lower <- intercept_ci[1] + slope_ci[1] * new_data[[metric]]
  
  plot <- ggplot() +
    geom_point(data = penguin_data, aes_string(x = metric, y = "growth_rate"), 
               color = "black", size = 1, alpha = 0.6) +
    geom_line(data = new_data, aes_string(x = metric, y = "fit"), 
              color = "blue", size = 1) +
    geom_ribbon(data = new_data, aes_string(x = metric, ymin = "lower", ymax = "upper"), 
                alpha = 0.2, fill = "blue") +
    labs(title = paste(title, "-", region_label),
         x = x_label,
         y = "Growth Rate") +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      plot.caption = element_text(hjust = 0.5, size = 10),
      plot.margin = margin(20, 20, 20, 20)
    ) +
    ylim(NA, 1.5)
  
  return(plot)
}

# ============================================================================
# MAIN REGIONAL ANALYSIS FUNCTION
# ============================================================================

run_regional_gls_analysis <- function(penguin_abundance_data, 
                                      metric_files,
                                      results_dir,
                                      central_cutoff_lat = -63.2,
                                      regions = c("Bransfield", "Central_WAP"),
                                      lags = 1:5,
                                      min_rows_for_fit = 30,
                                      metrics_priority = c(
                                        # daily SIC/extent file
                                        "mean_sic", "sd_sic", "ice_extent_km2",
                                        # duration/persistence file
                                        "mean_duration", "sd_duration",
                                        "mean_persistence", "sd_persistence"
                                      )) {
  
  # Create main results directory
  dir.create(results_dir, showWarnings = FALSE, recursive = TRUE)
  
  # Identify latitude column
  lat_col <- pick_lat_col(penguin_abundance_data)
  if (is.null(lat_col)) {
    stop("No latitude column found in penguin data.")
  }
  
  # Add year and region columns to penguin data
  penguin_data_full <- penguin_abundance_data %>%
    mutate(
      year = 1970 + season - 1,
      .lat_raw = .data[[lat_col]],
      .lat_sgn = if_else(.lat_raw < 0, .lat_raw, -abs(.lat_raw)),
      region = if_else(.lat_sgn <= central_cutoff_lat, "Central_WAP", "Bransfield")
    ) %>%
    filter(growth_rate <= 3, !is.na(growth_rate))
  
  # LOOP THROUGH EACH REGION
  for (region_name in regions) {
    
    cat("\n", rep("=", 70), "\n")
    cat(sprintf("ANALYZING REGION: %s\n", region_name))
    cat(rep("=", 70), "\n\n")
    
    # Create region-specific directory
    region_dir <- file.path(results_dir, region_name)
    dir.create(region_dir, showWarnings = FALSE, recursive = TRUE)
    
    # Subset penguin data for this region
    penguin_data_region <- penguin_data_full %>%
      filter(region == region_name)
    
    cat(sprintf("  N colonies in %s: %d\n", region_name, n_distinct(penguin_data_region$site_id)))
    cat(sprintf("  N observations in %s: %d\n\n", region_name, nrow(penguin_data_region)))
    
    # Initialize results storage
    all_results_list <- list()
    
    # LOOP THROUGH EACH METRIC FILE
    for (f in metric_files) {
      cat(sprintf("\nProcessing file: %s\n", basename(f)))
      
      dat <- fread(f)
      
      # Normalize presence of grouping columns
      if (!"HomeRangeSize" %in% names(dat)) dat[, HomeRangeSize := "Unknown"]
      if (!"Threshold" %in% names(dat)) dat[, Threshold := NA_real_]
      
      # Which metric columns are in THIS file?
      metric_cols <- intersect(metrics_priority, names(dat))
      if (length(metric_cols) == 0) {
        warning(sprintf("  No recognized metric columns in file: %s", basename(f)))
        next
      }
      
      cat(sprintf("  Found metrics: %s\n", paste(metric_cols, collapse = ", ")))
      
      home_sizes <- sort(unique(dat$HomeRangeSize))
      thresholds <- sort(unique(dat$Threshold))
      
      # LOOP THROUGH HOME RANGE SIZES AND THRESHOLDS
      for (h in home_sizes) {
        for (th in thresholds) {
          
          df_ht <- dat %>% filter(HomeRangeSize == h, is.na(Threshold) | Threshold == th)
          if (nrow(df_ht) == 0) next
          
          cat(sprintf("  HomeRange: %s, Threshold: %s\n", h, as.character(th)))
          
          # LOOP THROUGH EACH METRIC COLUMN
          for (metric_col in metric_cols) {
            
            # Map metric_col to a friendly name
            metric_name <- dplyr::case_when(
              metric_col == "mean_sic"         ~ "SIC",
              metric_col == "sd_sic"           ~ "SIC_SD",
              metric_col == "ice_extent_km2"   ~ "Extent",
              metric_col == "mean_duration"    ~ "Duration",
              metric_col == "sd_duration"      ~ "Duration_SD",
              metric_col == "mean_persistence" ~ "Persistence",  # aka Open Water Frequency
              metric_col == "sd_persistence"   ~ "Persistence_SD",
              TRUE                              ~ metric_col
            )
            
            cat(sprintf("    Metric: %s (%s)\n", metric_name, metric_col))
            
            # Build yearly time series for this metric
            d_year <- tryCatch(
              yearly_aggregate_metric(df_ht, metric_col),
              error = function(e) {
                warning(sprintf("      Skipping %s: %s", metric_col, e$message))
                return(NULL)
              }
            )
            if (is.null(d_year)) next
            
            # Fit GLS models across lags
            gls_results <- fit_gls_models_indiv(penguin_data_region, d_year, metric_name)
            
            # Extract results to data frames
            if (length(gls_results) > 0) {
              for (lag_name in names(gls_results)) {
                result <- gls_results[[lag_name]]
                
                result_df <- data.frame(
                  Region = region_name,
                  File = basename(f),
                  Metric = metric_name,
                  MetricColumn = metric_col,
                  HomeRangeSize = h,
                  Threshold = th,
                  Lag = result$lag,
                  N_Obs = nrow(result$penguin_data),
                  N_Colonies = n_distinct(result$penguin_data$site_id),
                  AIC = result$AIC,
                  BIC = result$BIC,
                  Coefficient_Intercept = result$coefficients[1, 1],
                  StdError_Intercept = result$coefficients[1, 2],
                  tValue_Intercept = result$coefficients[1, 3],
                  pValue_Intercept = result$coefficients[1, 4],
                  Coefficient = result$coefficients[2, 1],
                  StdError = result$coefficients[2, 2],
                  tValue = result$coefficients[2, 3],
                  pValue = result$coefficients[2, 4],
                  Bonferroni_Adjusted_pValue = result$bonferroni_p_value,
                  stringsAsFactors = FALSE
                )
                
                all_results_list[[length(all_results_list) + 1]] <- result_df
              }
            }
          }
        }
      }
    }
    
    # Export compiled results for this region
    if (length(all_results_list) > 0) {
      all_results_df <- bind_rows(all_results_list)
      
      # Save all results
      write.csv(all_results_df, 
                file.path(region_dir, "model_results_all_metrics.csv"), 
                row.names = FALSE)
      cat(sprintf("\nSaved all results: %s\n", 
                  file.path(region_dir, "model_results_all_metrics.csv")))
      
      # Save significant results
      significant_results <- all_results_df %>%
        filter(Bonferroni_Adjusted_pValue < 0.05)
      
      if (nrow(significant_results) > 0) {
        write.csv(significant_results, 
                  file.path(region_dir, "model_results_significant.csv"), 
                  row.names = FALSE)
        cat(sprintf("Saved significant results: %s\n", 
                    file.path(region_dir, "model_results_significant.csv")))
      } else {
        cat(sprintf("No significant results for %s\n", region_name))
      }
    }
  }
  
  cat("\n", rep("=", 70), "\n")
  cat("REGIONAL ANALYSIS COMPLETE\n")
  cat(rep("=", 70), "\n")
}

# ============================================================================
# EXAMPLE USAGE
# ============================================================================

# Define file paths
penguin_path <- "C:/Users/michael.wethington.BRILOON/OneDrive - Biodiversity Research Institute/Documents/Manuscripts - Antarctica/FrostBound_AQ_temporary/gentoo-abundance-model/modeled_gentoo_parameters.csv"

metric_files <- c(
  "C:/Users/michael.wethington.BRILOON/OneDrive - Biodiversity Research Institute/Documents/Manuscripts - Antarctica/FrostBound_AQ_temporary/gentoo-abundance-model/metric-calculation-csv/daily_sic_statistics.csv",
  "C:/Users/michael.wethington.BRILOON/OneDrive - Biodiversity Research Institute/Documents/Manuscripts - Antarctica/FrostBound_AQ_temporary/gentoo-abundance-model/metric-calculation-csv/sea_ice_duration_persistence_stats.csv"
)

results_path <- "C:/Users/michael.wethington.BRILOON/OneDrive - Biodiversity Research Institute/Documents/Manuscripts - Antarctica/FrostBound_AQ_temporary/RStudioProject/Results/Regional_Analysis"

# Load penguin data
penguin_data <- fread(penguin_path)

# Run regional analysis (processes all metrics from both files)
run_regional_gls_analysis(
  penguin_abundance_data = penguin_data,
  metric_files = metric_files,
  results_dir = results_path,
  central_cutoff_lat = -63.2,
  regions = c("Bransfield", "Central_WAP"),
  lags = 1:5,
  min_rows_for_fit = 30
)

# ============================================================================
# VISUALIZATION: Create comparison plots for significant results
# ============================================================================

# Load font for plots
font <- "Gudea"
font_add_google(family = font, font, db_cache = TRUE)
showtext_auto()
theme_set(theme_minimal(base_family = font, base_size = 12))

# Function to create comparison plots between regions
create_regional_comparison_plots <- function(results_dir, 
                                             penguin_data_path,
                                             metric_files,
                                             central_cutoff_lat = -63.2) {
  
  cat("\n", rep("=", 70), "\n")
  cat("CREATING REGIONAL COMPARISON PLOTS\n")
  cat(rep("=", 70), "\n\n")
  
  # Load penguin data
  penguin_full <- fread(penguin_data_path)
  lat_col <- pick_lat_col(penguin_full)
  
  # Add regions
  penguin_full <- penguin_full %>%
    mutate(
      year = 1970 + season - 1,
      .lat_raw = .data[[lat_col]],
      .lat_sgn = if_else(.lat_raw < 0, .lat_raw, -abs(.lat_raw)),
      region = if_else(.lat_sgn <= central_cutoff_lat, "Central_WAP", "Bransfield")
    ) %>%
    filter(growth_rate <= 3, !is.na(growth_rate))
  
  # Check if significant results exist for both regions
  brans_sig_path <- file.path(results_dir, "Bransfield", "model_results_significant.csv")
  central_sig_path <- file.path(results_dir, "Central_WAP", "model_results_significant.csv")
  
  if (!file.exists(brans_sig_path) & !file.exists(central_sig_path)) {
    cat("No significant results found for either region. Skipping plots.\n")
    return(NULL)
  }
  
  # Load significant results from both regions
  sig_results_list <- list()
  
  if (file.exists(brans_sig_path)) {
    sig_results_list[["Bransfield"]] <- read.csv(brans_sig_path)
  }
  
  if (file.exists(central_sig_path)) {
    sig_results_list[["Central_WAP"]] <- read.csv(central_sig_path)
  }
  
  # Create plot directory
  plot_dir <- file.path(results_dir, "Comparison_Plots")
  dir.create(plot_dir, showWarnings = FALSE)
  
  # Get all unique metric/threshold/homerange/lag combinations across both regions
  all_combos <- bind_rows(sig_results_list) %>%
    select(Metric, HomeRangeSize, Threshold, Lag) %>%
    distinct() %>%
    arrange(Metric, Lag)
  
  cat(sprintf("Found %d unique significant metric combinations\n", nrow(all_combos)))
  
  # Load all metric data
  metric_data_list <- lapply(metric_files, function(f) {
    dat <- fread(f)
    dat$source_file <- basename(f)
    return(dat)
  })
  
  # For each combination, create side-by-side plots
  for (i in 1:nrow(all_combos)) {
    combo <- all_combos[i, ]
    
    cat(sprintf("\nPlot %d/%d: %s, HR=%s, Threshold=%s, Lag=%d\n",
                i, nrow(all_combos), combo$Metric, combo$HomeRangeSize, 
                combo$Threshold, combo$Lag))
    
    plot_list <- list()
    
    # Create plot for each region that has this significant result
    for (region_name in names(sig_results_list)) {
      
      # Check if this region has this specific result
      region_result <- sig_results_list[[region_name]] %>%
        filter(Metric == combo$Metric,
               HomeRangeSize == combo$HomeRangeSize,
               Threshold == combo$Threshold,
               Lag == combo$Lag)
      
      if (nrow(region_result) == 0) {
        cat(sprintf("  Skipping %s (not significant)\n", region_name))
        next
      }
      
      # Get the metric column name from the result
      metric_col <- region_result$MetricColumn[1]
      
      # Find the appropriate metric file
      metric_file <- metric_files[grepl(region_result$File[1], metric_files)]
      if (length(metric_file) == 0) next
      
      # Load and process metric data
      metric_dat <- fread(metric_file) %>%
        filter(HomeRangeSize == combo$HomeRangeSize,
               (is.na(Threshold) | Threshold == combo$Threshold))
      
      # Aggregate to yearly
      metric_yearly <- yearly_aggregate_metric(metric_dat, metric_col) %>%
        mutate(year = year + combo$Lag)
      
      # Merge with penguin data for this region
      penguin_region <- penguin_full %>%
        filter(region == region_name) %>%
        left_join(metric_yearly, by = "year") %>%
        filter(!is.na(metric_value), !is.na(growth_rate))
      
      if (nrow(penguin_region) == 0) {
        cat(sprintf("  No data available for %s after merging\n", region_name))
        next
      }
      
      # Check for sufficient variation in metric
      if (sd(penguin_region$metric_value, na.rm = TRUE) == 0) {
        cat(sprintf("  No variation in metric for %s, skipping\n", region_name))
        next
      }
      
      # Refit the model for plotting
      model <- gls(growth_rate ~ metric_value, 
                   correlation = corAR1(form = ~1 | site_id), 
                   data = penguin_region)
      
      # Try to get confidence intervals with error handling
      conf_int <- tryCatch(
        intervals(model),
        error = function(e) {
          warning(sprintf("Could not compute confidence intervals for %s: %s", 
                          region_name, e$message))
          return(NULL)
        }
      )
      
      if (is.null(conf_int)) {
        cat(sprintf("  Skipping plot for %s (confidence interval error)\n", region_name))
        next
      }
      
      intercept_ci <- conf_int$coef[1, ]
      slope_ci <- conf_int$coef[2, ]
      
      # Create prediction data
      new_data <- data.frame(
        metric_value = seq(min(penguin_region$metric_value, na.rm = TRUE),
                           max(penguin_region$metric_value, na.rm = TRUE),
                           length.out = 100)
      )
      
      new_data$fit <- predict(model, new_data)
      new_data$upper <- intercept_ci[3] + slope_ci[3] * new_data$metric_value
      new_data$lower <- intercept_ci[1] + slope_ci[1] * new_data$metric_value
      
      # Create plot
      p <- ggplot() +
        geom_point(data = penguin_region, 
                   aes(x = metric_value, y = growth_rate), 
                   color = "black", size = 1.5, alpha = 0.6) +
        geom_line(data = new_data, 
                  aes(x = metric_value, y = fit), 
                  color = "blue", linewidth = 1) +
        geom_ribbon(data = new_data, 
                    aes(x = metric_value, ymin = lower, ymax = upper), 
                    alpha = 0.2, fill = "blue") +
        labs(title = region_name,
             subtitle = sprintf("N=%d, p=%.4f", 
                                nrow(penguin_region),
                                region_result$Bonferroni_Adjusted_pValue[1]),
             x = combo$Metric,
             y = "Growth Rate") +
        theme_minimal() +
        theme(
          plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
          plot.subtitle = element_text(hjust = 0.5, size = 10),
          plot.margin = margin(10, 10, 10, 10)
        ) +
        ylim(NA, 1.5)
      
      plot_list[[region_name]] <- p
    }
    
    # If we have plots for both regions, combine them
    if (length(plot_list) > 0) {
      combined_plot <- wrap_plots(plot_list, ncol = length(plot_list)) +
        plot_annotation(
          title = sprintf("%s vs Growth Rate (Lag %d years)", combo$Metric, combo$Lag),
          subtitle = sprintf("HomeRange: %s, Threshold: %s", 
                             combo$HomeRangeSize, combo$Threshold),
          theme = theme(
            plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            plot.subtitle = element_text(size = 12, hjust = 0.5)
          )
        )
      
      # Save plot
      plot_filename <- sprintf("%s_HR%s_Thr%s_Lag%d.png",
                               gsub(" ", "_", combo$Metric),
                               combo$HomeRangeSize,
                               combo$Threshold,
                               combo$Lag)
      
      tryCatch({
        suppressWarnings({
          ggsave(file.path(plot_dir, plot_filename),
                 plot = combined_plot,
                 width = 4 * length(plot_list),
                 height = 5,
                 dpi = 300)
        })
        cat(sprintf("  Saved: %s\n", plot_filename))
      }, error = function(e) {
        warning(sprintf("Could not save plot %s: %s", plot_filename, e$message))
      })
    }
  }
  
  cat("\n", rep("=", 70), "\n")
  cat("PLOT GENERATION COMPLETE\n")
  cat(rep("=", 70), "\n")
}

# Run visualization (wrapped in tryCatch to handle errors gracefully)
tryCatch({
  create_regional_comparison_plots(
    results_dir = results_path,
    penguin_data_path = penguin_path,
    metric_files = metric_files,
    central_cutoff_lat = -63.2
  )
}, error = function(e) {
  cat("\nError during plot generation:", e$message, "\n")
  cat("Continuing with summary table...\n")
})

# ============================================================================
# SUMMARY TABLE: Compare effect sizes between regions
# ============================================================================

create_regional_comparison_table <- function(results_dir) {
  
  cat("\n", rep("=", 70), "\n")
  cat("CREATING REGIONAL COMPARISON SUMMARY TABLE\n")
  cat(rep("=", 70), "\n\n")
  
  # Load significant results from both regions
  brans_sig_path <- file.path(results_dir, "Bransfield", "model_results_significant.csv")
  central_sig_path <- file.path(results_dir, "Central_WAP", "model_results_significant.csv")
  
  results_list <- list()
  
  if (file.exists(brans_sig_path)) {
    results_list[["Bransfield"]] <- read.csv(brans_sig_path)
  }
  
  if (file.exists(central_sig_path)) {
    results_list[["Central_WAP"]] <- read.csv(central_sig_path)
  }
  
  if (length(results_list) == 0) {
    cat("No significant results found. Cannot create comparison table.\n")
    return(NULL)
  }
  
  # Combine all results
  all_results <- bind_rows(results_list)
  
  # Create comparison summary
  comparison <- all_results %>%
    group_by(Region, Metric, Lag) %>%
    summarise(
      N_Models = n(),
      Mean_Coefficient = mean(Coefficient, na.rm = TRUE),
      SD_Coefficient = sd(Coefficient, na.rm = TRUE),
      Mean_pValue = mean(pValue, na.rm = TRUE),
      Min_pValue = min(pValue, na.rm = TRUE),
      Mean_N_Obs = mean(N_Obs, na.rm = TRUE),
      Mean_N_Colonies = mean(N_Colonies, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(Metric, Lag, Region)
  
  # Save comparison table
  write.csv(comparison, 
            file.path(results_dir, "Regional_Comparison_Summary.csv"),
            row.names = FALSE)
  
  cat("Saved summary table: Regional_Comparison_Summary.csv\n")
  
  # Print key comparisons
  cat("\n=== KEY COMPARISONS ===\n")
  
  # For each metric, show if both regions have significant effects
  metrics_in_both <- comparison %>%
    group_by(Metric, Lag) %>%
    filter(n() == 2) %>%  # Both regions present
    ungroup()
  
  if (nrow(metrics_in_both) > 0) {
    cat("\nMetrics significant in BOTH regions:\n")
    for (met in unique(metrics_in_both$Metric)) {
      cat(sprintf("\n%s:\n", met))
      subset_met <- metrics_in_both %>% filter(Metric == met)
      print(subset_met %>% select(Region, Lag, Mean_Coefficient, Min_pValue, Mean_N_Colonies))
    }
  }
  
  # Show metrics only in one region
  metrics_single <- comparison %>%
    group_by(Metric, Lag) %>%
    filter(n() == 1) %>%
    ungroup()
  
  if (nrow(metrics_single) > 0) {
    cat("\n\nMetrics significant in ONLY ONE region:\n")
    print(metrics_single %>% select(Region, Metric, Lag, Mean_Coefficient, Min_pValue))
  }
  
  cat("\n", rep("=", 70), "\n")
  
  return(comparison)
}

# ============================================================================
# FOREST PLOTS: Regional Comparison of Effect Sizes
# ============================================================================

create_regional_forest_plots <- function(results_dir) {
  
  cat("\n", rep("=", 70), "\n")
  cat("CREATING REGIONAL COMPARISON FOREST PLOTS\n")
  cat(rep("=", 70), "\n\n")
  
  # Load significant results from both regions
  brans_sig_path <- file.path(results_dir, "Bransfield", "model_results_significant.csv")
  central_sig_path <- file.path(results_dir, "Central_WAP", "model_results_significant.csv")
  
  results_list <- list()
  
  if (file.exists(brans_sig_path)) {
    results_list[["Bransfield"]] <- read.csv(brans_sig_path)
  }
  
  if (file.exists(central_sig_path)) {
    results_list[["Central_WAP"]] <- read.csv(central_sig_path)
  }
  
  if (length(results_list) == 0) {
    cat("No significant results found. Cannot create forest plots.\n")
    return(NULL)
  }
  
  # Combine all results
  all_results <- bind_rows(results_list)
  
  # Create forest plot directory
  forest_dir <- file.path(results_dir, "Forest_Plots")
  dir.create(forest_dir, showWarnings = FALSE)
  
  # Recode Lag column for better labels
  all_results$Lag_Label <- recode(all_results$Lag,
                                  `1` = '1 Year Lag',
                                  `2` = '2 Year Lag',
                                  `3` = '3 Year Lag',
                                  `4` = '4 Year Lag',
                                  `5` = '5 Year Lag')
  
  # Recode Metric names for better labels
  all_results$Metric_Label <- recode(all_results$Metric,
                                     'SIC' = 'Sea Ice Concentration',
                                     'SIC_SD' = 'SIC Variability',
                                     'Extent' = 'Sea Ice Extent',
                                     'Duration' = 'Sea Ice Duration',
                                     'Duration_SD' = 'Duration Variability',
                                     'Persistence' = 'Open Water Frequency',
                                     'Persistence_SD' = 'Open Water Freq. Variability')
  
  # ========================================================================
  # PLOT 1: Combined forest plot for all metrics (faceted by metric)
  # ========================================================================
  cat("\nCreating combined forest plot across all metrics...\n")
  
  combined_forest <- ggplot(all_results, 
                            aes(x = HomeRangeSize, y = Coefficient, 
                                color = Region, shape = Region)) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", alpha = 0.5) +
    geom_point(position = position_dodge(width = 0.5), size = 2) +
    geom_errorbar(aes(ymin = Coefficient - StdError, 
                      ymax = Coefficient + StdError),
                  position = position_dodge(width = 0.5),
                  width = 0.3,
                  linewidth = 0.5) +
    facet_wrap(~ Metric_Label + Lag_Label, scales = 'free', ncol = 5) +
    scale_color_manual(values = c("Bransfield" = "#2166AC", 
                                  "Central_WAP" = "#B2182B")) +
    scale_shape_manual(values = c("Bransfield" = 16, 
                                  "Central_WAP" = 17)) +
    labs(title = "Regional Comparison: Effect of Sea Ice Metrics on Gentoo Growth Rates",
         subtitle = "Bransfield (South Shetland Islands) vs Central WAP (Gerlache Strait/Anvers)",
         x = "Home Range Size",
         y = "Effect Size (Coefficient ± SE)",
         color = "Region",
         shape = "Region") +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      axis.text.y = element_text(size = 8),
      strip.text = element_text(size = 9, face = "bold"),
      plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(size = 11, hjust = 0.5),
      legend.position = "bottom",
      panel.grid.minor = element_blank()
    )
  
  # Save combined forest plot
  ggsave(file.path(forest_dir, "Regional_Comparison_Combined_Forest_Plot.png"),
         combined_forest, width = 18, height = 12, dpi = 300)
  
  ggsave(file.path(forest_dir, "Regional_Comparison_Combined_Forest_Plot.pdf"),
         combined_forest, width = 18, height = 12)
  
  cat("  Saved: Regional_Comparison_Combined_Forest_Plot.png/pdf\n")
  
  # ========================================================================
  # PLOT 2: Individual forest plots for each metric
  # ========================================================================
  cat("\nCreating individual forest plots for each metric...\n")
  
  unique_metrics <- unique(all_results$Metric_Label)
  
  for (metric in unique_metrics) {
    
    metric_data <- all_results %>% filter(Metric_Label == metric)
    
    if (nrow(metric_data) == 0) next
    
    cat(sprintf("  Creating plot for: %s\n", metric))
    
    metric_forest <- ggplot(metric_data,
                            aes(x = HomeRangeSize, y = Coefficient,
                                color = Region, shape = Region)) +
      geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", alpha = 0.5) +
      geom_point(position = position_dodge(width = 0.5), size = 3) +
      geom_errorbar(aes(ymin = Coefficient - StdError,
                        ymax = Coefficient + StdError),
                    position = position_dodge(width = 0.5),
                    width = 0.3,
                    linewidth = 0.7) +
      facet_wrap(~ Lag_Label, scales = 'free_x', nrow = 1) +
      scale_color_manual(values = c("Bransfield" = "#2166AC",
                                    "Central_WAP" = "#B2182B")) +
      scale_shape_manual(values = c("Bransfield" = 16,
                                    "Central_WAP" = 17)) +
      labs(title = paste("Regional Comparison:", metric),
           subtitle = "Effect on Gentoo Penguin Growth Rates",
           x = "Home Range Size",
           y = "Effect Size (Coefficient ± SE)",
           color = "Region",
           shape = "Region") +
      theme_minimal() +
      theme(
        axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 11, hjust = 0.5),
        strip.text = element_text(size = 11, face = "bold"),
        legend.position = "bottom",
        panel.grid.minor = element_blank()
      )
    
    # Save individual metric plot
    metric_filename <- gsub(" ", "_", tolower(metric))
    ggsave(file.path(forest_dir, paste0("Regional_Forest_", metric_filename, ".png")),
           metric_forest, width = 12, height = 6, dpi = 300)
    
    ggsave(file.path(forest_dir, paste0("Regional_Forest_", metric_filename, ".pdf")),
           metric_forest, width = 12, height = 6)
  }
  
  # ========================================================================
  # PLOT 3: Forest plot showing only metrics significant in BOTH regions
  # ========================================================================
  cat("\nCreating forest plot for metrics significant in both regions...\n")
  
  # Find metrics present in both regions
  metrics_both <- all_results %>%
    group_by(Metric_Label, Lag_Label, HomeRangeSize, Threshold) %>%
    summarise(n_regions = n_distinct(Region), .groups = "drop") %>%
    filter(n_regions == 2)
  
  if (nrow(metrics_both) > 0) {
    
    both_data <- all_results %>%
      semi_join(metrics_both, by = c("Metric_Label", "Lag_Label", 
                                     "HomeRangeSize", "Threshold"))
    
    both_forest <- ggplot(both_data,
                          aes(x = HomeRangeSize, y = Coefficient,
                              color = Region, shape = Region)) +
      geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", alpha = 0.5) +
      geom_point(position = position_dodge(width = 0.5), size = 3) +
      geom_errorbar(aes(ymin = Coefficient - StdError,
                        ymax = Coefficient + StdError),
                    position = position_dodge(width = 0.5),
                    width = 0.3,
                    linewidth = 0.7) +
      facet_wrap(~ Metric_Label + Lag_Label, scales = 'free', ncol = 4) +
      scale_color_manual(values = c("Bransfield" = "#2166AC",
                                    "Central_WAP" = "#B2182B")) +
      scale_shape_manual(values = c("Bransfield" = 16,
                                    "Central_WAP" = 17)) +
      labs(title = "Metrics Significant in BOTH Regions",
           subtitle = "Direct comparison of effect sizes where both regions show significance",
           x = "Home Range Size",
           y = "Effect Size (Coefficient ± SE)",
           color = "Region",
           shape = "Region") +
      theme_minimal() +
      theme(
        axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 11, hjust = 0.5),
        strip.text = element_text(size = 9, face = "bold"),
        legend.position = "bottom",
        panel.grid.minor = element_blank()
      )
    
    ggsave(file.path(forest_dir, "Regional_Forest_Both_Regions.png"),
           both_forest, width = 14, height = 10, dpi = 300)
    
    ggsave(file.path(forest_dir, "Regional_Forest_Both_Regions.pdf"),
           both_forest, width = 14, height = 10)
    
    cat("  Saved: Regional_Forest_Both_Regions.png/pdf\n")
  } else {
    cat("  No metrics significant in both regions\n")
  }
  
  # ========================================================================
  # PLOT 4: Simplified forest plot by Lag only (averaged across home ranges)
  # ========================================================================
  cat("\nCreating simplified forest plot (averaged by lag)...\n")
  
  lag_summary <- all_results %>%
    group_by(Region, Metric_Label, Lag_Label) %>%
    summarise(
      Mean_Coefficient = mean(Coefficient, na.rm = TRUE),
      SE_Coefficient = sqrt(mean(StdError^2, na.rm = TRUE)),  # Pool SEs
      N_Models = n(),
      .groups = "drop"
    )
  
  lag_forest <- ggplot(lag_summary,
                       aes(x = Lag_Label, y = Mean_Coefficient,
                           color = Region, shape = Region)) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", alpha = 0.5) +
    geom_point(position = position_dodge(width = 0.5), size = 3.5) +
    geom_errorbar(aes(ymin = Mean_Coefficient - SE_Coefficient,
                      ymax = Mean_Coefficient + SE_Coefficient),
                  position = position_dodge(width = 0.5),
                  width = 0.3,
                  linewidth = 0.8) +
    facet_wrap(~ Metric_Label, scales = 'free_y', ncol = 4) +
    scale_color_manual(values = c("Bransfield" = "#2166AC",
                                  "Central_WAP" = "#B2182B")) +
    scale_shape_manual(values = c("Bransfield" = 16,
                                  "Central_WAP" = 17)) +
    labs(title = "Regional Comparison: Average Effect Sizes by Lag",
         subtitle = "Mean coefficients across all home ranges",
         x = "Lag Period",
         y = "Mean Effect Size (Coefficient ± SE)",
         color = "Region",
         shape = "Region") +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
      plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(size = 11, hjust = 0.5),
      strip.text = element_text(size = 10, face = "bold"),
      legend.position = "bottom",
      panel.grid.minor = element_blank()
    )
  
  ggsave(file.path(forest_dir, "Regional_Forest_Simplified_by_Lag.png"),
         lag_forest, width = 14, height = 8, dpi = 300)
  
  ggsave(file.path(forest_dir, "Regional_Forest_Simplified_by_Lag.pdf"),
         lag_forest, width = 14, height = 8)
  
  cat("  Saved: Regional_Forest_Simplified_by_Lag.png/pdf\n")
  
  cat("\n", rep("=", 70), "\n")
  cat("FOREST PLOT GENERATION COMPLETE\n")
  cat(rep("=", 70), "\n")
  
  return(all_results)
}

forest_results <- create_regional_forest_plots(results_path)


# Create summary table (wrapped in tryCatch)
tryCatch({
  regional_comparison <- create_regional_comparison_table(results_path)
  
  cat("\n\n")
  cat(rep("=", 70), "\n")
  cat("REGIONAL ANALYSIS WORKFLOW COMPLETE!\n")
  cat(rep("=", 70), "\n")
  cat("\nOutputs saved in:", results_path, "\n")
  cat("  - Bransfield/: Results for South Shetland Islands region\n")
  cat("  - Central_WAP/: Results for Gerlache Strait/Anvers region\n")
  cat("  - Comparison_Plots/: Side-by-side visualizations\n")
  cat("  - Regional_Comparison_Summary.csv: Statistical comparison table\n")
}, error = function(e) {
  cat("\nError creating summary table:", e$message, "\n")
  cat("\nPartial results may still be available in regional subdirectories.\n")
})



```





**RUN: SOUTHERN OSCILLATION INDEX MIXED EFFECT MODEL**

```{r updated mixed effect  model}
###############################################################################
# SOI Analysis for Gentoo Penguins
###############################################################################
library(httr)       # for GET()
library(dplyr)
library(tidyr)      # for pivot_longer()
library(lubridate)
library(nlme)       # for mixed-effects models (lme)
library(readr)      # for reading CSVs
library(ggplot2)    # for plotting
library(tibble)     # for rownames_to_column()
library(knitr)      # for kable()
library(lme4)       # for lmer() models and bootMer()
library(gridExtra)  # for grid.arrange()
library(viridis)    # for better color palettes
library(xtable)     # for LaTeX table output
library(scales)     # for better axis formatting

# Set output directory
output_dir <- "C:/Users/michael.wethington.BRILOON/OneDrive - Biodiversity Research Institute/Documents/Manuscripts - Antarctica/FrostBound_AQ_temporary/gentoo-abundance-model/soi-analysis-results"

# Create directory if it doesn't exist
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

###############################################################################
# Define Data Directory and Load Data
###############################################################################
data_dir <- "C:/Users/michael.wethington.BRILOON/OneDrive - Biodiversity Research Institute/Documents/Manuscripts - Antarctica/FrostBound_AQ_temporary/gentoo-abundance-model/"
gentoo_params <- read_csv(file.path(data_dir, "modeled_gentoo_parameters.csv"))
str(gentoo_params)

###############################################################################
# Download and Parse SOI Data from NOAA
###############################################################################
url <- "https://www.cpc.ncep.noaa.gov/data/indices/soi"
soi_raw <- tryCatch(GET(url), error = function(e) { stop("Error fetching SOI data: ", e) })
soi_text <- content(soi_raw, "text")
soi_lines <- strsplit(soi_text, "\n")[[1]]
standardized_soi_start <- grep("STANDARDIZED    DATA", soi_lines)
soi_standardized_lines <- soi_lines[(standardized_soi_start + 3):length(soi_lines)]
soi_standardized_lines <- soi_standardized_lines[!grepl("-999.9", soi_standardized_lines)]

soi_data <- read.table(text = soi_standardized_lines, fill = TRUE, stringsAsFactors = FALSE)
colnames(soi_data) <- c("Year", month.abb)
soi_data <- soi_data %>% mutate(Year = as.integer(Year))

soi_data_long <- soi_data %>%
  pivot_longer(-Year, names_to = "Month", values_to = "SOI") %>%
  mutate(
    Month = match(Month, month.abb),
    SOI   = as.numeric(SOI)
  ) %>%
  filter(!is.na(SOI))

# Optionally save the processed data for future use
saveRDS(soi_data_long, file = file.path(data_dir, "soi_data_long.rds"))
head(soi_data_long)

###############################################################################
# Prepare Winter SOI and Create Lagged Variables (June–Sept)
###############################################################################
winter_soi <- soi_data_long %>%
  filter(Month %in% c(6, 7, 8, 9)) %>%
  group_by(Year) %>%
  summarise(winter_soi = mean(SOI, na.rm = TRUE), .groups = "drop")

max_lag <- 5
winter_soi_lags <- winter_soi
for (lag_i in 1:max_lag) {
  lag_col <- paste0("winter_soi_lag", lag_i)
  winter_soi_lags <- winter_soi_lags %>%
    mutate(!!lag_col := dplyr::lag(winter_soi, n = lag_i, order_by = Year))
}
head(winter_soi_lags)

###############################################################################
# Merge SOI Lags into Gentoo Growth Data
###############################################################################
merged_data <- gentoo_params %>%
  left_join(winter_soi_lags, by = c("year" = "Year"))
merged_data_complete <- merged_data %>% filter(!is.na(winter_soi_lag1))
head(merged_data_complete)

###############################################################################
# Fit Mixed-Effects Models for Different Lag Terms using nlme
###############################################################################
cat("Fitting models for different lags...\n")
model_list <- list()
for (lag in 1:5) {
  cat("Fitting model for lag", lag, "...\n")
  lag_term <- paste0("winter_soi_lag", lag)
  fixed_formula <- as.formula(paste("growth_rate ~", lag_term))
  random_formula <- as.formula(paste("~", lag_term, "| site"))
  
  model <- lme(
    fixed = fixed_formula,
    random = random_formula,
    correlation = corAR1(form = ~ year | site),
    data = merged_data,
    na.action = na.omit,
    control = lmeControl(opt = "optim", msMaxIter = 500)
  )
  
  model_list[[lag]] <- model
  cat("  Lag", lag, "AIC:", AIC(model), "\n")
}

# Define the slopes model for use in comparisons
model_lme_slopes <- model_list[[1]]  # The lag 1 model becomes our slopes model
cat("Model list created with", length(model_list), "models\n")

###############################################################################
# Refit a Model Using lmer (for Bootstrapping with bootMer)
###############################################################################
cat("Fitting lmer model for bootstrapping...\n")
model_lmer <- lmer(growth_rate ~ winter_soi_lag1 + (winter_soi_lag1 | site),
                   data = merged_data, REML = TRUE)
summary_capture <- capture.output(summary(model_lmer))
writeLines(summary_capture, file.path(output_dir, "lmer_model_summary.txt"))

cat("Running bootstrap analysis...\n")
boot_results <- bootMer(
  model_lmer,
  FUN = function(mod) fixef(mod),
  nsim = 1000
)
boot_capture <- capture.output(print(boot_results))
writeLines(boot_capture, file.path(output_dir, "bootstrap_results.txt"))

###############################################################################
# Compare Random-Intercept vs. Random-Slope Models
###############################################################################
cat("Fitting random-intercept model...\n")
model_random_intercept <- lme(
  fixed = growth_rate ~ winter_soi_lag1,
  random = ~ 1 | site,
  correlation = corAR1(form = ~ year | site),
  data = merged_data,
  na.action = na.omit,
  control = lmeControl(opt = "optim", msMaxIter = 500)
)
ri_summary <- summary(model_random_intercept)
ri_capture <- capture.output(print(ri_summary))
writeLines(ri_capture, file.path(output_dir, "random_intercept_model_summary.txt"))
cat("Random-Intercept Model AIC:", AIC(model_random_intercept), "\n")

# Compare via likelihood ratio test
cat("Comparing models via likelihood ratio test...\n")
anova_results <- anova(model_random_intercept, model_lme_slopes)
anova_capture <- capture.output(print(anova_results))
writeLines(anova_capture, file.path(output_dir, "model_comparison_anova.txt"))

###############################################################################
# Extract and compile results from all models
###############################################################################
cat("Extracting results from all models...\n")
model_summary <- data.frame(
  Lag = 1:5,
  AIC = sapply(model_list, AIC),
  BIC = sapply(model_list, BIC)
)

# Extract fixed effects from each model
for (i in 1:5) {
  # Get model summary
  mod_summary <- summary(model_list[[i]])
  
  # Get the fixed effects table
  tbl <- mod_summary$tTable
  
  # Extract the coefficient for the appropriate lag term
  lag_term <- paste0("winter_soi_lag", i)
  
  # Assign values to the data frame
  model_summary$Coefficient[i] <- tbl[lag_term, "Value"]
  model_summary$StdError[i] <- tbl[lag_term, "Std.Error"]
  model_summary$tValue[i] <- tbl[lag_term, "t-value"]
  model_summary$pValue[i] <- tbl[lag_term, "p-value"]
  model_summary$RandomSD[i] <- as.numeric(VarCorr(model_list[[i]])[1, 2])
  
  # Write full model summary to file
  writeLines(capture.output(print(mod_summary)), 
             file.path(output_dir, paste0("model_lag_", i, "_summary.txt")))
}

# Print and save model summary
print(model_summary)
write.csv(model_summary, file.path(output_dir, "soi_models_summary.csv"), row.names = FALSE)

###############################################################################
# Define Nature-style theme for publication-quality figures
###############################################################################
# Define color palette for Nature style
nature_blue <- "#0072B2"    # Primary blue
nature_red <- "#D55E00"     # Primary red/orange
nature_green <- "#009E73"   # Secondary green
nature_purple <- "#CC79A7"  # Secondary purple
nature_orange <- "#E69F00"  # Secondary orange

# Custom theme function for Nature-style plots
nature_theme <- function() {
  theme_minimal() +
    theme(
      # Text elements
      text = element_text(color = "#000000"),
      plot.title = element_text(size = 11, face = "plain", hjust = 0),
      plot.subtitle = element_text(size = 9, hjust = 0, margin = margin(b = 15)),
      axis.title = element_text(size = 9, face = "plain"),
      axis.text = element_text(size = 8),
      axis.text.x = element_text(margin = margin(t = 5)),
      axis.text.y = element_text(margin = margin(r = 5)),
      
      # Grid elements
      panel.grid.major = element_line(color = "#E5E5E5", size = 0.2),
      panel.grid.minor = element_blank(),
      
      # Plot aesthetics
      panel.background = element_rect(fill = "white", color = NA),
      plot.background = element_rect(fill = "white", color = NA),
      
      # Legend elements
      legend.position = "bottom",
      legend.title = element_text(size = 8),
      legend.text = element_text(size = 7),
      legend.key.size = unit(0.5, "cm"),
      legend.margin = margin(t = 5, b = 5),
      
      # Margins and layout
      plot.margin = margin(15, 15, 15, 15),
      
      # Caption
      plot.caption = element_text(size = 8, hjust = 0, face = "plain", margin = margin(t = 10))
    )
}

###############################################################################
# Create enhanced publication-quality figure: Effect sizes across lag times
###############################################################################
cat("Creating publication-quality effect size plot...\n")

# Add significance annotation
model_summary$sig <- ifelse(model_summary$pValue < 0.001, "***", 
                            ifelse(model_summary$pValue < 0.01, "**",
                                   ifelse(model_summary$pValue < 0.05, "*", "ns")))

# Calculate 95% confidence intervals
model_summary$ci_lower <- model_summary$Coefficient - 1.96 * model_summary$StdError
model_summary$ci_upper <- model_summary$Coefficient + 1.96 * model_summary$StdError

# Enhanced coefficient plot
lag_plot <- ggplot(model_summary, aes(x = Lag, y = Coefficient)) +
  # Add reference line at y=0
  geom_hline(yintercept = 0, linetype = "solid", color = "grey70", size = 0.3) +
  # Add error bars
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0.2, color = nature_blue, size = 0.4) +
  # Add points with significance coloring
  geom_point(aes(fill = factor(sig)), shape = 21, size = 3, stroke = 0.5, color = "white") +
  # Add connecting line
  geom_line(color = nature_blue, size = 0.4, alpha = 0.5) +
  # Set scale with good spacing
  scale_x_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5, 5.5)) +
  scale_y_continuous(limits = c(-0.06, 0.06), 
                     breaks = seq(-0.06, 0.06, 0.02),
                     labels = function(x) format(x, nsmall = 2)) +
  # Use the Nature color palette for significance
  scale_fill_manual(values = c("ns" = "#CCCCCC", "*" = "#56B4E9", 
                               "**" = "#0072B2", "***" = "#E69F00"),
                    name = "Significance", 
                    labels = c("ns" = "p ≥ 0.05", "*" = "p < 0.05", 
                               "**" = "p < 0.01", "***" = "p < 0.001")) +
  # Labels
  labs(x = "Time lag (years)",
       y = "SOI coefficient (effect on growth rate)",
       caption = "Figure 1 | SOI effect on Gentoo penguin population growth rates across time lags.") +
  # Apply Nature theme
  nature_theme() 

###############################################################################
# Create enhanced publication-quality figure: AIC plot
###############################################################################
cat("Creating publication-quality AIC plot...\n")

aic_plot <- ggplot(model_summary, aes(x = Lag, y = AIC)) +
  # Add points
  geom_point(size = 3, color = nature_red) +
  # Add connecting line
  geom_line(color = nature_red, size = 0.4, alpha = 0.5) +
  # Set scale with good spacing
  scale_x_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5, 5.5)) +
  scale_y_continuous(limits = c(1950, 2060), 
                     breaks = seq(1960, 2060, 20)) +
  # Labels
  labs(x = "Time lag (years)",
       y = "Akaike Information Criterion",
       caption = "Figure 2 | Model fit by lag time (lower AIC indicates better fit).") +
  # Apply Nature theme
  nature_theme()

###############################################################################
# Create enhanced publication-quality figure: SOI vs Growth Relationship
###############################################################################
cat("Creating publication-quality SOI-Growth relationship plot...\n")

# Filter data
scatter_data <- merged_data %>%
  filter(!is.na(winter_soi_lag1), !is.na(growth_rate))

# Create the enhanced relationship plot
soi_growth_plot <- ggplot(scatter_data, aes(x = winter_soi_lag1, y = growth_rate)) +
  # Add points
  geom_point(alpha = 0.6, size = 1.5, color = nature_blue) +
  # Add regression line with confidence interval
  geom_smooth(method = "lm", color = nature_red, fill = nature_red, alpha = 0.2, size = 0.5) +
  # Add better axis formatting
  scale_x_continuous(breaks = scales::pretty_breaks(n = 6)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
  # Labels
  labs(x = "Winter SOI (lag 1 year)",
       y = "Population growth rate",
       caption = "Figure 3 | Relationship between winter SOI (1-year lag) and Gentoo penguin growth.") +
  # Apply Nature theme
  nature_theme()

###############################################################################
# Create enhanced publication-quality figure: Geographical plot
###############################################################################
cat("Creating publication-quality geographical visualization...\n")

geo_plot <- ggplot(merged_data_complete, aes(x = longitude, y = latitude)) +
  # Add points with SOI coloring
  geom_point(aes(color = winter_soi_lag1), size = 2, alpha = 0.8) +
  # Use diverging color scale (blue-white-red)
  scale_color_gradient2(low = nature_blue, mid = "white", high = nature_red, 
                        midpoint = 0, name = "Winter SOI\n(lag 1 year)") +
  # Set scale with good spacing
  coord_fixed(ratio = 1) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 5)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +
  # Labels
  labs(x = "Longitude",
       y = "Latitude",
       caption = "Figure 4 | Geographical distribution of winter SOI values across Gentoo penguin colonies.") +
  # Apply Nature theme
  nature_theme() +
  theme(
    legend.position = c(0.85, 0.85),
    legend.background = element_rect(fill = alpha("white", 0.7), color = NA),
    legend.margin = margin(5, 5, 5, 5)
  )

###############################################################################
# Create enhanced publication-quality figure: Observed vs Fitted
###############################################################################
cat("Creating publication-quality observed vs. fitted plot...\n")

# Extract the fitted values and residuals from the final model
fitted_vals <- fitted(model_random_intercept)
resids      <- residuals(model_random_intercept)

# Reconstruct the "observed" values the model used
observed_vals <- fitted_vals + resids

# Put them in a dataframe with the same number of rows as the model
df_plot <- data.frame(
  observed = observed_vals,
  fitted   = fitted_vals
)

fit_plot <- ggplot(df_plot, aes(x = fitted, y = observed)) +
  geom_point(alpha = 0.6, size = 1.5, color = nature_blue) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = nature_red, size = 0.5) +
  labs(
    x = "Fitted growth rate",
    y = "Observed growth rate",
    caption = "Figure 5 | Model validation: observed versus fitted growth rates."
  ) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 6)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +
  nature_theme()

###############################################################################
# Create combined figure (panel layout for publication)
###############################################################################
cat("Creating combined panel figure for publication...\n")

combined_plot <- grid.arrange(
  lag_plot + theme(legend.position = "none"),
  aic_plot,
  soi_growth_plot,
  ncol = 1,
  heights = c(1, 0.8, 1),
  top = "Southern Oscillation Index effects on Gentoo penguin population growth"
)

###############################################################################
# Save all plots with publication-quality settings
###############################################################################

# Save the Nature-styled plots (PNG first for reliability)
ggsave(file.path(output_dir, "nature_soi_coefficient_by_lag.png"), lag_plot, 
       width = 89, height = 89, units = "mm", dpi = 600)

ggsave(file.path(output_dir, "nature_aic_by_lag.png"), aic_plot,
       width = 89, height = 89, units = "mm", dpi = 600)

ggsave(file.path(output_dir, "nature_soi_growth_relationship.png"), soi_growth_plot,
       width = 89, height = 89, units = "mm", dpi = 600)

ggsave(file.path(output_dir, "nature_geographical_distribution.png"), geo_plot,
       width = 89, height = 89, units = "mm", dpi = 600)

ggsave(file.path(output_dir, "nature_observed_vs_fitted.png"), fit_plot,
       width = 89, height = 89, units = "mm", dpi = 600)

ggsave(file.path(output_dir, "nature_combined_panels.png"), combined_plot,
       width = 183, height = 247, units = "mm", dpi = 600)

# Try to save PDFs with more robust device
tryCatch({
  ggsave(file.path(output_dir, "nature_soi_coefficient_by_lag.pdf"), lag_plot, 
         width = 89, height = 89, units = "mm", dpi = 600, device = cairo_pdf)
  
  ggsave(file.path(output_dir, "nature_aic_by_lag.pdf"), aic_plot,
         width = 89, height = 89, units = "mm", dpi = 600, device = cairo_pdf)
  
  ggsave(file.path(output_dir, "nature_soi_growth_relationship.pdf"), soi_growth_plot,
         width = 89, height = 89, units = "mm", dpi = 600, device = cairo_pdf)
  
  ggsave(file.path(output_dir, "nature_geographical_distribution.pdf"), geo_plot,
         width = 89, height = 89, units = "mm", dpi = 600, device = cairo_pdf)
  
  ggsave(file.path(output_dir, "nature_observed_vs_fitted.pdf"), fit_plot,
         width = 89, height = 89, units = "mm", dpi = 600, device = cairo_pdf)
  
  ggsave(file.path(output_dir, "nature_combined_panels.pdf"), combined_plot,
         width = 183, height = 247, units = "mm", dpi = 600, device = cairo_pdf)
  
  cat("PDF files saved successfully.\n")
}, error = function(e) {
  cat("Error saving PDF files:", e$message, "\n")
  cat("PNG files were saved successfully and can be used for submission.\n")
})

###############################################################################
# Also save originals for compatibility
###############################################################################

# Create and save the original plots as well
orig_lag_plot <- ggplot(model_summary, aes(x = Lag, y = Coefficient)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = Coefficient - 1.96 * StdError, 
                    ymax = Coefficient + 1.96 * StdError), width = 0.2) +
  geom_line(linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dotted", color = "gray50") +
  labs(title = "SOI Effect on Gentoo Penguin Growth by Lag Time",
       x = "Lag (Years)",
       y = "SOI Coefficient (Effect on Growth Rate)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12))

orig_aic_plot <- ggplot(model_summary, aes(x = Lag, y = AIC)) +
  geom_point(size = 3) +
  geom_line(linetype = "dashed") +
  labs(title = "Model Fit by Lag Time",
       x = "Lag (Years)",
       y = "AIC (lower is better)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12))

orig_soi_growth_plot <- ggplot(scatter_data, aes(x = winter_soi_lag1, y = growth_rate)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Relationship Between Winter SOI (1-year lag) and Gentoo Growth",
       x = "Winter SOI (1-year lag)",
       y = "Population Growth Rate") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size = 12))

# Save original versions too
ggsave(file.path(output_dir, "soi_coefficient_by_lag.png"), orig_lag_plot, width = 8, height = 6, dpi = 300)
ggsave(file.path(output_dir, "soi_aic_by_lag.png"), orig_aic_plot, width = 8, height = 6, dpi = 300)
ggsave(file.path(output_dir, "soi_growth_relationship.png"), orig_soi_growth_plot, width = 8, height = 6, dpi = 300)

# Save original combined plot
orig_combined_lag_plot <- grid.arrange(orig_lag_plot, orig_aic_plot, ncol = 1)
ggsave(file.path(output_dir, "soi_lag_combined.png"), orig_combined_lag_plot, width = 8, height = 10, dpi = 300)

###############################################################################
# Create LaTeX table of fixed effects
###############################################################################
cat("Creating fixed effects table...\n")
final_fixef <- as.data.frame(summary(model_random_intercept)$tTable)
# Save as CSV
write.csv(final_fixef, file.path(output_dir, "random_intercept_fixed_effects.csv"))

# Create LaTeX table
latex_fixef <- xtable(final_fixef, 
                      caption = "Fixed Effects for the Final Random-Intercept Model",
                      label = "tab:fixed_effects",
                      digits = 5)

latex_fixef_code <- capture.output(print(latex_fixef, 
                                         include.rownames = TRUE,
                                         caption.placement = "top"))

writeLines(latex_fixef_code, file.path(output_dir, "fixed_effects_latex.tex"))

# Create a nice table with kable
kable_output <- capture.output(kable(final_fixef, digits = 4,
                                     caption = "Fixed Effects for the Final Random-Intercept Model"))
writeLines(kable_output, file.path(output_dir, "fixed_effects_kable.txt"))

###############################################################################
# Generate LaTeX table of model comparisons
###############################################################################
cat("Creating model comparison table...\n")
# Create LaTeX table of model results
latex_table <- xtable(model_summary[, c("Lag", "Coefficient", "StdError", "tValue", "pValue", "AIC")], 
                      caption = "Effects of Southern Oscillation Index (SOI) on Gentoo Penguin Population Growth Rates",
                      label = "tab:soi_effects",
                      digits = c(0, 0, 4, 4, 2, 5, 1))

latex_code <- capture.output(print(latex_table, 
                                   include.rownames = FALSE,
                                   caption.placement = "top"))

writeLines(latex_code, file.path(output_dir, "soi_table_latex.tex"))

###############################################################################
# Comprehensive results summary
###############################################################################
cat("Creating comprehensive summary report...\n")
summary_file <- file.path(output_dir, "soi_analysis_summary.txt")
sink(summary_file)

cat("\n\nSOI ANALYSIS RESULTS SUMMARY\n")
cat("============================\n\n")

cat("1. Best models by AIC:\n")
cat("   Lag 5: AIC =", round(model_summary$AIC[5], 1), "\n")
cat("   Lag 1: AIC =", round(model_summary$AIC[1], 1), "\n\n")

cat("2. SOI effect on Gentoo growth (1-year lag):\n")
cat("   Coefficient =", round(model_summary$Coefficient[1], 4), 
    "±", round(model_summary$StdError[1], 4), "\n")
cat("   t-value =", round(model_summary$tValue[1], 2), 
    ", p-value =", format.pval(model_summary$pValue[1], digits = 3), "\n\n")

cat("3. SOI effect on Gentoo growth (5-year lag):\n")
cat("   Coefficient =", round(model_summary$Coefficient[5], 4), 
    "±", round(model_summary$StdError[5], 4), "\n")
cat("   t-value =", round(model_summary$tValue[5], 2), 
    ", p-value =", format.pval(model_summary$pValue[5], digits = 3), "\n\n")

cat("4. Random effects structure:\n")
cat("   Colony-specific intercept SD =", 
    round(as.numeric(VarCorr(model_random_intercept)[1, 2]), 4), "\n")
cat("   Residual SD =", 
    round(as.numeric(VarCorr(model_random_intercept)[2, 2]), 4), "\n\n")

cat("5. Interpretation:\n")
if (model_summary$pValue[1] < 0.05) {
  cat("   Positive SOI values (La Niña conditions) are associated with\n")
  cat("   increased Gentoo penguin growth rates in the following year.\n")
  cat("   For each unit increase in SOI, growth rates increase by approximately\n")
  cat("  ", round(model_summary$Coefficient[1], 3), "units.\n\n")
} else {
  cat("   No significant relationship between SOI and growth rates at lag 1.\n\n")
}

cat("6. Random slopes vs. random intercepts:\n")
cat("   The random slopes model (AIC =", round(AIC(model_lme_slopes), 1), 
    ") performs slightly better than\n")
cat("   the random intercepts model (AIC =", round(AIC(model_random_intercept), 1), ").\n")
cat("   The correlation between random intercepts and slopes is", 
    round(VarCorr(model_lme_slopes)[3, 2], 3), ".\n\n")

cat("This analysis supports the hypothesis that large-scale climate oscillations\n")
cat("influence Gentoo penguin population dynamics, likely through their effects\n")
cat("on sea ice conditions along the Western Antarctic Peninsula.\n")

# Modified section to fix the error:
cat("6. Random slopes vs. random intercepts:\n")
cat("   The random slopes model (AIC =", round(AIC(model_lme_slopes), 1), 
    ") performs slightly better than\n")
cat("   the random intercepts model (AIC =", round(AIC(model_random_intercept), 1), ").\n")

# Instead of trying to directly access VarCorr(model_lme_slopes)[3, 2], let's extract it properly:
vc_matrix <- VarCorr(model_lme_slopes)
if(nrow(vc_matrix) >= 3) {
  corr_value <- vc_matrix[3, 2]
  cat("   The correlation between random intercepts and slopes is", 
      round(as.numeric(corr_value), 3), ".\n\n")
} else {
  cat("   (Note: Correlation between random intercepts and slopes not available in this model structure)\n\n")
}

sink()

# Create a nice-formatted summary table for manuscript
key_findings <- data.frame(
  Metric = c("SOI Effect (Lag 1)", "SOI Effect (Lag 5)", 
             "Random Intercept SD", "AR(1) Correlation", 
             "AIC (Lag 1 Model)", "AIC (Lag 5 Model)",
             "AIC (Random Intercept Model)"),
  Value = c(
    sprintf("%.4f ± %.4f", model_summary$Coefficient[1], model_summary$StdError[1]),
    sprintf("%.4f ± %.4f", model_summary$Coefficient[5], model_summary$StdError[5]),
    sprintf("%.4f", as.numeric(VarCorr(model_random_intercept)[1, 2])),
    sprintf("%.4f", as.numeric(model_random_intercept$modelStruct$corStruct)),
    sprintf("%.1f", model_summary$AIC[1]),
    sprintf("%.1f", model_summary$AIC[5]),
    sprintf("%.1f", AIC(model_random_intercept))
  ),
  Interpretation = c(
    ifelse(model_summary$pValue[1] < 0.05, "Significant positive effect", "Non-significant"),
    ifelse(model_summary$pValue[5] < 0.05, "Significant negative effect", "Non-significant"),
    "Moderate colony-level variation",
    "Minimal temporal autocorrelation",
    "Better than lags 2-4",
    "Best model fit",
    "Similar to random slopes model"
  )
)

write.csv(key_findings, file.path(output_dir, "soi_key_findings.csv"), row.names = FALSE)

# Create Nature-style table for publication
nature_table <- xtable(
  key_findings[, 1:2],
  caption = "Table 1. Southern Oscillation Index effects on Gentoo penguin population growth rates.",
  label = "tab:soi_key_findings"
)

nature_table_code <- capture.output(print(nature_table, 
                                          include.rownames = FALSE,
                                          caption.placement = "top"))
writeLines(nature_table_code, file.path(output_dir, "nature_soi_table.tex"))

# Output manuscript-ready results in a formatted file
manuscript_text <- file.path(output_dir, "manuscript_text_soi.txt")
sink(manuscript_text)

cat("SOI Effects on Gentoo Penguin Population Growth Rates\n")
cat("=====================================================\n\n")

cat("Our analysis of Southern Oscillation Index (SOI) effects on Gentoo penguin population growth\n")
cat("reveals a complex pattern of temporal relationships. Positive SOI values (La Niña conditions)\n")
cat("are significantly associated with increased growth rates in the following year (coefficient =\n")
cat(sprintf("%.4f ± %.4f, t = %.2f, p < 0.001", 
            model_summary$Coefficient[1], 
            model_summary$StdError[1],
            model_summary$tValue[1]), "). \n\n")

cat("Interestingly, we observed a 5-year lagged negative relationship (coefficient = ")
cat(sprintf("%.4f ± %.4f, t = %.2f, p < 0.001", 
            model_summary$Coefficient[5], 
            model_summary$StdError[5],
            model_summary$tValue[5]), "), \n")
cat("suggesting a complex oscillatory pattern in SOI effects on penguin demographics. The model\n")
cat("with 5-year lag provides the best fit (AIC = ", round(model_summary$AIC[5], 1), ") followed by the\n")
cat("1-year lag model (AIC = ", round(model_summary$AIC[1], 1), ").\n\n")

cat("These findings support the hypothesis that large-scale climate oscillations influence\n")
cat("Gentoo penguin population dynamics through their effects on sea ice conditions.\n")
cat("SOI-driven changes in sea ice extent and duration likely create complex demographic\n")
cat("responses through direct effects on adult survival and indirect effects on recruitment\n")
cat("that manifest over multiple years.\n\n")

sink()

cat("\nAnalysis complete. Publication-quality results saved to:", output_dir, "\n")
cat("Enhanced Nature-style figures created and saved successfully.\n")

```


